{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vO1tjOayUr3b",
    "outputId": "1b972979-0d13-4699-a55e-24374295bb53",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install datasets\n",
    "#!pip install ekphrasis\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJX-JlugKves"
   },
   "source": [
    "## Load in dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lUvpg3NPbZnz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\evanc\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ekphrasis\\classes\\tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\evanc\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ekphrasis\\classes\\exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../Preprocess')\n",
    "\n",
    "from dataCollect import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>annotatorid1</th>\n",
       "      <th>target1</th>\n",
       "      <th>label1</th>\n",
       "      <th>annotatorid2</th>\n",
       "      <th>target2</th>\n",
       "      <th>label2</th>\n",
       "      <th>annotatorid3</th>\n",
       "      <th>target3</th>\n",
       "      <th>label3</th>\n",
       "      <th>rationales</th>\n",
       "      <th>final_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1179055004553900032_twitter</td>\n",
       "      <td>[i, dont, think, im, getting, my, baby, them, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[None]</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "      <td>[None]</td>\n",
       "      <td>normal</td>\n",
       "      <td>3</td>\n",
       "      <td>[None]</td>\n",
       "      <td>normal</td>\n",
       "      <td>[]</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1179063826874032128_twitter</td>\n",
       "      <td>[we, cannot, continue, calling, ourselves, fem...</td>\n",
       "      <td>1</td>\n",
       "      <td>[None]</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "      <td>[None]</td>\n",
       "      <td>normal</td>\n",
       "      <td>3</td>\n",
       "      <td>[None]</td>\n",
       "      <td>normal</td>\n",
       "      <td>[]</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1178793830532956161_twitter</td>\n",
       "      <td>[nawt, yall, niggers, ignoring, me]</td>\n",
       "      <td>4</td>\n",
       "      <td>[African]</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "      <td>[None]</td>\n",
       "      <td>normal</td>\n",
       "      <td>3</td>\n",
       "      <td>[African]</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>[]</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1179088797964763136_twitter</td>\n",
       "      <td>[&lt;user&gt;, i, am, bit, confused, coz, chinese, p...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Asian]</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>4</td>\n",
       "      <td>[Asian]</td>\n",
       "      <td>offensive</td>\n",
       "      <td>3</td>\n",
       "      <td>[Asian]</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>hatespeech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1179085312976445440_twitter</td>\n",
       "      <td>[this, bitch, in, whataburger, eating, a, burg...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Caucasian, Women]</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>2</td>\n",
       "      <td>[Women, Caucasian]</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>3</td>\n",
       "      <td>[Women, Caucasian]</td>\n",
       "      <td>offensive</td>\n",
       "      <td>[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>hatespeech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       post_id  \\\n",
       "0  1179055004553900032_twitter   \n",
       "1  1179063826874032128_twitter   \n",
       "2  1178793830532956161_twitter   \n",
       "3  1179088797964763136_twitter   \n",
       "4  1179085312976445440_twitter   \n",
       "\n",
       "                                                text  annotatorid1  \\\n",
       "0  [i, dont, think, im, getting, my, baby, them, ...             1   \n",
       "1  [we, cannot, continue, calling, ourselves, fem...             1   \n",
       "2                [nawt, yall, niggers, ignoring, me]             4   \n",
       "3  [<user>, i, am, bit, confused, coz, chinese, p...             1   \n",
       "4  [this, bitch, in, whataburger, eating, a, burg...             4   \n",
       "\n",
       "              target1      label1  annotatorid2             target2  \\\n",
       "0              [None]      normal             2              [None]   \n",
       "1              [None]      normal             2              [None]   \n",
       "2           [African]      normal             2              [None]   \n",
       "3             [Asian]  hatespeech             4             [Asian]   \n",
       "4  [Caucasian, Women]  hatespeech             2  [Women, Caucasian]   \n",
       "\n",
       "       label2  annotatorid3             target3      label3  \\\n",
       "0      normal             3              [None]      normal   \n",
       "1      normal             3              [None]      normal   \n",
       "2      normal             3           [African]  hatespeech   \n",
       "3   offensive             3             [Asian]  hatespeech   \n",
       "4  hatespeech             3  [Women, Caucasian]   offensive   \n",
       "\n",
       "                                          rationales final_label  \n",
       "0                                                 []      normal  \n",
       "1                                                 []      normal  \n",
       "2                                                 []      normal  \n",
       "3  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  hatespeech  \n",
       "4  [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  hatespeech  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'data_file' : '../Data/dataset.json', 'class_names' : '../Data/classes.npy'}\n",
    "\n",
    "raw_data = get_annotated_data(params)\n",
    "\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8r0CrpzBL_Kq"
   },
   "source": [
    "## Implementation of EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c6xE2NEeAxFu",
    "outputId": "6b41c2fd-2010-408f-bc86-36f9a5ef45c1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\evanc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "from random import shuffle\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "I6TiYx2wA55Z",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:170: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:170: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "C:\\Users\\evanc\\AppData\\Local\\Temp/ipykernel_22324/625384722.py:170: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  words = [word for word in words if word is not '']\n"
     ]
    }
   ],
   "source": [
    "# Easy data augmentation techniques for text classification\n",
    "# Jason Wei and Kai Zou\n",
    "\n",
    "random.seed(2022)\n",
    "\n",
    "#stop words list\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', \n",
    "\t\t\t'ours', 'ourselves', 'you', 'your', 'yours', \n",
    "\t\t\t'yourself', 'yourselves', 'he', 'him', 'his', \n",
    "\t\t\t'himself', 'she', 'her', 'hers', 'herself', \n",
    "\t\t\t'it', 'its', 'itself', 'they', 'them', 'their', \n",
    "\t\t\t'theirs', 'themselves', 'what', 'which', 'who', \n",
    "\t\t\t'whom', 'this', 'that', 'these', 'those', 'am', \n",
    "\t\t\t'is', 'are', 'was', 'were', 'be', 'been', 'being', \n",
    "\t\t\t'have', 'has', 'had', 'having', 'do', 'does', 'did',\n",
    "\t\t\t'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or',\n",
    "\t\t\t'because', 'as', 'until', 'while', 'of', 'at', \n",
    "\t\t\t'by', 'for', 'with', 'about', 'against', 'between',\n",
    "\t\t\t'into', 'through', 'during', 'before', 'after', \n",
    "\t\t\t'above', 'below', 'to', 'from', 'up', 'down', 'in',\n",
    "\t\t\t'out', 'on', 'off', 'over', 'under', 'again', \n",
    "\t\t\t'further', 'then', 'once', 'here', 'there', 'when', \n",
    "\t\t\t'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "\t\t\t'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "\t\t\t'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', \n",
    "\t\t\t'very', 's', 't', 'can', 'will', 'just', 'don', \n",
    "\t\t\t'should', 'now', '']\n",
    "\n",
    "#cleaning up text\n",
    "import re\n",
    "def get_only_chars(line):\n",
    "\n",
    "    clean_line = \"\"\n",
    "\n",
    "    line = line.replace(\"â€™\", \"\")\n",
    "    line = line.replace(\"'\", \"\")\n",
    "    line = line.replace(\"-\", \" \") #replace hyphens with spaces\n",
    "    line = line.replace(\"\\t\", \" \")\n",
    "    line = line.replace(\"\\n\", \" \")\n",
    "    line = line.lower()\n",
    "\n",
    "    for char in line:\n",
    "        if char in 'qwertyuiopasdfghjklzxcvbnm ':\n",
    "            clean_line += char\n",
    "        else:\n",
    "            clean_line += ' '\n",
    "\n",
    "    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n",
    "    if clean_line[0] == ' ':\n",
    "        clean_line = clean_line[1:]\n",
    "    return clean_line\n",
    "\n",
    "########################################################################\n",
    "# Synonym replacement\n",
    "# Replace n words in the sentence with synonyms from wordnet\n",
    "########################################################################\n",
    "\n",
    "def synonym_replacement(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\trandom_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "\trandom.shuffle(random_word_list)\n",
    "\tnum_replaced = 0\n",
    "\tfor random_word in random_word_list:\n",
    "\t\tsynonyms = get_synonyms(random_word)\n",
    "\t\tif len(synonyms) >= 1:\n",
    "\t\t\tsynonym = random.choice(list(synonyms))\n",
    "\t\t\tnew_words = [synonym if word == random_word else word for word in new_words]\n",
    "\t\t\t#print(\"replaced\", random_word, \"with\", synonym)\n",
    "\t\t\tnum_replaced += 1\n",
    "\t\tif num_replaced >= n: #only replace up to n words\n",
    "\t\t\tbreak\n",
    "\n",
    "\t#this is stupid but we need it, trust me\n",
    "\tsentence = ' '.join(new_words)\n",
    "\tnew_words = sentence.split(' ')\n",
    "\n",
    "\treturn new_words\n",
    "\n",
    "def get_synonyms(word):\n",
    "\tsynonyms = set()\n",
    "\tfor syn in wordnet.synsets(word): \n",
    "\t\tfor l in syn.lemmas(): \n",
    "\t\t\tsynonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "\t\t\tsynonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "\t\t\tsynonyms.add(synonym) \n",
    "\tif word in synonyms:\n",
    "\t\tsynonyms.remove(word)\n",
    "\treturn list(synonyms)\n",
    "\n",
    "########################################################################\n",
    "# Random deletion\n",
    "# Randomly delete words from the sentence with probability p\n",
    "########################################################################\n",
    "\n",
    "def random_deletion(words, p):\n",
    "\n",
    "\t#obviously, if there's only one word, don't delete it\n",
    "\tif len(words) == 1:\n",
    "\t\treturn words\n",
    "\n",
    "\t#randomly delete words with probability p\n",
    "\tnew_words = []\n",
    "\tfor word in words:\n",
    "\t\tr = random.uniform(0, 1)\n",
    "\t\tif r > p:\n",
    "\t\t\tnew_words.append(word)\n",
    "\n",
    "\t#if you end up deleting all words, just return a random word\n",
    "\tif len(new_words) == 0:\n",
    "\t\trand_int = random.randint(0, len(words)-1)\n",
    "\t\treturn [words[rand_int]]\n",
    "\n",
    "\treturn new_words\n",
    "\n",
    "########################################################################\n",
    "# Random swap\n",
    "# Randomly swap two words in the sentence n times\n",
    "########################################################################\n",
    "\n",
    "def random_swap(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\tfor _ in range(n):\n",
    "\t\tnew_words = swap_word(new_words)\n",
    "\treturn new_words\n",
    "\n",
    "def swap_word(new_words):\n",
    "\trandom_idx_1 = random.randint(0, len(new_words)-1)\n",
    "\trandom_idx_2 = random_idx_1\n",
    "\tcounter = 0\n",
    "\twhile random_idx_2 == random_idx_1:\n",
    "\t\trandom_idx_2 = random.randint(0, len(new_words)-1)\n",
    "\t\tcounter += 1\n",
    "\t\tif counter > 3:\n",
    "\t\t\treturn new_words\n",
    "\tnew_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] \n",
    "\treturn new_words\n",
    "\n",
    "########################################################################\n",
    "# Random insertion\n",
    "# Randomly insert n words into the sentence\n",
    "########################################################################\n",
    "\n",
    "def random_insertion(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\tfor _ in range(n):\n",
    "\t\tadd_word(new_words)\n",
    "\treturn new_words\n",
    "\n",
    "def add_word(new_words):\n",
    "\tsynonyms = []\n",
    "\tcounter = 0\n",
    "\twhile len(synonyms) < 1:\n",
    "\t\trandom_word = new_words[random.randint(0, len(new_words)-1)]\n",
    "\t\tsynonyms = get_synonyms(random_word)\n",
    "\t\tcounter += 1\n",
    "\t\tif counter >= 10:\n",
    "\t\t\treturn\n",
    "\trandom_synonym = synonyms[0]\n",
    "\trandom_idx = random.randint(0, len(new_words)-1)\n",
    "\tnew_words.insert(random_idx, random_synonym)\n",
    "\n",
    "########################################################################\n",
    "# main data augmentation function\n",
    "########################################################################\n",
    "\n",
    "def eda(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9):\n",
    "\t\n",
    "\tsentence = get_only_chars(sentence)\n",
    "\twords = sentence.split(' ')\n",
    "\twords = [word for word in words if word is not '']\n",
    "\tnum_words = len(words)\n",
    "\t\n",
    "\taugmented_sentences = []\n",
    "\tnum_new_per_technique = int(num_aug/4)+1\n",
    "\n",
    "\t#sr\n",
    "\tif (alpha_sr > 0):\n",
    "\t\tn_sr = max(1, int(alpha_sr*num_words))\n",
    "\t\tfor _ in range(num_new_per_technique):\n",
    "\t\t\ta_words = synonym_replacement(words, n_sr)\n",
    "\t\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\t#ri\n",
    "\tif (alpha_ri > 0):\n",
    "\t\tn_ri = max(1, int(alpha_ri*num_words))\n",
    "\t\tfor _ in range(num_new_per_technique):\n",
    "\t\t\ta_words = random_insertion(words, n_ri)\n",
    "\t\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\t#rs\n",
    "\tif (alpha_rs > 0):\n",
    "\t\tn_rs = max(1, int(alpha_rs*num_words))\n",
    "\t\tfor _ in range(num_new_per_technique):\n",
    "\t\t\ta_words = random_swap(words, n_rs)\n",
    "\t\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\t#rd\n",
    "\tif (p_rd > 0):\n",
    "\t\tfor _ in range(num_new_per_technique):\n",
    "\t\t\ta_words = random_deletion(words, p_rd)\n",
    "\t\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\taugmented_sentences = [get_only_chars(sentence) for sentence in augmented_sentences]\n",
    "\tshuffle(augmented_sentences)\n",
    "\n",
    "\t#trim so that we have the desired number of augmented sentences\n",
    "\tif num_aug >= 1:\n",
    "\t\taugmented_sentences = augmented_sentences[:num_aug]\n",
    "\telse:\n",
    "\t\tkeep_prob = num_aug / len(augmented_sentences)\n",
    "\t\taugmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n",
    "\n",
    "\t#append the original sentence\n",
    "\taugmented_sentences.append(sentence)\n",
    "\n",
    "\treturn augmented_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the text in string format\n",
    "raw_data['text_str'] = raw_data.apply(lambda x: \" \".join(x.text), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>annotatorid1</th>\n",
       "      <th>target1</th>\n",
       "      <th>label1</th>\n",
       "      <th>annotatorid2</th>\n",
       "      <th>target2</th>\n",
       "      <th>label2</th>\n",
       "      <th>annotatorid3</th>\n",
       "      <th>target3</th>\n",
       "      <th>label3</th>\n",
       "      <th>rationales</th>\n",
       "      <th>final_label</th>\n",
       "      <th>text_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1179055004553900032_twitter</td>\n",
       "      <td>[i, dont, think, im, getting, my, baby, them, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[None]</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "      <td>[None]</td>\n",
       "      <td>normal</td>\n",
       "      <td>3</td>\n",
       "      <td>[None]</td>\n",
       "      <td>normal</td>\n",
       "      <td>[]</td>\n",
       "      <td>normal</td>\n",
       "      <td>i dont think im getting my baby them white 9 h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1179063826874032128_twitter</td>\n",
       "      <td>[we, cannot, continue, calling, ourselves, fem...</td>\n",
       "      <td>1</td>\n",
       "      <td>[None]</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "      <td>[None]</td>\n",
       "      <td>normal</td>\n",
       "      <td>3</td>\n",
       "      <td>[None]</td>\n",
       "      <td>normal</td>\n",
       "      <td>[]</td>\n",
       "      <td>normal</td>\n",
       "      <td>we cannot continue calling ourselves feminists...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1178793830532956161_twitter</td>\n",
       "      <td>[nawt, yall, niggers, ignoring, me]</td>\n",
       "      <td>4</td>\n",
       "      <td>[African]</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "      <td>[None]</td>\n",
       "      <td>normal</td>\n",
       "      <td>3</td>\n",
       "      <td>[African]</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>[]</td>\n",
       "      <td>normal</td>\n",
       "      <td>nawt yall niggers ignoring me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1179088797964763136_twitter</td>\n",
       "      <td>[&lt;user&gt;, i, am, bit, confused, coz, chinese, p...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Asian]</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>4</td>\n",
       "      <td>[Asian]</td>\n",
       "      <td>offensive</td>\n",
       "      <td>3</td>\n",
       "      <td>[Asian]</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>&lt;user&gt; i am bit confused coz chinese ppl can n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1179085312976445440_twitter</td>\n",
       "      <td>[this, bitch, in, whataburger, eating, a, burg...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Caucasian, Women]</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>2</td>\n",
       "      <td>[Women, Caucasian]</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>3</td>\n",
       "      <td>[Women, Caucasian]</td>\n",
       "      <td>offensive</td>\n",
       "      <td>[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>this bitch in whataburger eating a burger with...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       post_id  \\\n",
       "0  1179055004553900032_twitter   \n",
       "1  1179063826874032128_twitter   \n",
       "2  1178793830532956161_twitter   \n",
       "3  1179088797964763136_twitter   \n",
       "4  1179085312976445440_twitter   \n",
       "\n",
       "                                                text  annotatorid1  \\\n",
       "0  [i, dont, think, im, getting, my, baby, them, ...             1   \n",
       "1  [we, cannot, continue, calling, ourselves, fem...             1   \n",
       "2                [nawt, yall, niggers, ignoring, me]             4   \n",
       "3  [<user>, i, am, bit, confused, coz, chinese, p...             1   \n",
       "4  [this, bitch, in, whataburger, eating, a, burg...             4   \n",
       "\n",
       "              target1      label1  annotatorid2             target2  \\\n",
       "0              [None]      normal             2              [None]   \n",
       "1              [None]      normal             2              [None]   \n",
       "2           [African]      normal             2              [None]   \n",
       "3             [Asian]  hatespeech             4             [Asian]   \n",
       "4  [Caucasian, Women]  hatespeech             2  [Women, Caucasian]   \n",
       "\n",
       "       label2  annotatorid3             target3      label3  \\\n",
       "0      normal             3              [None]      normal   \n",
       "1      normal             3              [None]      normal   \n",
       "2      normal             3           [African]  hatespeech   \n",
       "3   offensive             3             [Asian]  hatespeech   \n",
       "4  hatespeech             3  [Women, Caucasian]   offensive   \n",
       "\n",
       "                                          rationales final_label  \\\n",
       "0                                                 []      normal   \n",
       "1                                                 []      normal   \n",
       "2                                                 []      normal   \n",
       "3  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  hatespeech   \n",
       "4  [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  hatespeech   \n",
       "\n",
       "                                            text_str  \n",
       "0  i dont think im getting my baby them white 9 h...  \n",
       "1  we cannot continue calling ourselves feminists...  \n",
       "2                      nawt yall niggers ignoring me  \n",
       "3  <user> i am bit confused coz chinese ppl can n...  \n",
       "4  this bitch in whataburger eating a burger with...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "33O6PjbyBFhg"
   },
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Set initial parameters to run EDA on data\n",
    "# run all cells below to create dataframe for each set of parameters\n",
    "#######################################\n",
    "\n",
    "def initialize_params(num_aug=9, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, alpha_rd=0.1):\n",
    "    # num_aug: number of augmented sentences to generate per original sentence\n",
    "    # alpha_sr: how much to replace each word by synonyms\n",
    "    # alpha_ri: how much to insert new words that are synonyms\n",
    "    # alpha_rs: how much to swap words\n",
    "    # alpha_rd: how much to delete words\n",
    "    return num_aug, alpha_sr, alpha_ri, alpha_rs, alpha_rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_EDA(raw_data, num_aug, alpha_sr, alpha_ri, alpha_rs, alpha_rd):\n",
    "    # initialize a dataframe for output\n",
    "    df_EDA = pd.DataFrame(columns = ['post_id', 'text_str', 'final_label'])\n",
    "\n",
    "    # Run EDA for all the rows in raw data \n",
    "    for i in range(len(raw_data)):\n",
    "        #generate more data with standard augmentation\n",
    "        aug_sentences = eda(raw_data.loc[i,'text_str'] , \n",
    "                            alpha_sr=alpha_sr, alpha_ri=alpha_ri, alpha_rs=alpha_rs, p_rd=alpha_rd, num_aug=num_aug)\n",
    "        post_id = [raw_data.loc[i,'post_id'] ] * (num_aug+1)\n",
    "        final_label = [raw_data.loc[i,'final_label'] ] * (num_aug+1)\n",
    "\n",
    "        df_append = pd.DataFrame({\"post_id\": post_id,\n",
    "                                  \"text_str\": aug_sentences, \n",
    "                                  \"final_label\": final_label})\n",
    "\n",
    "        # append to EDA dataframe\n",
    "        df_EDA = df_EDA.append(df_append, ignore_index = True)\n",
    "        \n",
    "    return df_EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\evanc\\AppData\\Local\\Temp/ipykernel_22324/2397526769.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_EDA = df_EDA.append(df_append, ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "df_EDA = run_EDA(raw_data, 7, 0.1,0.1,0.1,0.1)\n",
    "df_EDA.to_csv('../test_data_set/EDA_7_all_0_1s.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\evanc\\AppData\\Local\\Temp/ipykernel_22324/2397526769.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_EDA = df_EDA.append(df_append, ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "# alpha_sr: how much to replace each word by synonyms\n",
    "df_EDA = run_EDA(raw_data, 7, 0.7,0.1,0.1,0.1)\n",
    "df_EDA.to_csv('../test_data_set/EDA_7_0_7_sr_rest_0_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\evanc\\AppData\\Local\\Temp/ipykernel_22324/2397526769.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_EDA = df_EDA.append(df_append, ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "# alpha_ri: how much to insert new words that are synonyms\n",
    "df_EDA = run_EDA(raw_data, 7, 0.1,0.7,0.1,0.1)\n",
    "df_EDA.to_csv('../test_data_set/EDA_7_0_7_ri_rest_0_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\evanc\\AppData\\Local\\Temp/ipykernel_22324/2397526769.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_EDA = df_EDA.append(df_append, ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "# alpha_rs: how much to swap words\n",
    "df_EDA = run_EDA(raw_data, 7, 0.1,0.1,0.7,0.1)\n",
    "df_EDA.to_csv('../test_data_set/EDA_7_0_7_rs_rest_0_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "lUvpg3NPbZnz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\evanc\\AppData\\Local\\Temp/ipykernel_22324/2397526769.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_EDA = df_EDA.append(df_append, ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "# alpha_rd: how much to delete words\n",
    "df_EDA = run_EDA(raw_data, 7, 0.1,0.1,0.1,0.7)\n",
    "df_EDA.to_csv('../test_data_set/EDA_7_0_7_rd_rest_0_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\evanc\\AppData\\Local\\Temp/ipykernel_22324/2397526769.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_EDA = df_EDA.append(df_append, ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "df_EDA = run_EDA(raw_data, 7, 0.5,0.5,0.5,0.5)\n",
    "df_EDA.to_csv('../test_data_set/EDA_7_all_0_5s.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Data_Augmentation",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
