# w266-project-Dera-Evan-Kieran

Data Augmentation Experiments on the HateXplain Dataset
Kieran Berton, Evan Chan, Chukwudera Mojekwu

Introduction

We propose a project based on HateXplain (Mathew et al., 2020), the first benchmark dataset covering multiple aspects of online offensive and toxic hatespeech. Each post in the dataset is annotated from three different perspectives: a basic 3-class classification (i.e., hate, offensive or normal), the target community (i.e., the community that has been the victim of hate speech/offensive speech in the post), and the rationales, i.e., the portions of the post on which their labelling decision (as hate, offensive or normal) is based. The original HateXplain paper surveys a number of different model architectures and explores several accuracy and explainability metrics based on the rationales fed to the model. 

What we plan on doing

We would like to continue the efforts of the original authors by augmenting the HateXplain dataset in various ways that we have seen used throughout NLP literature (Feng et. al, 2021). Some examples include so-called “Easy” Augmentation methods including random substitutions, insertions, and deletions (Wei & Zou, 2019) and more advanced augmentation methods including those described in HateGAN (Cao & Lee, 2020) where synthetic examples of hateful or offensive speech generated by an adversarial model would be included in training. Our goal will be to show improvement in model performance (as graded across various accuracy and explainability metrics) using these augmentation methods. At the same time we hope to gain a better understanding of methods commonly used for augmenting text-based datasets and explaining NLP model performance.

Motivation

The approach, dataset and models presented here are intended to support more accurate and robust detection and classification of online hate which is important in the effort to curb online discrimination and toxicity. Identifying hate speech is a uniquely difficult task due to the differences in what can be classified as ‘hate speech’ in different contexts or domains (Schmidt and Weigan 2017). For example, the word ‘nigga’ is used everyday in online language by the African American community (Vigna et al. 2017) but can also be used maliciously in other contexts. For this reason, it is important to ensure that hate speech detection is robust to various nuances in meaning that may be unique to the domain of discourse. We are interested in testing whether data augmentation techniques will improve the model’s ability to distinguish between these various nuances in meaning 
