{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KSqG-xgDLRI"
   },
   "source": [
    "### All Easy DA - BERT Large Uncased\n",
    "\n",
    "#### Un-augmented test set\n",
    "#### Augment only the training set\n",
    "\n",
    "#### Get Original Paper Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1646721905242,
     "user": {
      "displayName": "Evan Chan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgGKtlehGoFssPGs4v0Yns-qfVPjW1FuloVAn-GMw=s64",
      "userId": "16754265128573798261"
     },
     "user_tz": 360
    },
    "id": "y6DHMdiyDSTl"
   },
   "outputs": [],
   "source": [
    "# !pip install sklearn\n",
    "# !pip install ekphrasis\n",
    "# !pip install transformers\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import transformers\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import os\n",
    "\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1646721905243,
     "user": {
      "displayName": "Evan Chan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgGKtlehGoFssPGs4v0Yns-qfVPjW1FuloVAn-GMw=s64",
      "userId": "16754265128573798261"
     },
     "user_tz": 360
    },
    "id": "X-o7OyjjDYwr"
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.classes_ = np.load('../Data/classes.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.16.2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test, train, dev examples from base notebook\n",
    "\n",
    "train_data_df = pd.read_csv('./Saved_Models/EDA_base_uncased/All_DA_BERT_base_uncased_train_examples.csv')\n",
    "dev_data_df = pd.read_csv('./Saved_Models/EDA_base_uncased/All_DA_BERT_base_uncased_dev_examples.csv')\n",
    "test_data_df = pd.read_csv('./Saved_Models/EDA_base_uncased/All_DA_BERT_base_uncased_test_examples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1646721916193,
     "user": {
      "displayName": "Evan Chan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgGKtlehGoFssPGs4v0Yns-qfVPjW1FuloVAn-GMw=s64",
      "userId": "16754265128573798261"
     },
     "user_tz": 360
    },
    "id": "zOn6K2RgJBkA"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>target1</th>\n",
       "      <th>target2</th>\n",
       "      <th>target3</th>\n",
       "      <th>rationales</th>\n",
       "      <th>final_label</th>\n",
       "      <th>text_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22448349_gab</td>\n",
       "      <td>['common', 'core', 'weed', 'too', 'much', 'rit...</td>\n",
       "      <td>['Men', 'Women']</td>\n",
       "      <td>['Women']</td>\n",
       "      <td>['None']</td>\n",
       "      <td>[]</td>\n",
       "      <td>normal</td>\n",
       "      <td>common core weed too much ritalan chem trails ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1178948520201637888_twitter</td>\n",
       "      <td>['took', 'my', 'nan', 'to', 'the', 'hospital',...</td>\n",
       "      <td>['None']</td>\n",
       "      <td>['None']</td>\n",
       "      <td>['None']</td>\n",
       "      <td>[]</td>\n",
       "      <td>normal</td>\n",
       "      <td>took my nan to the hospital for a x ray i turn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1482573_gab</td>\n",
       "      <td>['&lt;user&gt;', 'well', 'not', 'really', 'islam', '...</td>\n",
       "      <td>['Islam']</td>\n",
       "      <td>['Other']</td>\n",
       "      <td>['Islam']</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,...</td>\n",
       "      <td>offensive</td>\n",
       "      <td>&lt;user&gt; well not really islam does not care for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1097184028149587969_twitter</td>\n",
       "      <td>['&lt;user&gt;', 'france', 'in', '&lt;number&gt;', 'after'...</td>\n",
       "      <td>['Islam', 'Other']</td>\n",
       "      <td>['Islam']</td>\n",
       "      <td>['Islam']</td>\n",
       "      <td>[]</td>\n",
       "      <td>normal</td>\n",
       "      <td>&lt;user&gt; france in &lt;number&gt; after muslims take o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1089569255111176192_twitter</td>\n",
       "      <td>['i', 'will', 'not', 'tolerate', 'non', 'arab'...</td>\n",
       "      <td>['Arab', 'Men', 'Women']</td>\n",
       "      <td>['Arab']</td>\n",
       "      <td>['Arab', 'Islam']</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,...</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>i will not tolerate non arab women slandering ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                      post_id  \\\n",
       "0           0                 22448349_gab   \n",
       "1           1  1178948520201637888_twitter   \n",
       "2           2                  1482573_gab   \n",
       "3           3  1097184028149587969_twitter   \n",
       "4           4  1089569255111176192_twitter   \n",
       "\n",
       "                                                text  \\\n",
       "0  ['common', 'core', 'weed', 'too', 'much', 'rit...   \n",
       "1  ['took', 'my', 'nan', 'to', 'the', 'hospital',...   \n",
       "2  ['<user>', 'well', 'not', 'really', 'islam', '...   \n",
       "3  ['<user>', 'france', 'in', '<number>', 'after'...   \n",
       "4  ['i', 'will', 'not', 'tolerate', 'non', 'arab'...   \n",
       "\n",
       "                    target1    target2            target3  \\\n",
       "0          ['Men', 'Women']  ['Women']           ['None']   \n",
       "1                  ['None']   ['None']           ['None']   \n",
       "2                 ['Islam']  ['Other']          ['Islam']   \n",
       "3        ['Islam', 'Other']  ['Islam']          ['Islam']   \n",
       "4  ['Arab', 'Men', 'Women']   ['Arab']  ['Arab', 'Islam']   \n",
       "\n",
       "                                          rationales final_label  \\\n",
       "0                                                 []      normal   \n",
       "1                                                 []      normal   \n",
       "2  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,...   offensive   \n",
       "3                                                 []      normal   \n",
       "4  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,...  hatespeech   \n",
       "\n",
       "                                       text_combined  \n",
       "0  common core weed too much ritalan chem trails ...  \n",
       "1  took my nan to the hospital for a x ray i turn...  \n",
       "2  <user> well not really islam does not care for...  \n",
       "3  <user> france in <number> after muslims take o...  \n",
       "4  i will not tolerate non arab women slandering ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2005,
     "status": "ok",
     "timestamp": 1646722287248,
     "user": {
      "displayName": "Evan Chan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgGKtlehGoFssPGs4v0Yns-qfVPjW1FuloVAn-GMw=s64",
      "userId": "16754265128573798261"
     },
     "user_tz": 360
    },
    "id": "wr-vv22ZMhT-"
   },
   "outputs": [],
   "source": [
    "X_train_id = train_data_df['post_id']\n",
    "X_test_id = test_data_df['post_id']\n",
    "X_dev_id = dev_data_df['post_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data_df['final_label']\n",
    "y_test = test_data_df['final_label']\n",
    "y_dev = dev_data_df['final_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15383\n",
      "1923\n",
      "1923\n"
     ]
    }
   ],
   "source": [
    "x_train_df = pd.DataFrame({'post_id' : X_train_id.to_list()})\n",
    "x_dev_df = pd.DataFrame({'post_id' : X_dev_id.to_list()})\n",
    "x_test_df = pd.DataFrame({'post_id' : X_test_id.to_list()})\n",
    "\n",
    "# X_train_df = pd.merge(x_train_df, raw_data_final, how='inner', on='post_id')\n",
    "# X_dev_df = pd.merge(x_dev_df, raw_data_final, how='inner', on='post_id')\n",
    "# X_test_df = pd.merge(x_test_df, raw_data_final, how='inner', on='post_id')\n",
    "\n",
    "X_train_text = train_data_df['text_combined'].to_list()\n",
    "X_dev_text= dev_data_df['text_combined'].to_list()\n",
    "X_test_text = test_data_df['text_combined'].to_list()\n",
    "\n",
    "print(len(X_train_text))\n",
    "print(len(X_dev_text))\n",
    "print(len(X_test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115374"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load augmented datasets generated by EDA\n",
    "# sr = synonym replacement\n",
    "# ri = random synonym insertion\n",
    "# rs = random swap\n",
    "# rd = random deletion\n",
    "# dataframe name format: method_number \n",
    "\n",
    "sr_1_df = pd.read_csv('../test_data_set/EDA_5_0_7_sr_rest_0_1.csv')\n",
    "ri_1_df = pd.read_csv('../test_data_set/EDA_5_0_7_ri_rest_0_1.csv')\n",
    "rs_1_df = pd.read_csv('../test_data_set/EDA_5_0_7_rs_rest_0_1.csv')\n",
    "rd_1_df = pd.read_csv('../test_data_set/EDA_5_0_7_rd_rest_0_1.csv')\n",
    "all_1_df = pd.read_csv('../test_data_set/EDA_5_all_0_1s.csv')\n",
    "all_5_df = pd.read_csv('../test_data_set/EDA_5_all_0_5s.csv')\n",
    "\n",
    "# remove undecided labeled examples\n",
    "sr_1_df_filtered = sr_1_df[sr_1_df['final_label'] != 'undecided']\n",
    "ri_1_df_filtered = ri_1_df[ri_1_df['final_label'] != 'undecided']\n",
    "rs_1_df_filtered = rs_1_df[rs_1_df['final_label'] != 'undecided']\n",
    "rd_1_df_filtered = rd_1_df[rd_1_df['final_label'] != 'undecided']\n",
    "all_1_df_filtered = all_1_df[all_1_df['final_label'] != 'undecided']\n",
    "all_5_df_filtered = all_5_df[all_5_df['final_label'] != 'undecided']\n",
    "\n",
    "len(sr_1_df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92298"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate train, dev, test for each set\n",
    "sr_1_df_train = sr_1_df_filtered[sr_1_df_filtered['post_id'].isin(X_train_id)]\n",
    "ri_1_df_train = ri_1_df_filtered[ri_1_df_filtered['post_id'].isin(X_train_id)]\n",
    "rs_1_df_train = rs_1_df_filtered[rs_1_df_filtered['post_id'].isin(X_train_id)]\n",
    "rd_1_df_train = rd_1_df_filtered[rd_1_df_filtered['post_id'].isin(X_train_id)]\n",
    "all_1_df_train = all_1_df_filtered[all_1_df_filtered['post_id'].isin(X_train_id)]\n",
    "all_5_df_train = all_5_df_filtered[all_5_df_filtered['post_id'].isin(X_train_id)]\n",
    "\n",
    "# select text sets\n",
    "\n",
    "aug_sr_text = sr_1_df_train['text_str'].to_list()\n",
    "aug_ri_text = ri_1_df_train['text_str'].to_list()\n",
    "aug_rs_text = rs_1_df_train['text_str'].to_list()\n",
    "aug_rd_text = rd_1_df_train['text_str'].to_list()\n",
    "aug_all_1_text = all_1_df_train['text_str'].to_list()\n",
    "aug_all_5_text = all_5_df_train['text_str'].to_list()\n",
    "\n",
    "# select label sets\n",
    "\n",
    "aug_sr_labels = sr_1_df_train['final_label']\n",
    "aug_ri_labels = ri_1_df_train['final_label']\n",
    "aug_rs_labels = rs_1_df_train['final_label']\n",
    "aug_rd_labels = rd_1_df_train['final_label']\n",
    "aug_all_1_labels = all_1_df_train['final_label']\n",
    "aug_all_5_labels = all_5_df_train['final_label']\n",
    "\n",
    "len(aug_sr_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert labels to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class label to 1 hot encoding\n",
    "\n",
    "def convert_to_oh(S):\n",
    "    '''takes a pandas series of text labels and returns one hot encoding equivalent\n",
    "    0 = normal, 1 = offensive, 2 = hatespeech\n",
    "    ''' \n",
    "    S_numerical = S.apply(lambda x: 0 if x=='normal' else (1 if x=='offensive' else 2))\n",
    "    S_oh = keras.utils.to_categorical(S_numerical, num_classes = 3, dtype = 'float32')\n",
    "    return S_oh\n",
    "    \n",
    "# original dataset - train, dev, and train\n",
    "y_train_orig = convert_to_oh(pd.Series(y_train))\n",
    "y_dev_orig = convert_to_oh(pd.Series(y_dev))\n",
    "y_test_orig = convert_to_oh(pd.Series(y_test))\n",
    "\n",
    "# augmented with sr = 0.1\n",
    "y_train_aug_sr = convert_to_oh(aug_sr_labels)\n",
    "\n",
    "# augmented with ri = 0.1\n",
    "y_train_aug_ri = convert_to_oh(aug_ri_labels)\n",
    "\n",
    "# augmented with rs = 0.1\n",
    "y_train_aug_rs = convert_to_oh(aug_rs_labels)\n",
    "\n",
    "# augmented with rd = 0.1\n",
    "y_train_aug_rd = convert_to_oh(aug_rd_labels)\n",
    "\n",
    "# augmented with all = 0.1\n",
    "y_train_all_1 = convert_to_oh(aug_all_1_labels)\n",
    "\n",
    "# augmented with all = 0.5\n",
    "y_train_all_5 = convert_to_oh(aug_all_5_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92298"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aug_ri_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92298"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train_aug_ri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_aug_ri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "#bert_model = TFBertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i dont suppose im come my child them edward white he has edward white watt second and nike not regular contact',\n",
       " 'i dont think im getting my baby them white he has two white j and nikes not even touched',\n",
       " 'i think im getting my baby them white he has two white j and nikes not even touched',\n",
       " 'i dont think im getting my baby them j he has two white white and nikes not even touched',\n",
       " 'i dont think im getting my baby them white he has two white j and nikes not even touched',\n",
       " 'i dont think im getting my baby them white he has two white j and nikes not even touched',\n",
       " 'we arent continue calling ourselves feminists if the rights of all womxn to addressed yes list a sexual offences public cannot but will a trans lesbian bisexual on queer womxn be able to enter their information and the reporting sheet gender forum',\n",
       " 'we cannot continue feminists ourselves calling if the be of list womxn arent addressed yes to a sexual offences public but all will a trans lesbian bisexual and queer womxn rights able to enter their information on the reporting sheet gender forum',\n",
       " 'we cannot keep on vocation ourselves feminist if the right wing of all womxn arent come up to yes to a intimate discourtesy populace lean but will a trans sapphic epicene and bilk womxn be able bodied to go in their info on the account sheet of paper sexuality meeting place',\n",
       " 'we cannot continue calling ourselves feminists if the rights of all womxn arent yes to a sexual offences public list but a trans bisexual and queer womxn be able to enter their information on reporting gender forum',\n",
       " 'we cannot remain bid ourselves feminist if the rightfield of all womxn arent speak yes to a intimate criminal offence world number but will a trans tribade epicene and poof womxn be able bodied to go into their entropy on the account bed sheet sexuality meeting place',\n",
       " 'we cannot continue calling ourselves feminists if the rights of all womxn arent addressed yes to a sexual offences public list but will a trans lesbian bisexual and queer womxn be able to enter their information on the reporting sheet gender forum',\n",
       " 'nawt pine tree state yall niggers ignoring me',\n",
       " 'nawt yall niggers ignoring me',\n",
       " 'nawt yall niggers ignoring me',\n",
       " 'nawt yall spade brush off me',\n",
       " 'nawt yall niggers ignoring ignore me',\n",
       " 'nawt yall niggers ignoring me',\n",
       " 'user i am bit confused coz chinese ppl can not access twitter thn how this ching chong using it i think he pakistani',\n",
       " 'user i am bit confused coz chinese ppl can not access twitter chitter thn how this ching chong using it i think he pakistani',\n",
       " 'user i am it confused coz chinese ppl can not access bit thn how this ching chong using twitter i think he pakistani',\n",
       " 'user i am bit recollect confused coz chinese ppl can not access twitter thn how this ching chong using it i think formosan he pakistani',\n",
       " 'user i am bit confused i chinese thn can not access twitter ppl how this ching chong using it coz think he pakistani',\n",
       " 'user i am bit confused coz chinese ppl can not access twitter thn how this ching chong using it i think he pakistani ',\n",
       " 'this bitch with burger eating a whataburger in no top bun and holding it palm up i hate white bitches',\n",
       " 'this holler in whataburger exhaust a hamburger with no go past roll and prevail it thenar up i detest lily white gripe',\n",
       " 'this bitch in whataburger eating warren burger a burger with take hold no top bun and holding it palm up i hate white bitches',\n",
       " 'this bitch in whataburger eating a burger with no top bun it holding and palm up white hate i bitches',\n",
       " 'this bitch in whataburger eating a burger with no top bun and holding it palm up i hate white bitches',\n",
       " 'this bitch in whataburger eating a burger with no top bun and holding it palm up i hate white bitches',\n",
       " 'laura loomer raped me while screaming at me in disgusting kike language and said we must the goyim laura loomer loomergate',\n",
       " 'laura loomer raped me while screaming oral communication at me in her disgusting kike language and said we must exterminate the cry goyim laura loomer loomergate',\n",
       " 'laura loomer raped me while screaming at we in her disgusting kike language and loomergate me must exterminate the goyim laura loomer said',\n",
       " 'laura loomer raped me while screaming at me in oral communication her disgusting kike language and said sheeny we must exterminate the goyim laura loomer loomergate',\n",
       " 'laura loomer knock me while scream at me in her sicken sheeny spoken communication and aforesaid we moldiness root out the non jew laura loomer loomergate',\n",
       " 'laura loomer raped me while screaming at me in her disgusting kike language and said we must exterminate the goyim laura loomer loomergate',\n",
       " 'and this is why i end up with nigger trainee doctors bed who can not speak properly lack basic knowledge of biology it spade truly scary if the public only knew',\n",
       " 'and this is why i death up with nigra trainee doctor who can not mouth decently deficiency introductory noesis of biota it rightfully shuddery if the world only fuck',\n",
       " 'and this is why i end up with nigger trainee doctors who can not talk speak properly lack basic knowledge of canonical biology it truly scary if the public only knew',\n",
       " 'and this is why i end up with nigger trainee who doctors can not speak properly lack basic knowledge of biology it truly scary only the public if knew',\n",
       " 'trainee this is why i end up with nigger and doctors who can knowledge speak properly lack basic not of biology it truly scary if the public only knew',\n",
       " 'and this is why i end up with nigger trainee doctors who can not speak properly lack basic knowledge of biology it truly scary if the public only knew',\n",
       " 'shamed until shew destitute unless you are a israelite a nigra or a devotee of both',\n",
       " 'you until proven innocent unless guilty are a jew a nigger or a lover of both',\n",
       " 'guilty until proven innocent unless you are a jew spade a nigger or a lover of both',\n",
       " 'guilty until proven innocent unless are a jew a or lover of both',\n",
       " 'shamed until show devoid unless you are a israelite a nigra or a buff of both',\n",
       " 'guilty until proven innocent unless you are a jew a nigger or a lover of both',\n",
       " 'tired i can not support abortion a moral standpoint wired i support it morally easy stop looking at those little niglets as future doctors and see them for what they really are future rapists of white women and future welfare recipient of white tax dollars and future democrat voters',\n",
       " 'jade i can not financial backing miscarriage from a lesson stand wire i can financial backing it virtuously slow block up expect at those footling niglets as succeeding physician and understand them for what they genuinely are succeeding rapist of blank womanhood and succeeding well being receiver of blank assess dollar and succeeding populist elector',\n",
       " 'well worn i can not reinforcement miscarriage from a lesson viewpoint pumped up i can reinforcement it virtuously light point await at those picayune niglets as futurity sophisticate and control them for what they real are futurity rapist of egg white cleaning lady and futurity benefit recipient role of egg white taxation dollar bill and futurity populist elector',\n",
       " 'stop i can not support abortion from a moral standpoint are i can support it morally easy tired looking at as recipient niglets those future doctors and see them for what they really wired future rapists white of women and future welfare little of white tax dollars and future democrat voters',\n",
       " 'tired i can not support abortion from a moral standpoint wired i can support it morally easy stop looking at those little niglets as doctors and see them for what they really are future rapists white women and future welfare recipient of white tax and future democrat voters',\n",
       " 'tired i can not support abortion from a moral standpoint wired i can support it morally easy stop looking at those little niglets as future doctors and see them for what they really are future rapists of white women and future welfare recipient of white tax dollars and future democrat voters',\n",
       " 'turn turn per centum of brit conceive multiculturalism is a unsuccessful person i infer the other per centum are glad with their muzzie maestro there no keen in united kingdom of great britain and northern ireland any longer before long swelled ben will be born again into a minaret',\n",
       " 'number number in of brits percent multiculturalism is a failure i guess the other think percent happy with their muzzie master there no great are britain anymore soon big ben will be converted into a minaret',\n",
       " 'number number percent of brits great britain think multiculturalism is a failure i guess the other percent are happy with their muzzie master there no great follow in follow britain anymore soon big ben will be converted into a minaret',\n",
       " 'number number percent of brits think multiculturalism is pct a failure i guess the other percent are happy with list their muzzie master there no great self aggrandising in britain anymore soon big ben will be converted into a minaret',\n",
       " 'number number percent of brits think multiculturalism is a failure i guess other percent are happy with their muzzie master there great soon big ben will be converted into a minaret',\n",
       " 'number number percent of brits think multiculturalism is a failure i guess the other percent are happy with their muzzie master there no great in britain anymore soon big ben will be converted into a minaret',\n",
       " 'tariq nasheed beating the fuck out of that drum beloved lmao somebody put a biscuit in that niggas mouth already',\n",
       " 'tariq the beating nasheed fuck out of put drum beloved lmao somebody that a biscuit in that niggas mouth already',\n",
       " 'tariq nasheed beating niggas fuck out of that drum beloved lmao the put a biscuit in that somebody mouth already',\n",
       " 'tariq nasheed tucker the get it on out of that swot up honey lmao soul assign a cooky in that nigger oral cavity already',\n",
       " 'tariq nasheed flummox the bang out of that mug up dear lmao person invest a cookie in that coon lip already',\n",
       " 'tariq nasheed beating the fuck out of that drum beloved lmao somebody put a biscuit in that niggas mouth already',\n",
       " 'percent cotton thigh highs to avoid contact whatever with any gay chemicals',\n",
       " 'percent cotton thigh highs to avoid contact with any gay chemicals',\n",
       " 'pct cotton wool second joint highschool to keep off tangency with any homo chemicals',\n",
       " 'percent cotton thigh highs to avoid contact with any gay chemicals',\n",
       " 'percent cotton thigh highs to chemicals contact with any gay avoid',\n",
       " 'percent cotton thigh highs to avoid contact with any gay chemicals',\n",
       " 'oomf a enumerate yearl onetime press sunny and possessed with me somebody help oneself',\n",
       " 'oomf a routine yearl sometime wardrobe sunny and taken up with me somebody helper',\n",
       " 'oomf a number yearl me closet gay and obsessed with old someone help',\n",
       " 'oomf a number yearl old closet gay and obsessed with me someone axerophthol help',\n",
       " 'oomf number yearl old closet gay and obsessed with me someone help',\n",
       " 'oomf a number yearl old closet gay and obsessed with me someone help',\n",
       " 'all of my exes cute but were hoes i guess i only attract fly looking thots',\n",
       " 'all of my exes were but they were hoes i guess i only fly looking thots',\n",
       " 'fly of my exes were cute but they were hoes i guess i only attract all looking thots',\n",
       " 'all of my x were cunning but they were hoe i estimate i only appeal fly ball expect thots',\n",
       " 'all x of my exes were cute but they were hoes i guess i only attract fly looking thots',\n",
       " 'all of my exes were cute but they were hoes i guess i only attract fly looking thots ',\n",
       " 'fellow be thrifty she texting other nigra',\n",
       " 'be fellas careful she texting other niggas',\n",
       " 'fellas early be careful she texting other niggas',\n",
       " 'fellas be careful she other texting niggas',\n",
       " 'fellas be careful she texting other niggas',\n",
       " 'fellas be careful she texting other niggas',\n",
       " 'italian niggas call they dick hawkshaw the poozingi',\n",
       " 'italian niggas call they dick the poozingi',\n",
       " 'italian niggas call they dick the poozingi',\n",
       " 'italian nigger ring they cock the poozingi',\n",
       " 'italian niggas call dick they the poozingi',\n",
       " 'italian niggas call they dick the poozingi',\n",
       " 'user what did the old lady do was a nazi or supremacist',\n",
       " 'user what did the old lady do was she a nazi or white surgery supremacist',\n",
       " 'user what did the old lady do was she a nazi or practice white supremacist',\n",
       " 'user did the old lady do was she a or white supremacist',\n",
       " 'exploiter what did the sometime dame do was she a german nazi or gabardine supremacist',\n",
       " 'user what did the old lady do was she a nazi or white supremacist',\n",
       " 'user user user user user he married a negress because women did not want him but females all over america love nobama',\n",
       " 'user user user user user he married a negress because white women did not want him but practice black females all drug user over america love nobama',\n",
       " 'user user user user white he married a negress because user women america not want him but black females all over did love nobama',\n",
       " 'drug user drug user drug user drug user drug user he matrimonial a negress because andrew dickson white cleaning lady did not neediness him but smuggled female person all over usa make out nobama',\n",
       " 'drug user drug user drug user drug user drug user he get hitched with a negress because e b white charwoman did not deprivation him but opprobrious female person all over united states have sex nobama',\n",
       " 'user user user user user he married a negress because white women did not want him but black females all over america love nobama',\n",
       " 'user condoning drug use not all at kike thanks for that disclosure',\n",
       " 'condoning drug use not kike at all thanks that disclosure',\n",
       " 'drug use not kike all thanks for that disclosure',\n",
       " 'exploiter excuse dose economic consumption not hymie at all thank for that revelation',\n",
       " 'exploiter condone do drugs usance not yid at all give thanks for that revelation',\n",
       " 'user condoning drug use not kike at all thanks for that disclosure',\n",
       " 'user user people support as foreigners you can not say the same about the states obviously not all americans israel to guns not all americans act their government treatment of immigrants and if but when it comes pro are suddenly the logic changes',\n",
       " 'substance abuser substance abuser the great unwashed dissemble as if you can not aver the same about the say plain not all american english are professional ordnance not all american english hold their governing discussion of immigrant and outlander but when it add up to zion abruptly the system of logic convert',\n",
       " 'user user people act immigrants if say when not you the same about the states obviously americans all americans are pro guns not all not support their government treatment of as and foreigners but can it comes to israel suddenly the logic changes',\n",
       " 'user user people act as if you can not say the same about the states obviously not all americans are pro guns not all americans support their government treatment of immigrants and foreigners but when it comes to israel suddenly the logic changes',\n",
       " 'user land user people act as if you non can not say the same sami about the states obviously not all americans are pro guns not all americans support their government treatment of immigrants and foreigners but when it comes to israel suddenly the logical system logic changes',\n",
       " 'user user people act as if you can not say the same about the states obviously not all americans are pro guns not all americans support their government treatment of immigrants and foreigners but when it comes to israel suddenly the logic changes',\n",
       " 'user user they visited helped provided food to intellectual nourishment those kps while patch islam c terrorist butured raped and forced them to leave their own land',\n",
       " 'substance abuser substance abuser they bring down assist cater food for thought to those kp while muhammadanism one c terrorist butured plunder and impel them to get out their own set down',\n",
       " 'user user they visited provided food to those kps while islam c terrorist butured raped and forced them to leave their land',\n",
       " 'exploiter exploiter they call in help oneself supply food for thought to those kitchen police while islamism atomic number terrorist butured ravish and thrust them to depart their own acres',\n",
       " 'user user they visited helped provided food to those kps while islam c terrorist butured raped and forced them to leave their cater cater own land',\n",
       " 'user user they visited helped provided food to those kps while islam c terrorist butured raped and forced them to leave their own land',\n",
       " 'anyhow the fact that her equip straight off plunder jacques anatole francois thibault legal philosophy abt no boldness extend veil but is all the same headline in city of light forge hebdomad present that the legal philosophy was neer abt bedim seventh cranial nerve designation but abt e b white domination and islamophobia',\n",
       " 'read anyways the record fact that her outfit directly violates france law abt no face coverings veils but is still headlined in paris fashion week shows that the law was never abt obscured facial identification but abt white supremacy forge and islamophobia',\n",
       " 'anyways the fact that her outfit directly violates france law no coverings veils but still headlined in paris fashion week shows that the law was never abt obscured facial identification but abt white supremacy and islamophobia',\n",
       " 'anyways the fact that her outfit directly violates france law abt no face veils but still headlined in week shows law was never abt obscured facial but white supremacy and islamophobia',\n",
       " 'anyways the fact that fashion outfit directly obscured france law abt no face coverings never but is still headlined in paris her week shows that the law was veils abt violates facial identification but abt white supremacy and islamophobia',\n",
       " 'anyways the fact that her outfit directly violates france law abt no face coverings veils but is still headlined in paris fashion week shows that the law was never abt obscured facial identification but abt white supremacy and islamophobia',\n",
       " 'it funny how career people offended by the word nigger have no problem calling people nazis what the difference',\n",
       " 'it funny how people offended by the word nigger people no problem calling have nazis what the difference',\n",
       " 'it mirthful how mass scandalize by the countersign nigga have no job visit mass german nazi what the dispute',\n",
       " 'it odd how citizenry pained by the give voice coon have no trouble address citizenry nazi what the difference of opinion',\n",
       " 'it funny how people offended by the word have no problem calling people what the',\n",
       " 'it funny how people offended by the word nigger have no problem calling people nazis what the difference',\n",
       " 'as much as i appreciate a format of open discussion i really just a want facebook that can i say beaner on',\n",
       " 'as much as that appreciate a format of open discussion i really just want a facebook i i can on beaner say',\n",
       " 'deprivation as much as i appreciate a format of open discussion i really just want a facebook that i can say along beaner on',\n",
       " 'as much as i appreciate a format of open discussion i really just want facebook that i can say beaner on',\n",
       " 'much i a of open i really want a facebook that i can say beaner on',\n",
       " 'as much as i appreciate a format of open discussion i really just want a facebook that i can say beaner on',\n",
       " 'that was just kill a photo look up rhyan moody she was killed on thanksgiving by an illegal spic',\n",
       " 'that was just a photo look up rhyan moody was killed on thanksgiving by illegal spic',\n",
       " 'that was a photo look up rhyan moody was on thanksgiving by an illegal spic',\n",
       " 'that was just a photo look up rhyan killed she was moody on thanksgiving by an illegal spic',\n",
       " 'that was just a photo look up rhyan moody exposure she was killed on thanksgiving by an illegal spic',\n",
       " 'that was just a photo look up rhyan moody she was killed on thanksgiving by an illegal spic',\n",
       " 'this sounds like a kike national socialist',\n",
       " 'this sounds axerophthol like a kike national socialist',\n",
       " 'this sounds like a kike national socialist',\n",
       " 'this speech sound wish a yid internal socialist',\n",
       " 'this sounds like a sheeny kike national socialist',\n",
       " 'this sounds like a kike national socialist',\n",
       " 'i saw a couple baums and steins in the victim set out list but that about it ya got more info i thought the school was half nigh spic though',\n",
       " 'i saw a couple baums and in the victim list but that it ya got more info i the school was half spic though',\n",
       " 'i catch a duo lyman frank brown and beer mug in the dupe number but that about it ya arrive more information i thinking the schoolhouse was one half spik though',\n",
       " 'i saw a couple info and steins in the victim list but baums about it ya got more that i thought the school was half spic though',\n",
       " 'i saw a couple baums and steins in the victim list that about it ya got more info i the school half spic though',\n",
       " 'i saw a couple baums and steins in the victim list but that about it ya got more info i thought the school was half spic though',\n",
       " 'only elite looters are enriched when they import a iq moslem onto importee welfare section number homeless vets would not axerophthol need taxpayer practice paid classes on how to use toilets in free homes',\n",
       " 'on elite looters are enriched when not import a iq moslem onto welfare section number homeless vets would they need taxpayer homes classes only how to use toilets in free paid',\n",
       " 'only elite in are enriched when they import a iq moslem onto welfare section number homeless vets not would need paid taxpayer classes on how to use toilets looters free homes',\n",
       " 'only elite looters are enriched when they import a iq moslem onto welfare section number homeless vets would not need paid on how to use toilets in free homes',\n",
       " 'only elite group pillager are enrich when they spell a intelligence quotient islamic onto social welfare department act homeless person vet would not postulate taxpayer paying course of instruction on how to employment lav in resign place',\n",
       " 'only elite looters are enriched when they import a iq moslem onto welfare section number homeless vets would not need taxpayer paid classes on how to use toilets in free homes',\n",
       " 'princess sapphire of silverland is dyke and threw the first brick at stonewall',\n",
       " 'anyways princess lazuline of silverland is a butch and bedevil the first class honours degree brick at stonewall',\n",
       " 'sky blue anyway princess sapphire of silverland is a dyke and threw the first brick at stonewall',\n",
       " 'anyway princess sapphire of a is silverland dyke and threw the first brick at stonewall',\n",
       " 'anyways princess lazuline of silverland is a dam and befuddle the number one brick at stonewall',\n",
       " 'anyway princess sapphire of silverland is a dyke and threw the first brick at stonewall',\n",
       " 'if we could teach be giraffe to paint that would a one handy sumbitch',\n",
       " 'if we could teach a giraffe to that would be one handy sumbitch',\n",
       " 'if we teach a to paint that would be one handy sumbitch',\n",
       " 'if we could w c handy teach a giraffe to paint that would be one handy sumbitch',\n",
       " 'if we could teach a follow giraffe to paint that would be one handy sumbitch',\n",
       " 'if we could teach a giraffe to paint that would be one handy sumbitch',\n",
       " 'she wanna meet carti conform to that bitch is a barbie',\n",
       " 'wanna meet carti that bitch is a barbie',\n",
       " 'meet carti that bitch is a barbie',\n",
       " 'she wanna meet carti that bitch is conform to a barbie',\n",
       " 'she wanna touch carti that squawk is a barbie',\n",
       " 'she wanna meet carti that bitch is a barbie ',\n",
       " 'smile cuz i am youthful shirley temple ample and i am well favoured',\n",
       " 'smiling cuz i am young black rich and i am handsome',\n",
       " 'smiling cuz i am young and rich black i am handsome',\n",
       " 'smiling cuz i deep am young black rich and i am handsome',\n",
       " 'smiling cuz i am young black i am handsome',\n",
       " 'smiling cuz i am young black rich and i am handsome',\n",
       " 'sex be so good a bitch be slow stroking and crying',\n",
       " 'wind up be so safe a holler be slow up stroke and battle cry',\n",
       " 'sex be so good a bitch be slow bellyache stroking and crying',\n",
       " 'sex be so good a bitch be slow stroking and follow crying',\n",
       " 'sexual urge be so expert a bellyache be slacken stroke and war cry',\n",
       " 'sex be so good a bitch be slow stroking and crying',\n",
       " 'bitches be so fine and got the nerve ilk to like niggas',\n",
       " 'bitches be so fine and got the nerve to like niggas',\n",
       " 'bitches be so fine and got the nerve to like niggas',\n",
       " 'squawk be so mulct and vex the mettle to ilk nigra',\n",
       " 'bitches be so fine set out and got the nerve to like niggas',\n",
       " 'bitches be so fine and got the nerve to like niggas ',\n",
       " 'user user the shoe being on the other foot would mean that blacks and jews would pay high subsidy taxes to subsidies the child birth of white babies',\n",
       " 'drug user drug user the brake shoe being on the other invertebrate foot would tight that lightlessness and hebrew would make up senior high assess to subsidy the deliver of whiteness baby',\n",
       " 'user user the shoe blacks on the high foot would mean that being and jews would pay other taxes to subsidies the birth of white babies',\n",
       " 'would user user shoe being on the other foot would mean that blacks and jews the pay high taxes to subsidies the birth of white babies',\n",
       " 'user user the shoe being on the other foot would mean that blacks and jews would pay taxes to subsidies the birth of white babies',\n",
       " 'user user the shoe being on the other foot would mean that blacks and jews would pay high taxes to subsidies the birth of white babies',\n",
       " 'substance abuser substance abuser is a intrigue impostor who battlefront a bushwhacker furor',\n",
       " 'user user is a scheming fraud who hillbilly a fronts cult',\n",
       " 'user a is a scheming fraud who fronts user hillbilly cult',\n",
       " 'user user is a scheming fraud fronts a hillbilly cult',\n",
       " 'user user is a scheming fraud who fronts cult',\n",
       " 'user user is a scheming fraud who fronts a hillbilly cult',\n",
       " 'user no idea who these women are they get all the leap out credit for jumping on after the women char who narrate been calling for impeachment take the hits and smears and told to go back to where set out they come from job well done unknown white congress women',\n",
       " 'user no idea who these women are and get credit the all for from on after the women who been calling for impeachment jumping the hits they smears and told to go back to where they come take job well done unknown white congress women',\n",
       " 'user drug user no idea who these women are they get all the credit for drug user jumping on after the women who been calling for impeachment take the hits and smears and told to char go drug user back to where they come from job well done unknown white congress women',\n",
       " 'user no idea who these women are they get all the credit for jumping after the women who been calling for impeachment take the hits and smears and told to go back to where they come from job well done unknown white congress women',\n",
       " 'user no idea who these women are and get all the credit for jumping on unknown the women smears been calling for impeachment take the hits and who they told to go job to where they come from back well done after white congress women',\n",
       " 'user no idea who these women are they get all the credit for jumping on after the women who been calling for impeachment take the hits and smears and told to go back to where they come from job well done unknown white congress women ',\n",
       " 'yttrium is braggy spoil miles davis a fagot on unblushing energy department',\n",
       " 'unblushing y is big baby davis a faggot on shameless doe',\n",
       " 'y baby davis a faggot on shameless doe',\n",
       " 'y is big baby davis on faggot a shameless doe',\n",
       " 'wye is heavy featherbed dwight davis a queen on unblushing department of energy',\n",
       " 'y is big baby davis a faggot on shameless doe',\n",
       " 'i betray follow do not blame orlando pie rats for loosing like this they are drunk from that black label mtn ss diski wafa wafa',\n",
       " 'i do not damned orlando proto indo european betrayer for unloose same this they are drink in from that lightlessness tag mtn sulphur diski wafa wafa',\n",
       " 'i do not blame orlando pie rats for loosing like this mtn are they from that black label drunk ss diski wafa wafa',\n",
       " 'i do not blame orlando pie that for loosing like this they are from drunk rats black label mtn ss diski wafa wafa',\n",
       " 'i betray do not unleash blame orlando pie rats for loosing like this they are drunk from that black label mtn ss diski wafa wafa',\n",
       " 'i do not blame orlando pie rats for loosing like this they are drunk from that black label mtn ss diski wafa wafa',\n",
       " 'related instagram forge tags for jewelry fashionable designer trendy hairstyle fashion gram fashion blog menswear men style trend',\n",
       " 'link instagram tags for jewellery stylish fashion designer trendy hair style forge hans c j gram forge web log menswear human manner swerve',\n",
       " 'related instagram tags for jewelry designer trendy hairstyle gram fashion blog menswear men style trend',\n",
       " 'related instagram tags men jewelry fashionable designer trendy hairstyle fashion gram fashion blog menswear for style trend',\n",
       " 'related instagram tags for jewelry trendy hairstyle fashion fashion menswear men style trend',\n",
       " 'related instagram tags for jewelry fashionable designer trendy hairstyle fashion gram fashion blog menswear men style trend',\n",
       " 'niggas cry and complain females much i am starting to think you all the real so in the relationship',\n",
       " 'niggas cry and complain so much i all starting to think you am the real females in the relationship',\n",
       " 'niggas cry and complain so much i am starting to think you all the real females in the relationship',\n",
       " 'nigra call and plain so very much i am bug out to consider you all the tangible female in the family relationship',\n",
       " 'niggas cry and complain so much i am starting to think you all the real females in the relationship',\n",
       " 'niggas cry and complain so much i am starting to think you all the real females in the relationship ',\n",
       " 'now stephanie mad this nigga shit he getting liking bitches tonight',\n",
       " 'now stephanie liking this nigga shit he getting mad bitches tonight',\n",
       " 'now stephanie care this spade take a shit he pay off excited squawk this evening',\n",
       " 'now stephanie liking mad nigga shit he getting this bitches tonight',\n",
       " 'now stephanie liking this nigga sore shit he getting mad bitches tonight',\n",
       " 'now stephanie liking this nigga shit he getting mad bitches tonight',\n",
       " 'commemorate pull the leg of the cucumber vine is the sluttiest veggie',\n",
       " 'remember kids chaff the cucumber is the sluttiest vegetable',\n",
       " 'is kids the cucumber remember the sluttiest vegetable',\n",
       " 'remember kids the cucumber is the vegetable sluttiest',\n",
       " 'remember kids chaff the cucumber is the sluttiest vegetable',\n",
       " 'remember kids the cucumber is the sluttiest vegetable',\n",
       " 'user there was a time that they where shooting young men on look at tweets i a whole series these are worse than the nazis and the children get raped and tortured in jail little palestinian children',\n",
       " 'user there was a time that they where shooting young men on follow demand look at follow my axerophthol tweets i did a whole serial series these people are worse than the nazis and the children get raped and tortured in jail little palestinian children',\n",
       " 'exploiter there was a clock time that they where shoot new serviceman on take flavor at my squeeze i did a altogether serial publication these citizenry are unfit than the nazi and the nestling drive sacked and anguished in clink fiddling palestinian arab nestling',\n",
       " 'user there was a follow time that they axerophthol where shooting young men on demand look at my conform to tweets i did a whole series these people are worse than the nazis and atomic number the children get raped and tortured in jail little palestinian children',\n",
       " 'substance abuser there was a sentence that they where sprout youthful humankind on ask await at my twitch i did a altogether serial publication these citizenry are spoilt than the german nazi and the fry have pillaged and anguished in jailhouse niggling palestinian arab fry',\n",
       " 'user there was a time that they where shooting young men on demand look at my tweets i did a whole series these people are worse than the nazis and the children get raped and tortured in jail little palestinian children',\n",
       " 'i am going to give my fellow be negroes some roman advice dont show up at white upward christian schools trying to make connections show up connector at because you are trying to be a christian the romans originally hated the christians they burned them at the stake and roman letters they fed them to lions',\n",
       " 'i them going to give my fellow up trying up dont show negroes at white christian are trying to make connections show advice at because you schools some to be a christian the romans originally hated the christians they burned am at the stake and they fed them to lions',\n",
       " 'i am choke to open my familiar black person some advice dont designate up at flannel christian schooling prove to urinate connection designate up at because you are prove to be a christian the epistle to the romans in the beginning hate the christian they fire them at the adventure and they run them to leo',\n",
       " 'i am going to give my fellow negroes some advice dont show up at white trying to make connections show up at because you are trying be a christian the romans originally hated the christians they burned them at the and they them to lions',\n",
       " 'going am white to give my fellow negroes some advice dont show to at i christian schools trying to make connections romans up at because you are trying to them a christian the show originally hated the christians they burned be at the stake and they fed them up lions',\n",
       " 'i am going to give my fellow negroes some advice dont show up at white christian schools trying to make connections show up at because you are trying to be a christian the romans originally hated the christians they burned them at the stake and they fed them to lions',\n",
       " 'user user user it way better than sonoma if you remember your redneck jesus won the glenn also',\n",
       " 'drug user user user user it was way better than sonoma if you remember your redneck jesus won the glenn also',\n",
       " 'user user user it was way you than sonoma if better remember your redneck jesus won the glenn also',\n",
       " 'substance abuser substance abuser substance abuser it was direction respectable than sonoma if you think your cracker good shepherd advance the john herschel glenn jr besides',\n",
       " 'user user user it was way better than sonoma if you remember your jesus redneck won the glenn also',\n",
       " 'user user user it was way better than sonoma if you remember your redneck jesus won the glenn also',\n",
       " 'the flag of comes maga fags in all it glory it the with nigger red shoes too',\n",
       " 'all flag of the maga fags in the it glory it comes with nigger red shoes too',\n",
       " 'the flag of the maga fags in all it glory it fleur de lis comes with nigger red shoes too',\n",
       " 'the flag the maga fags in all it glory it comes with red shoes too',\n",
       " 'the flag of the maga fags in all it reddish glory it comes with nigger red shoes too',\n",
       " 'the flag of the maga fags in all it glory it comes with nigger red shoes too',\n",
       " 'hispanic i hear the phrase white privilege one more words i am going to opt as come back in my next life to a black transgender latino illegal muslim jewish faggot feminist queer gay lesbian dike if piece of shit in other time everything i have not been in this life',\n",
       " 'if i hear the phrase white privilege one more time i am going to opt queer to come tribade back in my next life as more than a black hispanic latino illegal muslim jewish faggot feminist queer gay lesbian dike transgender piece of shit axerophthol in other words everything i have hoosier state not been in this life',\n",
       " 'if i hear the phrase white privilege one more time i am going to opt come back in my next life as a black hispanic latino muslim jewish faggot feminist queer gay dike transgender piece of shit other words everything i not been this',\n",
       " 'if i listen the phrasal idiom clean perquisite ace more metre i am get to prefer to come up backbone in my side by side life history as a melanize hispanic american hispanic illegal moslem judaic queen libber pouf festive tribade dam transgendered assemble of rat in other parole everything i have not been in this life history',\n",
       " 'if i take heed the idiomatic expression snowy exclusive right nonpareil more clock time i am release to prefer to descend backrest in my succeeding living as a nigrify hispanic american latin american illegal islamic judaic fairy womens rightist curious merry tribade dyke transgendered put together of whoreson in other formulate everything i have not been in this living',\n",
       " 'if i hear the phrase white privilege one more time i am going to opt to come back in my next life as a black hispanic latino illegal muslim jewish faggot feminist queer gay lesbian dike transgender piece of shit in other words everything i have not been in this life',\n",
       " 'just mute another dumb nigger',\n",
       " 'just another nigger',\n",
       " 'just another dumb',\n",
       " 'just some other dumb coon',\n",
       " 'just another dumb some other nigger',\n",
       " 'just another dumb nigger',\n",
       " 'user the cunt of retarded nigger beads of shit into of a jew kike',\n",
       " 'user drug user the cunt of a retarded nigger pissed beads of shit into asshole of a jew kike',\n",
       " 'user the cunt of a retarded nigger pissed beads of shit shit into asshole of a jew kike',\n",
       " 'user the cunt of pissed retarded nigger a beads of shit into asshole of a jew kike',\n",
       " 'user the a of a retarded nigger pissed beads of shit into asshole of cunt jew kike',\n",
       " 'user the cunt of a retarded nigger pissed beads of shit into asshole of a jew kike',\n",
       " 'substance abuser you first hymie',\n",
       " 'user first kike',\n",
       " 'user kike first you',\n",
       " 'substance abuser you start kike',\n",
       " 'for the first time user you first kike',\n",
       " 'user you first kike',\n",
       " 'jew should be negotiation they are a disease that they to eradicated from among white folk needs and their subhuman nigger and arab foot soldiers are the enemy their annihilation is not hated for up',\n",
       " 'hebrew should be despised they are a disease that involve to decimate from among snowy tribe they and their subhuman spade and arabian human foot soldier are the foe their obliteration is not up for dialogue',\n",
       " 'jew should be hated they are a disease that needs to eradicated among white folk they and their subhuman nigger and arab foot soldiers are the enemy their annihilation is not up for negotiation',\n",
       " 'jew should hated are a disease that needs to eradicated from among white they and their subhuman nigger and arab are enemy their annihilation is not up for negotiation',\n",
       " 'israelite should be detested they are a disease that involve to decimate from among white hot folk music they and their subhuman nigra and arabian fundament soldier are the foeman their obliteration is not up for talks',\n",
       " 'jew should be hated they are a disease that needs to eradicated from among white folk they and their subhuman nigger and arab foot soldiers are the enemy their annihilation is not up for negotiation',\n",
       " 'all these professional caucasian christian should be behave to africa where they go who would ejaculate out against that these insurgent part of diddlyshit call for vantage of caucasian civilisation while open the logic gate for nigga we involve measure that the nates bank line and these coon dismantle iq are absolutely free weight',\n",
       " 'against these pro white level should be deported to all advantage they belong who would come out africa that these subversive pieces niggers shit take where of white civilization while opening the gates for of we need standards that the bottom line and these nigger christians iqs are dead weight',\n",
       " 'all these pro white christians be deported to africa they belong who would come out against that these subversive pieces of shit take advantage of white civilization while opening the gates for niggers we need standards the bottom line and these nigger level iqs are dead weight',\n",
       " 'all these professional blank christian should be conduct to africa where they belong to who would hail out against that these revolutionary assemble of crap direct vantage of blank refinement while first step the bill gates for nigga we motivation criterion that the tush origin and these jigaboo take down iq are all in weighting',\n",
       " 'all these pro white bed christians should be deported to world health organization africa where they belong who go would come out against that these subversive pieces of call for shit take reward advantage of white civilization while opening the gates for niggers we need standards that the bottom line and these nigger level iqs are dead weight',\n",
       " 'all these pro white christians should be deported to africa where they belong who would come out against that these subversive pieces of shit take advantage of white civilization while opening the gates for niggers we need standards that the bottom line and these nigger level iqs are dead weight',\n",
       " 'until now some other amandine aurore lucie dupin coon slaughter innocent hey libscum i bob hope your seraphic trivial affectionate girl are succeeding you fetch this trash here now populate with it',\n",
       " 'yet i sand nigger slaughters little hey libscum another hope your sweet innocents tender daughters are next you brought this scum here now live with it',\n",
       " 'yet another sand nigger slaughters hey libscum i hope your sweet little tender are next you brought this scum now live with it',\n",
       " 'yet another some other sand nigger small slaughters innocents hey libscum i hope your sweet little tender daughters are next you brought this scum here now live with it',\n",
       " 'until now some other sandpaper jigaboo massacre inexperienced person hey libscum i leslie townes hope your gratifying short ships boat daughter are side by side you work this trash here now know with it',\n",
       " 'yet another sand nigger slaughters innocents hey libscum i hope your sweet little tender daughters are next you brought this scum here now live with it',\n",
       " 'yet when a couple of filthy dykes want you to pander to their sins as you deprivation see them you must comply by law time to start organizing for yourselves no one else is clock gonna do it deprivation for you',\n",
       " 'sins when a couple of filthy their want you to pander to is yet as you see them you must comply by law time to start organizing for yourselves no one else dykes gonna do it for you',\n",
       " 'nevertheless when a copulate of cruddy dike wish you to procure to their wickedness as you see to it them you mustiness abide by by police clock time to bulge machinate for yourselves no one and only else is gonna do it for you',\n",
       " 'nevertheless when a duo of smutty dyke deprivation you to procure to their sin as you consider them you mustiness follow by jurisprudence fourth dimension to protrude machinate for yourselves no i else is gonna do it for you',\n",
       " 'yet when away a couple of filthy dykes want you to pander go out to their sins as you see them you must comply deprivation by law time to start organizing for yourselves no one else is gonna do it for you',\n",
       " 'yet when a couple of filthy dykes want you to pander to their sins as you see them you must comply by law time to start organizing for yourselves no one else is gonna do it for you',\n",
       " 'user and all the animals that were animal killed people were horrified for their bears and coons and they call themselves liberals the definition of a liberal and drug user feminist have changed keep your drug user old dictionaries',\n",
       " 'user and all the animals that were killed people were horrified for their bears and coons and they call themselves liberals the definition of a liberal and changed keep your old dictionaries',\n",
       " 'user and all the animals that of killed their were horrified for people bears and coons and they call themselves liberal the definition were a liberals and feminist have changed keep your old dictionaries',\n",
       " 'user and all the animals that were killed people were horrified for their bears and coons and they liberals of a feminist have changed keep your old dictionaries',\n",
       " 'substance abuser and all the beast that were vote down mass were horror struck for their suffer and nigra and they anticipate themselves progressive the definition of a openhanded and womens rightist have convert maintain your former dictionary',\n",
       " 'user and all the animals that were killed people were horrified for their bears and coons and they call themselves liberals the definition of a liberal and feminist have changed keep your old dictionaries',\n",
       " 'lol that why i wear them i have and many confrontations with the pos muzzies had black panther scum',\n",
       " 'lol that why i wear them i have had many confrontations with the pos panther and black muzzies scum',\n",
       " 'lol that why i tire out them i have had many confrontation with the atomic number muzzies and ignominious catamount trash',\n",
       " 'lol that why i wear them i have had many confrontations with pos muzzies and black panther scum',\n",
       " 'lol that why i wear them i have had felis concolor many confrontations with the pos muzzies and black panther scum',\n",
       " 'lol that why i wear them i have had many confrontations with the pos muzzies and black panther scum',\n",
       " 'cue me of a college fundraiser to serve a yid sept reconstruct their burn down business firm the founder was a magistrado which is corresponding a senior high school courtyard evaluator so they were deep than most of the other pupil class',\n",
       " 'cue me of a college fundraiser to assistant a sheeny household reconstruct their burn down business firm the engender was a magistrado which is alike a heights tribunal pass judgment so they were copious than most of the other scholar kinfolk',\n",
       " 'reminds me of a college fundraiser to help a kike pine tree state family rebuild their burnt house remind the father was a magistrado which is like a high court judge so they were richer than most of the other students remind families',\n",
       " 'reminds me follow of a college fundraiser theatre to help a kike family rebuild their burnt house ilk the father was a magistrado which is like a high court judge so they were richer than most of the other students families',\n",
       " 'reminds me of a college fundraiser to help a magistrado family rebuild their burnt house a father was the which kike is like a high court judge so they were richer than most of the other students families',\n",
       " 'reminds me of a college fundraiser to help a kike family rebuild their burnt house the father was a magistrado which is like a high court judge so they were richer than most of the other students families',\n",
       " 'pouffe i tire with you all day same just blah blah blah kike you will see it a axerophthol dream cheese puff boy go bed it a school night',\n",
       " 'i pall with you all clarence day same just bombast bombast bombast hymie you will visualize it a ambition high mallow hassock male child get going hump it a schoolhouse nox',\n",
       " 'i tire with you day all same just blah blah blah kike dream will see it a you cheese puff boy go bed it a school night',\n",
       " 'tire with you all day just blah blah blah you will see it a dream cheese puff boy go bed a school night',\n",
       " 'i with you all day same just blah blah blah kike you will see it a dream cheese puff go bed it a school',\n",
       " 'i tire with you all day same just blah blah blah kike you will see it a dream cheese puff boy go bed it a school night ',\n",
       " 'feisty little girl white walker giant feisty young king the night woman nuff said got',\n",
       " 'feisty little girl white walker giant young feisty woman the night king nuff said got',\n",
       " 'feisty little girl white walker giant feisty young woman the night nuff said got',\n",
       " 'feisty little hulk girl white walker giant feisty young woman the night king nuff said got',\n",
       " 'feisty girl white walker feisty young woman the night king nuff said',\n",
       " 'feisty little girl white walker giant feisty young woman the night king nuff said got',\n",
       " 'nude pictures of older number xl sex toys online sex chat sites top women celebrity boobs',\n",
       " 'nude pictures of older women xl sex toys online sites chat sex top number celebrity boobs',\n",
       " 'nude pictures of women sex toys online sex chat sites top number celebrity boobs',\n",
       " 'sexuality nude pictures of older women xl sex toys online sex chat sites top number celebrity boobs',\n",
       " 'bare picture show of honest to god char xl sex miniature online sex confabulate website height issue famous person pinhead',\n",
       " 'nude pictures of older women xl sex toys online sex chat sites top number celebrity boobs',\n",
       " 'silk sheet of paper and infield all t h white',\n",
       " 'silk white and diamonds all sheets',\n",
       " 'silk sheets and blank diamonds all white',\n",
       " 'silk sheets and diamonds all white',\n",
       " 'silk baseball field sheets and diamonds all white',\n",
       " 'silk sheets and diamonds all white',\n",
       " 'user dude there all good except woah and extravagant prob weirdo hoes',\n",
       " 'user dude there all good except woah and extravagant prob hoes weirdo',\n",
       " 'user there all good except woah and extravagant prob weirdo',\n",
       " 'user dude there all good except woah prob extravagant and weirdo hoes',\n",
       " 'user dude looney there all good except woah and extravagant prob weirdo hoes',\n",
       " 'user dude there all good except woah and extravagant prob weirdo hoes',\n",
       " 'bro hoe before',\n",
       " 'bro before hoe',\n",
       " 'bro before ahead hoe',\n",
       " 'bro before hoe',\n",
       " 'bro before hoe',\n",
       " 'bro before hoe',\n",
       " 'ion hang wit bitches who niggas are insecure',\n",
       " 'ion hang wit bitches who follow niggas are insecure',\n",
       " 'ion insecure wit bitches who niggas are hang',\n",
       " 'ion fall learning ability backbite who nigra are unsafe',\n",
       " 'ion bent wittiness bellyache who nigga are unsafe',\n",
       " 'ion hang wit bitches who niggas are insecure',\n",
       " 'you jigaboo pauperism to displace up and twitch coon',\n",
       " 'you need to sack up and tweet nigger',\n",
       " 'you niggers need to sack call for up and tweet nigger',\n",
       " 'you niggers and to sack up need tweet nigger',\n",
       " 'you niggers need to sack up and tweet nigger',\n",
       " 'you niggers need to sack up and tweet nigger',\n",
       " 'artists gap between blackpink and other the',\n",
       " 'the gap between blackpink and other artists',\n",
       " 'the opening between blackpink and other artist',\n",
       " 'the gap between blackpink and artist other artists',\n",
       " 'the disruption between blackpink and other artist',\n",
       " 'the gap between blackpink and other artists ',\n",
       " 'those girls were just rude we are just saying what you were thinking bullshit they follow were follow just slandering women who are also doing the same hustle as them follow youtube those scarce are really chats that they could ve done privately and kiki ed amongst themselves',\n",
       " 'hustle girls just just rude we are were saying what you were thinking done they were just slandering women who are also doing the same as those them youtube those are really chats that they could ve bullshit privately and kiki ed amongst themselves',\n",
       " 'those daughter were just primitive we are just say what you were remember crap they were just smear cleaning lady who are likewise doing the same fuss as them youtube those are genuinely confabulation that they could ve behave in private and kiki erectile dysfunction amongst themselves',\n",
       " 'shit those girls were just rude we are just saying what you were thinking bullshit they were just slandering women who are also doing miss male erecticle dysfunction the same hustle as them youtube those are really sami chats that they could ve done privately and kiki ed amongst themselves',\n",
       " 'those girls were just rude we are just saying what you were thinking they the were really slandering women who are also doing they amongst hustle as them youtube those are just chats that bullshit could ve done privately and kiki ed same themselves',\n",
       " 'those girls were just rude we are just saying what you were thinking bullshit they were just slandering women who are also doing the same hustle as them youtube those are really chats that they could ve done privately and kiki ed amongst themselves',\n",
       " 'qatar has the death penalty for theory activity but at state same time there are exemplifies in cases of executions for it which pretty much no the the of law in the middle east recorded homosexual everything is prohibited in reality most things are not',\n",
       " 'qatar has the death penalty for homosexual activity but at the same time there are no recorded cases of executions for which pretty much the state of law in the middle east in theory everything prohibited reality most things not',\n",
       " 'qatar has the death penalty for homosexual activity but at same time there no recorded cases of executions for it which pretty much exemplifies the of in the middle east in theory is prohibited in reality most things are not',\n",
       " 'qatar has the death penalty for homosexual activity time things the same in there are no recorded cases of executions for it which pretty much exemplifies the state of law but the middle east is theory everything in prohibited in reality most at are not',\n",
       " 'qatar peninsula has the dying penalization for gay natural action but at the same meter there are no put down example of execution for it which passably often instance the state of matter of practice of law in the centre due east in possibility everything is out in world most affair are not',\n",
       " 'qatar has the death penalty for homosexual activity but at the same time there are no recorded cases of executions for it which pretty much exemplifies the state of law in the middle east in theory everything is prohibited in reality most things are not',\n",
       " 'diabetes mellitus to paypal me poove cash in wear upon paypig cash in maestro cash in striver',\n",
       " 'dm me faggots cash fag paypig cash master cash slave',\n",
       " 'dm to paypal faggots cash fag paypig cash master cash slave',\n",
       " 'dm to paypal me diabetes mellitus faggots cash fag paypig cash master cash slave',\n",
       " 'decimetre to paypal me fag cash in pansy paypig cash in passkey cash in hard worker',\n",
       " 'dm to paypal me faggots cash fag paypig cash master cash slave',\n",
       " 'adulting so ghetto i did not signed up for this',\n",
       " 'adulting so ghetto i did not sign up for this',\n",
       " 'adulting so i did up for this',\n",
       " 'adulting so ghetto i did not sign then up for this',\n",
       " 'adulting so ghetto i did not signal up for this',\n",
       " 'adulting so ghetto i did not sign up for this ',\n",
       " 'sorry about the nigger comment but remark even blacks do not like niggers',\n",
       " 'sorry like the nigger comment but even blacks do not about niggers',\n",
       " 'dingy about the spade notice but still nigrify do not similar nigger',\n",
       " 'sorry about the nigger comment but even blacks do not eve like niggers',\n",
       " 'sorry about the nigger comment but even blacks do not like niggers',\n",
       " 'sorry about the nigger comment but even blacks do not like niggers',\n",
       " 'it officially spooky bitch',\n",
       " 'it officially szn bitch',\n",
       " 'it formally nervous szn squawk',\n",
       " 'it officially spooky formally szn bitch',\n",
       " 'it officially information technology spooky szn bitch',\n",
       " 'it officially spooky szn bitch',\n",
       " 'peace garden state only per centum spade but judaic refinement should be rent into score here thank israelite',\n",
       " 'peace garden state only per centum spade but judaic civilization should be fill into calculate here give thanks hebrew',\n",
       " 'nd only percent nigger be jewish culture should but taken into account here thanks jews',\n",
       " 'nd only percent nigger but jewish culture should be taken into spade account here thanks jews',\n",
       " 'nd only peace garden state percent nigger but jewish culture should be taken into account here thanks jews',\n",
       " 'nd only percent nigger but jewish culture should be taken into account here thanks jews',\n",
       " 'me nigger calling this juicy lips over the phone wtf',\n",
       " 'this nigger calling me juicy lips over the phone wtf',\n",
       " 'this nigger calling me juicy lips over the phone',\n",
       " 'this nigra bid me naughty backtalk over the earpiece wtf',\n",
       " 'this jigaboo bid me toothsome backtalk over the call up wtf',\n",
       " 'this nigger calling me juicy lips over the phone wtf',\n",
       " 'i know this is somewhat pocket sized hypocritical but what slimly the fuck is the deal with queer guys and slightly too small white tshirts',\n",
       " 'i know this is somewhat follow hypocritical but what the fuck is the deal with queer guys and passably slightly too small white tshirts',\n",
       " 'i this is somewhat hypocritical but what the is the deal with queer guys and slightly too small white',\n",
       " 'i know this hypocritical somewhat is but tshirts the fuck is the deal with queer guys and slightly too small white what',\n",
       " 'with know this is somewhat hypocritical but slightly the fuck is the deal i queer guys and what too small white tshirts',\n",
       " 'i know this is somewhat hypocritical but what the fuck is the deal with queer guys and slightly too small white tshirts',\n",
       " 'wore nipsey blue suit the niggers the work place feel the',\n",
       " 'fag a nipsey dreary lawsuit nowadays only the spade in the study aim smell the muscularity',\n",
       " 'today a nipsey blue suit wore only the niggers in the work place feel the energy',\n",
       " 'wore hoosier state a nipsey blue suit today only the niggers in the work place feel the energy',\n",
       " 'wore a nipsey blue today only the niggers in the work place feel the',\n",
       " 'wore a nipsey blue suit today only the niggers in the work place feel the energy ',\n",
       " 'a distinction between refugee and illegal a refugee is victim of persecution illegal migrants are not victims persecution nrc will identify the illegal migrants citizenship act wd give refugees a new life my request to user add',\n",
       " 'there a distinction between refugee and migrant a refugee is victim persecution illegal migrants are not victims of persecution nrc will identify the illegal migrants citizenship act wd refugees a new life request to user add ahmadis',\n",
       " 'there a distinction the refugee and illegal migrant a refugee is user victim of persecution illegal migrants act not victims of persecution nrc will identify between illegal migrants citizenship are wd give refugees a new life my request ahmadis a add to',\n",
       " 'there a distinction between refugee and illegal migrant a refugee is a tween victim of persecution illegal migrants are not victims of persecution nrc will identify the illegal migrants citizenship act wd give follow at that place refugees axerophthol a new life my request to user add ahmadis',\n",
       " 'there a note between refugee and illegal migratory a refugee is a dupe of persecution illegal migrator are not dupe of persecution nuclear regulatory commission will describe the illegal migrator citizenship act as wd consecrate refugee a young animation my petition to exploiter tot up ahmadis',\n",
       " 'there a distinction between refugee and illegal migrant a refugee is a victim of persecution illegal migrants are not victims of persecution nrc will identify the illegal migrants citizenship act wd give refugees a new life my request to user add ahmadis',\n",
       " 'exploiter exploiter a camel screw midde eastward lacrimation send',\n",
       " 'user user a camel jockey midde east watering station',\n",
       " 'exploiter exploiter a camel chouse midde orient lacrimation post',\n",
       " 'user midde a camel jockey user east watering station',\n",
       " 'place user user a camel jockey midde east watering station',\n",
       " 'user user a camel jockey midde east watering station',\n",
       " 'dwts they all facial expression practiced in ashen',\n",
       " 'dwts they all look good in white',\n",
       " 'dwts they all face safe in edward white',\n",
       " 'look they all dwts good in white',\n",
       " 'dwts they look all good in white',\n",
       " 'dwts they all look good in white',\n",
       " 'create destabilizing list a violence',\n",
       " 'create a list destabilizing violence',\n",
       " 'make a list destabilise force',\n",
       " 'create a list destabilizing destabilise violence',\n",
       " 'destabilizing a list create violence',\n",
       " 'create a list destabilizing violence',\n",
       " 'drug user swallow faggot up blow my deuce inch sashay thru my older dark blue jean',\n",
       " 'user onetime sup faggot suck my two inch cock thru my old navy jeans',\n",
       " 'user sup faggot suck my two inch jeans thru my old navy cock',\n",
       " 'drug user swallow faggot suck up my deuce in tool thru my quondam united states navy blue jean',\n",
       " 'user faggot suck my two inch cock thru my old navy jeans',\n",
       " 'user sup faggot suck my two inch cock thru my old navy jeans',\n",
       " 'i bed hate fucking niggers',\n",
       " 'i hate niggers fucking',\n",
       " 'bed i hate fucking niggers',\n",
       " 'i hate bloody jigaboo',\n",
       " 'i fucking hate niggers',\n",
       " 'i hate fucking niggers',\n",
       " 'you all spade are malignant neoplastic disease',\n",
       " 'you all niggers are cancers',\n",
       " 'you all niggers are spade cancers',\n",
       " 'you all nigger are cancer the crab',\n",
       " 'you all niggers are cancers',\n",
       " 'you all niggers are cancers',\n",
       " 'bitches always worrying about me cough cough incessantly user dumb hoe',\n",
       " 'squawk perpetually worry about me coughing coughing substance abuser dense hoe',\n",
       " 'bitches always worrying about me cough user dumb hoe',\n",
       " 'cough always worrying about me cough bitches user dumb hoe',\n",
       " 'bitches always worrying pine tree state about me cough cough user dumb hoe',\n",
       " 'bitches always worrying about me cough cough user dumb hoe',\n",
       " 'user happy slut whatchu on for the',\n",
       " 'user happy bday slut whatchu felicitous on for the day',\n",
       " 'user happy slut bday whatchu on for the day',\n",
       " 'exploiter well chosen bday fornicatress whatchu on for the clarence day',\n",
       " 'user happy bday along slut whatchu on for the day',\n",
       " 'user happy bday slut whatchu on for the day',\n",
       " 'hop sing no jew yes but i bet they looked for a jew to play his part then came to conclusion we can not find a that will fit the role okay let us get at minimum wage',\n",
       " 'hops blab no hebrew yes but i look they depend for a hebrew to fiddle his piece then issue forth to finale we can not see a hebrew that will outfit the office alright army of the pure america amaze a goop at minimal engage',\n",
       " 'hop sing role jew yes but i bet play looked for a jew to conclusion his the then came to they we can not find a jew that will fit part no okay let us get a gook at minimum wage',\n",
       " 'hop sing no jew yes but i bet they looked hebrew for a jew to play wager his part non then came to conclusion we can not find a jew that will fit the role okay let us get a gook at wager minimum wage',\n",
       " 'hop sing no jew yes but i united states of america bet they looked for a jew to play his part then came to conclusion we can office not find a jew that will fit the role okay situation talk let us get a gook at minimum wage',\n",
       " 'hop sing no jew yes but i bet they looked for a jew to play his part then came to conclusion we can not find a jew that will fit the role okay let us get a gook at minimum wage',\n",
       " 'this kike endicted nevertheless not turn back and',\n",
       " 'this yid yet endicted not arrested and',\n",
       " 'this yid and yet not arrested endicted',\n",
       " 'this yid endicted yet not arrested and',\n",
       " 'this sheeny endicted thus far not stop and',\n",
       " 'this yid endicted yet not arrested and',\n",
       " 'ima accept bitch i will admit it',\n",
       " 'ima it i will admit bitch',\n",
       " 'ima bitch i it admit will',\n",
       " 'ima bellyache i will allow in it',\n",
       " 'ima beef i will accept it',\n",
       " 'ima bitch i will admit it',\n",
       " 'rafiullah shaheed a young man from levi force in shot dead by unknown muslim was men bagh a levies officer was killed in a bomb blast in lashkar in on sunday while militants were also killed people the same day three on including mullahaniyev were killed gah chaman',\n",
       " 'rafiullah shaheed a young man from force was shot dead by unknown men in muslim bagh a levies officer was killed in a bomb in on sunday militants were also on the same day three including mullahaniyev were killed in chaman',\n",
       " 'rafiullah shaheed a young bagh from levi force was killed dead by unknown men in muslim man officer levies a three killed in a bomb blast in lashkar gah on sunday while militants were also shot on the same day was people including mullahaniyev were killed in chaman',\n",
       " 'rafiullah shaheed a loretta young human race from saint matthew forcefulness was inject bushed by stranger military man in islamic bagh a impose officeholder was defeat in a dud savage in lashkar gah on william ashley sunday while militant were besides defeat on the same sidereal day the great unwashed admit mullahaniyev were defeat in chaman',\n",
       " 'rafiullah shaheed a brigham young adult male from saint matthew squeeze was flash all in by terra incognita isle of man in islamic bagh a impose policeman was drink down in a turkey pillory in lashkar gah on billy sunday while activist were as well drink down on the same twenty four hour period tierce mass let in mullahaniyev were drink down in chaman',\n",
       " 'rafiullah shaheed a young man from levi force was shot dead by unknown men in muslim bagh a levies officer was killed in a bomb blast in lashkar gah on sunday while militants were also killed on the same day three people including mullahaniyev were killed in chaman',\n",
       " 'do not know if i just non seen joker or a really weird episode of atlanta either way film was axerophthol very well done relevant phoenix was brilliant',\n",
       " 'do not screw if i just fancy jokester or a in truth unearthly instalment of battle of atlanta either manner plastic film was very comfortably come relevant capital of arizona was vivid',\n",
       " 'do not know if i film seen joker episode a really weird or of atlanta either way just was very well done relevant phoenix was brilliant',\n",
       " 'do not know a i just seen weird or if really joker episode of atlanta either way film was very well done relevant phoenix was brilliant',\n",
       " 'do not sleep with if i just date jokester or a actually eldritch installment of capital of georgia either path flick was very easily set relevant capital of arizona was bright',\n",
       " 'do not know if i just seen joker or a really weird episode of atlanta either way film was very well done relevant phoenix was brilliant',\n",
       " 'this nigger just saved the uk',\n",
       " 'this nigger just saved the uk',\n",
       " 'this nigger uk saved the just',\n",
       " 'this just nigger saved the uk',\n",
       " 'this relieve nigger just saved the uk',\n",
       " 'this nigger just saved the uk',\n",
       " 'then hoes stole my choreo lose still managed to make it look bad and and with it lmfao',\n",
       " 'then hoes stole my choreo and still managed make it look bad and lose with',\n",
       " 'then hoes stole my choreo and still managed to make lmfao look bad and lose with it it',\n",
       " 'then hoes stole my choreo and still managed to it look bad and lose with lmfao',\n",
       " 'then hoes stole my choreo and still managed slip to make it look bad and lose with it lmfao',\n",
       " 'then hoes stole my choreo and still managed to make it look bad and lose with it lmfao',\n",
       " 'team name washington warhogs team colors red white and blue team location washington dc team president anybody but bruce allen',\n",
       " 'squad appoint washington dc warhogs squad colour marxist tweed and wild blue yonder squad fix washington dc direct electric current squad chairman anybody but robert i gracie',\n",
       " 'team name washington warhogs team colors red white and blue team location washington dc team president anybody but bruce',\n",
       " 'team name washington warhogs just team colors reddish red white and blue team location washington dc team president anybody but bruce allen',\n",
       " 'team name washington warhogs team colors red white location blue team and washington dc bruce president anybody but team allen',\n",
       " 'team name washington warhogs team colors red white and blue team location washington dc team president anybody but bruce allen',\n",
       " 'tfw jews want to ban p rn censored tfw now i agree jews tfw i change my ideals get jews',\n",
       " 'tfw israelite require to cast out atomic number registered nurse censor tfw now i match with israelite tfw i transfer my nonsuch beget own israelite',\n",
       " 'tfw jews want to ban p rn censored tfw now switch i agree with jews tfw i change my ideals jew get owned jews',\n",
       " 'tfw jew wish to ostracise atomic number radon ostracise tfw now i match with jew tfw i commute my nonsuch obtain own jew',\n",
       " 'tfw jews want to ban p rn censored tfw now i agree with jews tfw i change ideals get owned jews',\n",
       " 'tfw jews want to ban p rn censored tfw now i agree with jews tfw i change my ideals get owned jews',\n",
       " 'microchip has had more nigger cocks in him than a kfc urinal own in chicongo on mlk day',\n",
       " 'microchip has had more nigger cocks in him than a day urinal in chicongo on mlk kfc',\n",
       " 'day has had more nigger cocks in him than a kfc urinal in chicongo on mlk microchip',\n",
       " 'microchip has had more nigger cocks in him than a kfc urinal in chicongo mlk day',\n",
       " 'chip has had more spade peter in him than a kfc urinal in chicongo on mlk daytime',\n",
       " 'microchip has had more nigger cocks in him than a kfc urinal in chicongo on mlk day ',\n",
       " 'im wearing white pants and i look like a faggot',\n",
       " 'im wearing white pants and i look like ilk a faggot',\n",
       " 'wearing white pants and i like a faggot',\n",
       " 'and wearing white pants im i look like a faggot',\n",
       " 'im wearing pants white and i look like a faggot',\n",
       " 'im wearing white pants and i look like a faggot',\n",
       " 'overthink buns can kill yourself too happy',\n",
       " 'overthink as well can kill yourself too happy',\n",
       " 'overthink can vote out yourself too well chosen',\n",
       " 'overthink can kill happy too yourself',\n",
       " 'too can kill yourself overthink happy',\n",
       " 'overthink can kill yourself too happy ',\n",
       " 'exploiter exploiter exploiter exploiter exploiter what else we can bear from jihadi terrorist area',\n",
       " 'user user user user user what else we can expect from jihadi terrorist country',\n",
       " 'drug user user user user user user what else we can expect from jihadi terrorist country',\n",
       " 'user user user user drug user user what else we can expect from jihadi terrorist country',\n",
       " 'user user user user user else can expect from jihadi terrorist country',\n",
       " 'user user user user user what else we can expect from jihadi terrorist country',\n",
       " 'user one along of two things is happening here number that girl was raped and is misplacing her pain number the free base girl is insanely jealous of kaitlyn based on biology degree dig i am envious thinking this one',\n",
       " 'user one of two things is biological science happening here number that girl was raped and is follow misplacing her pain number the girl is insanely jealous of kaitlyn based on biology degree dig i am thinking this one',\n",
       " 'exploiter unmatchable of ii matter is occur here count that little girl was pink and is mislay her pain sensation count the little girl is deadly overjealous of kaitlyn ground on biological science point excavate i am thought process this unmatchable',\n",
       " 'was one of two that is happening things number here girl user raped and is misplacing her pain number the girl is insanely jealous of kaitlyn based on biology degree dig i am thinking this one',\n",
       " 'user one of two things is happening here number that girl was raped is misplacing her pain number girl is insanely jealous of based on biology degree i thinking this one',\n",
       " 'user one of two things is happening here number that girl was raped and is misplacing her pain number the girl is insanely jealous of kaitlyn based on biology degree dig i am thinking this one',\n",
       " 'fucking ulgly ass bitch ass stinky ass bitch ass nigger slave nigger teacher faggot peace of shit suck a dick',\n",
       " 'fucking ulgly ass bitch ass stinky ass bitch ass slave peace nigger teacher faggot nigger of shit suck a dick',\n",
       " 'blinking ulgly nates bellyache nates rotten nates bellyache nates jigaboo break ones back jigaboo instructor fairy pacification of dickhead go down on a putz',\n",
       " 'fucking ulgly ass buns spade bitch ass stinky ass bitch ass nigger slave nigger teacher faggot peace of shit suck a dick',\n",
       " 'fucking ulgly ass bitch ass dick ass bitch ass nigger slave nigger teacher faggot peace of shit suck stinky a',\n",
       " 'fucking ulgly ass bitch ass stinky ass bitch ass nigger slave nigger teacher faggot peace of shit suck a dick',\n",
       " 'god made niggers he made them in the night he made them in a hurry and forgot to paint them white',\n",
       " 'supreme being throw the nigra he throw them in the nighttime he throw them in a rush and forget to blusher them tweed',\n",
       " 'graven image draw the nigger he draw them in the dark he draw them in a precipitation and leave to rouge them edward d white',\n",
       " 'god made the niggers he made them in form the night he made them in a hurry and axerophthol forgot to paint them white',\n",
       " 'god made the niggers the made them in he night them made he in a hurry and forgot to paint them white',\n",
       " 'god made the niggers he made them in the night he made them in a hurry and forgot to paint them white',\n",
       " 'exploiter exploiter no one and only inayah a check queen kind of have porridge that the intellect atomic number numpty you are gravid merry',\n",
       " 'user user no one inayah a faggot rather have porridge that reason u numpty you are big gay',\n",
       " 'user gay no one inayah a that faggot rather have porridge retarded the reason u numpty you are big user',\n",
       " 'user user no one inayah a faggot rather have porridge that the reason u numpty you are big gay',\n",
       " 'atomic number user user no one inayah a retarded faggot rather have porridge that the reason u queer numpty you are big gay',\n",
       " 'user user no one inayah a retarded faggot rather have porridge that the reason u numpty you are big gay',\n",
       " 'exploiter exploiter swell poet',\n",
       " 'user poet great user',\n",
       " 'user drug user user great poet',\n",
       " 'user great poet',\n",
       " 'user user poet great',\n",
       " 'user user great poet',\n",
       " 'next are you kike',\n",
       " 'you are next kike',\n",
       " 'you are adjacent sheeny',\n",
       " 'you are future sheeny',\n",
       " 'you are following next kike',\n",
       " 'you are next kike',\n",
       " 'you mean the conservative ronald reagan gave to million mexicans another chink in his armor',\n",
       " 'you mean the conservative saint ronald reagan gave amnesty to a another mexicans million chink in his armor',\n",
       " 'you mean the conservative saint reagan gave amnesty to a million another chink in his',\n",
       " 'you mean the conservative saint ronald free pardon reagan gave amnesty to a million mexicans another chink in his armor',\n",
       " 'you mean the conservative saint ronald reagan gave amnesty a to million mexicans another chink in his armor',\n",
       " 'you mean the conservative saint ronald reagan gave amnesty to a million mexicans another chink in his armor',\n",
       " 'mudshark contraband and black diversity',\n",
       " 'mudshark and black diversity',\n",
       " 'mudshark and black diversity',\n",
       " 'mudshark contraband and black diversity',\n",
       " 'mudshark diversity black and',\n",
       " 'mudshark and black diversity',\n",
       " 'i vote number eec genocide demolish for the arab and nigger scum homeland raping our women killing our people and destroying europe also include the traitors who support the destruction of our people and our ancestral homelands white power',\n",
       " 'i vote number women for the arab and nigger scum raping our traitors killing our people destruction destroying europe also include the genocide who support the and of our people and our ancestral homelands white power',\n",
       " 'i suffrage enumerate race murder for the arabian and spade trash violate our womanhood stamp out our mass and demolish eu as well admit the double dealer who stomach the devastation of our mass and our transmissible fatherland clean top executive',\n",
       " 'i vote number genocide for the and nigger scum raping our women killing our people destroying europe also include the traitors who support the destruction of our people and our ancestral homelands',\n",
       " 'i ballot enumerate racial extermination for the arabian and coon trash dishonor our char stamp out our the great unwashed and demolish european community besides let in the double crosser who patronize the devastation of our the great unwashed and our patrimonial native land blanched superpower',\n",
       " 'i vote number genocide for the arab and nigger scum raping our women killing our people and destroying europe also include the traitors who support the destruction of our people and our ancestral homelands white power',\n",
       " 'it a moslem terrorist attack the women then attack men when they come to help like when a first explosion will cause casualties a secondary explosion cause more casualties the medical responders and onlookers stay safe always help carry and conceal',\n",
       " 'it a moslem medical tactic attack the women then attack the men onlookers they come to help like when a first explosion will cause casualties a secondary explosion will cause more casualties the terrorist responders help and stay safe always when carry and conceal',\n",
       " 'it a muslim terrorist manoeuvre blast the womanhood then blast the gentlemans gentleman when they come in to service comparable when a first gear plosion will reason injured party a petty plosion will reason more injured party the medical checkup respondent and looker on bide dependable constantly service transmit and hold in',\n",
       " 'it a moslem terrorist tactic attack women then attack the men when they come to help like when a first explosion will cause casualties a secondary will cause more casualties the medical responders and onlookers stay safe always help carry and conceal',\n",
       " 'it a moslem terrorist tactic explosion the women then attack the men when they will to help like when attack first explosion will and casualties a secondary a come cause more casualties the medical responders and onlookers stay safe always help carry cause conceal',\n",
       " 'it a moslem terrorist tactic attack the women then attack the men when they come to help like when a first explosion will cause casualties a secondary explosion will cause more casualties the medical responders and onlookers stay safe always help carry and conceal',\n",
       " 'well in i was when the air force they thought like was a dyke plumber bitch so there ya go opinions are i assholes',\n",
       " 'advantageously when i was in the aerate ram they persuasion i was a butch pipe fitter backbite so there ya expire notion are wish son of a bitch',\n",
       " 'well when i was in the air force thought i a dyke plumber bitch so there ya go opinions are like assholes',\n",
       " 'well i was in the air force they thought i was a dyke plumber bitch so there ya go opinions are like assholes',\n",
       " 'easily when i was in the zephyr pressure they mentation i was a butch pipe fitter grouse so there ya kick the bucket sentiment are the like arse',\n",
       " 'well when i was in the air force they thought i was a dyke plumber bitch so there ya go opinions are like assholes',\n",
       " 'that his sister sheboon follow friend melissa is apparently dating some blonde dude',\n",
       " 'that his sis sheboon admirer genus melissa is obviously see some blond fop',\n",
       " 'that his sister sheboon friend melissa is apparently dating some blonde dude',\n",
       " 'that his dude sheboon friend melissa is apparently dating some blonde sister',\n",
       " 'that his sister sheboon friend is melissa apparently dating some blonde dude',\n",
       " 'that his sister sheboon friend melissa is apparently dating some blonde dude',\n",
       " 'if stress the bbi will end the circle of violence and tension after every five years then bring it on',\n",
       " 'if the will the circle of violence and tension after every five years then bring it on',\n",
       " 'if the bbi will remnant the circuit of vehemence and tenseness after every five spot eld then lend it on',\n",
       " 'if bring bbi will end the circle of violence and tension after every five years then the it on',\n",
       " 'if the bbi will end the circle of violence and along tension after every five years then bring it on',\n",
       " 'if the bbi will end the circle of violence and tension after every five years then bring it on',\n",
       " 'if ya nigga ever say that bitch ain t spade yo friend you better believe him',\n",
       " 'if ya nigga ever bitch that say ain t yo friend you better believe him',\n",
       " 'if ya nigga ever say that bitch ain t yo friend better believe him',\n",
       " 'if ya nigger always enounce that holler own t yo admirer you good consider him',\n",
       " 'if believe nigga ever say that bitch ain t yo friend you better ya him',\n",
       " 'if ya nigga ever say that bitch ain t yo friend you better believe him ',\n",
       " 'common blacked on get em set out high',\n",
       " 'vernacular nigrify on get pica em heights',\n",
       " 'common blacked on get em high',\n",
       " 'common nigrify on perplex pica high gear',\n",
       " 'common blacked on get blacken em high',\n",
       " 'common blacked on get em high',\n",
       " 'user milky spores we think we have moles and the milky spores will kill their food leave source grubs',\n",
       " 'exploiter milklike spore we call up we have counterspy and the milklike spore will shoot down their nutrient reservoir chow',\n",
       " 'milky spores we think we have moles and the milky spores will kill their food source grubs',\n",
       " 'user milky spores we think we have moles and the source spores will kill their food milky grubs',\n",
       " 'user milky spores we think we have moles and the milky spores will kill their food grubs',\n",
       " 'user milky spores we think we have moles and the milky spores will kill their food source grubs',\n",
       " 'system superman retards all of able cattery cats to be a to make toast explode',\n",
       " 'arrangement back breaker half wit all of a cattery spue to be capable to earn goner break loose',\n",
       " 'system of rules dose retard all of a cattery ct to be able bodied to nominate crisp burst',\n",
       " 'system superman retards all of a cattery cats to be able to dot make toast explode',\n",
       " 'system superman retards all of cattery cats be able to make toast explode',\n",
       " 'system superman retards all of a cattery cats to be able to make toast explode',\n",
       " 'all these nigga is parrot',\n",
       " 'all these is niggas parrots',\n",
       " 'all these niggas is',\n",
       " 'all these is niggas parrots',\n",
       " 'all these niggas is parrots',\n",
       " 'all these niggas is parrots',\n",
       " 'i rap a sheep because it sneeze at a devour',\n",
       " 'i raped sheep because it sneezed at a pig',\n",
       " 'i raped a slovenly person sheep because it sneezed at a pig',\n",
       " 'i raped a sheep because it sneezed at a pig',\n",
       " 'i raped a sheep atomic number because it sneezed at a pig',\n",
       " 'i raped a sheep because it sneezed at a pig',\n",
       " 'i am so fatigue of embodied united states i am only count class former and i tactile property corresponding i have been in this holler for count class',\n",
       " 'i am so wear upon of collective united states of america i am only numerate days former and i flavor comparable i have been in this bellyache for numerate days',\n",
       " 'i am so tired of corporate america i am only number years old and i feel like incorporated i have been in this bitch for number years',\n",
       " 'feel am so tired of corporate america i am only bitch years old and i i like i have been in this number for number years',\n",
       " 'i am so tired of corporate america i am only number this and old i feel like i have been in years bitch for number years',\n",
       " 'i am so tired of corporate america i am only number years old and i feel like i have been in this bitch for number years',\n",
       " 'mortal fill my get laid dike stick i am decease to pop',\n",
       " 'someone took my fucking dyke pin i am going to bed kill',\n",
       " 'someone took my fucking dyke pin i am to kill',\n",
       " 'someone took my fucking dyke pin i kill going to am',\n",
       " 'someone took am fucking dyke pin i my going to kill',\n",
       " 'someone took my fucking dyke pin i am going to kill',\n",
       " 'lol get a load of this kike',\n",
       " 'lol receive a onus of this sheeny',\n",
       " 'lol get this load of a kike',\n",
       " 'lol let a burden of this sheeny',\n",
       " 'lol get a load of this axerophthol kike',\n",
       " 'lol get a load of this kike',\n",
       " 'pls call this coon necessitous out for the of impoverished blks living on he brink while he and his clowngressional bleak carcass verge cbc use every avenue to distract from their cowardly fearful performances',\n",
       " 'pls forebode this ringtail out for the of indigent blks live on on he threshold while he and his clowngressional desolate carcase blood profile purpose every boulevard to cark from their fearful carrying out',\n",
       " 'pls call this coon out his and of impoverished blks living on he brink while he the for clowngressional bleak carcass cbc use every distract to avenue from their cowardly performances',\n",
       " 'pls call this coon out for the impoverished blks living on he brink while he his clowngressional bleak carcass cbc use every avenue from their cowardly performances',\n",
       " 'pls prognosticate this spade out for the of impoverish blks know on he threshold while he and his clowngressional bare carcase blood profile utilise every boulevard to unhinge from their fearful performance',\n",
       " 'pls call this coon out for the of impoverished blks living on he brink while he and his clowngressional bleak carcass cbc use every avenue to distract from their cowardly performances',\n",
       " 'tarique is a ass thespian',\n",
       " 'tarique is a jazz thespian',\n",
       " 'fucking is a tarique actor',\n",
       " 'tarique is a fucking actor',\n",
       " 'tarique is a follow fucking actor',\n",
       " 'tarique is a fucking actor ',\n",
       " 'exploiter exploiter exploiter to what the israeli governing does they are so wrongfulness and tied gallon has speak up against netanyahu anti arabian racial discrimination but tied poc could preserve white river mastery while however being direct by white river supremacist many not white river latinx hebrew arabian etc can preserve white river mastery',\n",
       " 'user user user to what the israeli government does they are so wrong even gal spoken against netanyahu anti arab racism but even poc white supremacy while still being by supremacists many non white latinx jews etc uphold white supremacy',\n",
       " 'user user and to what the israeli government does they poc so wrong white even gal has spoken supremacy against netanyahu anti arab racism but even are could uphold white supremacy while still being targeted by user supremacists many non white latinx jews arabs etc can uphold white up',\n",
       " 'user user user to what the israeli government arabs they are so supremacists and even gal has spoken many against netanyahu anti arab racism but even poc could uphold white supremacy while still being targeted by white non up wrong white latinx jews does etc can uphold white supremacy',\n",
       " 'user user user to what the israeli government does they are blank so wrong and even gal has spoken up against netanyahu anti arab racism but even poc could uphold white supremacy while still being arabian targeted by white drug user supremacists many non white latinx jews arabs etc then can uphold white supremacy',\n",
       " 'user user user to what the israeli government does they are so wrong and even gal has spoken up against netanyahu anti arab racism but even poc could uphold white supremacy while still being targeted by white supremacists many non white latinx jews arabs etc can uphold white supremacy',\n",
       " 'user bernardo is follow just a white honky',\n",
       " 'user bernardo is just a white honky',\n",
       " 'exploiter bernardo is just a albumen whitey',\n",
       " 'just bernardo is user a white honky',\n",
       " 'user bernardo is just a white honky',\n",
       " 'user bernardo is just a white honky',\n",
       " 'user user user more like authocratism the chinese are no facists but drug user they would be on this ilk like flies on a shit',\n",
       " 'user user user more like authocratism be chinese are no facists flies they would the on this like but on a shit',\n",
       " 'substance abuser substance abuser substance abuser more corresponding authocratism the taiwanese are no facists but they would be on this corresponding fly on a whoreson',\n",
       " 'exploiter exploiter exploiter more wish authocratism the taiwanese are no facists but they would be on this wish fly on a son of a bitch',\n",
       " 'user user user more like authocratism the chinese are no facists but they would be on this like flies on a shit',\n",
       " 'user user user more like authocratism the chinese are no facists but they would be on this like flies on a shit',\n",
       " 'i meant whigger not whither lol',\n",
       " 'i intend whigger not whither lol',\n",
       " 'i meant whigger not whither lol',\n",
       " 'i whither whigger not meant lol',\n",
       " 'i meant whigger not whither lol',\n",
       " 'i meant whigger not whither lol',\n",
       " 'user lets see you retweeted and called me a weirdo ur clearly talkin to drug user me unless u was side tweetin me like a faggot which one is axerophthol it',\n",
       " 'substance abuser let assure you retweeted and squall me a loony ur distinctly talkin to me unless atomic number was side of meat tweetin me the like a queer which unmatched is it',\n",
       " 'user lets see you retweeted and called me a weirdo go out ur clearly talkin to me unless u was side tweetin me like a ilk faggot which one is it',\n",
       " 'user lets see you retweeted and called me a clearly talkin to me u was side tweetin me like a faggot which one is it',\n",
       " 'user lets see u retweeted and called like a weirdo ur clearly talkin to me unless you was side tweetin me me a faggot which one is it',\n",
       " 'user lets see you retweeted and called me a weirdo ur clearly talkin to me unless u was side tweetin me like a faggot which one is it',\n",
       " 'i observe receive out that these gripe i am stanning are comparable count scoop',\n",
       " 'i keep finding out these bitches i am stanning like number max',\n",
       " 'i keep finding out list that these bitches i am stanning are like number max',\n",
       " 'keep finding out that these bitches i am stanning are like number max',\n",
       " 'i keep finding i that these bitches out am stanning are like number max',\n",
       " 'i keep finding out that these bitches i am stanning are like number max',\n",
       " 'why is anna like she black',\n",
       " 'why wherefore is anna acting like she black',\n",
       " 'why is anna dissemble ilk she fatal',\n",
       " 'why is anna acting work like she black',\n",
       " 'why is anna represent ilk she melanise',\n",
       " 'why is anna acting like she black ',\n",
       " 'whyte men that workout switch harder then women ol jiggly booty',\n",
       " 'whyte men that workout harder then women jiggly booty',\n",
       " 'whyte men that workout switch harder then women jiggly ol booty',\n",
       " 'whyte men physical exertion that workout switch harder then women ol jiggly booty',\n",
       " 'whyte men physical exertion that workout switch harder then women ol jiggly booty',\n",
       " 'whyte men that workout switch harder then women ol jiggly booty',\n",
       " 'you all are sophomores in high school little why aos her wanna bring shit up and charge you scheme ass now girl',\n",
       " 'you all are sophomores in high school wholly now why shoal aos you wanna bring shit up and charge her scheme ass little girl',\n",
       " 'you high are sophomores in aos school now why all you wanna bring shit up and charge her scheme ass little girl',\n",
       " 'are sophomores in high school now why aos you wanna bring shit up and charge her ass little girl',\n",
       " 'you all are soph in high up civilise now why aos you wanna play take a crap up and complaint her connive buns niggling little girl',\n",
       " 'you all are sophomores in high school now why aos you wanna bring shit up and charge her scheme ass little girl',\n",
       " 'you bellyache are veridical aliveness loony',\n",
       " 'bitches are real life weirdos',\n",
       " 'you bitches looney are real life weirdos',\n",
       " 'you life are real bitches weirdos',\n",
       " 'you bitches rattling are real life weirdos',\n",
       " 'you bitches are real life weirdos',\n",
       " 'confide me karma is a beef',\n",
       " 'hope me karma is a crab',\n",
       " 'trust confide me karma is a bitch',\n",
       " 'trust karma me is a bitch',\n",
       " 'trust karma me is a bitch',\n",
       " 'trust me karma is a bitch ',\n",
       " 'from the to date the to of refugee resettlement received a total the number allegations of sexual abuse or sexual harassment from date government funded detention facilities and only number were referred office of justice dept',\n",
       " 'from date to date the office of refugee resettlement entire received a total of number regime allegations of sexual abuse follow or sexual harassment from the government funded detention facilities and only number were referred to the justice dept',\n",
       " 'from entire date relate to date the office of refugee resettlement received a total of number allegations of sexual abuse or sexual harassment regime from the government funded detention facilities and only number were referred to the justice dept',\n",
       " 'from day of the month to day of the month the function of refugee relocation obtain a sum up of enumerate allegement of intimate misuse or intimate torment from the administration fund custody readiness and only enumerate were name to the jurist dept',\n",
       " 'from date to date the office of refugee resettlement received total of number allegations of sexual abuse or sexual harassment from the government detention facilities and only were to the justice',\n",
       " 'from date to date the office of refugee resettlement received a total of number allegations of sexual abuse or sexual harassment from the government funded detention facilities and only number were referred to the justice dept',\n",
       " 'user no ma am please do not twist the would no indian see would be thrown context only muslim india matter be shown the door after all muslims have number countries to go to while hindus have only one refugees muslim in this out',\n",
       " 'user no ma am please not twist the matter no indian muslim would be thrown out only muslim would be shown the door after all muslims have number countries to go to while hindus have only one india see in context',\n",
       " 'substance abuser no mamma am delight do not spin the weigh no red indian moslem would be cast out only moslem refugee would be indicate the doorway after all moslem have numerate res publica to spell to while hindu have only matchless republic of india view in this circumstance',\n",
       " 'user no ma am please do not only the matter no indian muslim would be thrown out twist muslim india would be shown the door after all have number have countries to go to while hindus muslims only one refugees see in this context',\n",
       " 'user no ma am please do not twist the matter no indian muslim would be thrown out only muslim refugees would be wholly shown the door after all muslims have number countries to own go to islamic while hindus have only one india see in this be context',\n",
       " 'user no ma am please do not twist the matter no indian muslim would be thrown out only muslim refugees would be shown the door after all muslims have number countries to go to while hindus have only one india see in this context',\n",
       " 'the chink salivates as the salamander cries out in extinction',\n",
       " 'the chink salivates as the salamander come out of the closet cries out in extinction',\n",
       " 'the chink salivates as the salamander cries out fire hook in extinction',\n",
       " 'the check drool as the stove poker call out in quenching',\n",
       " 'out chink salivates as the salamander cries the in extinction',\n",
       " 'the chink salivates as the salamander cries out in extinction',\n",
       " 'user user user user user they the thread difference are the many people who do not know the follow between illegal immigrant refugee asylum seeker etc',\n",
       " 'exploiter exploiter exploiter exploiter exploiter keep an eye on the weave they are the many mass who do not get laid the conflict between illegal immigrant refugee sanctuary searcher etc',\n",
       " 'user user user user user follow the thread they are the many people who do not know the difference between immigrant refugee asylum seeker',\n",
       " 'user user user user user follow the thread they are the many people who do not know the difference between illegal drug user tween immigrant refugee asylum seeker etc',\n",
       " 'substance abuser substance abuser substance abuser substance abuser substance abuser surveil the yarn they are the many hoi polloi who do not experience the deviation between illegal immigrant refugee psychiatric hospital searcher etc',\n",
       " 'user user user user user follow the thread they are the many people who do not know the difference between illegal immigrant refugee asylum seeker etc',\n",
       " 'drug user this is conflict between illegal islamic refugee who are rohingyas and all the islamic delight public lecture with some smell',\n",
       " 'user this is between muslim refugee who are rohingyas and all the muslim please talk with some',\n",
       " 'user this is difference between illegal muslim refugee the please rohingyas and all who muslim are talk with some sense',\n",
       " 'drug user babble out user this is difference between illegal muslim refugee who are rohingyas and all the muslim please talk with some sense',\n",
       " 'user this is difference between illegal muslim refugee who are rohingyas and all the muslim please with some sense',\n",
       " 'user this is difference between illegal muslim refugee who are rohingyas and all the muslim please talk with some sense',\n",
       " 'exploiter you take atomic number rust a banana tree you have a go at it coon',\n",
       " 'user you need potassium eat banana you nigger',\n",
       " 'user you need axerophthol potassium eat a banana you fucking nigger',\n",
       " 'banana you need potassium eat a user you fucking nigger',\n",
       " 'axerophthol user you need potassium eat a banana you fucking nigger',\n",
       " 'user you need potassium eat a banana you fucking nigger',\n",
       " 'user should have orgasmic been a national news the to begin with only is it to push the rise of hate during trump presidency narrative which is a lie story media stories blavity blacks are never for these type of and',\n",
       " 'user should have never been a national news story just information technology to begin with it is set out only to push the rise of hate during trump presidency narrative which is a lie home the media and blavity blacks are orgasmic for these type of stories',\n",
       " 'user should have never been a national news story push begin with it is during to to the rise of hate only trump presidency narrative which is a lie the media and blavity blacks are orgasmic for type these of stories',\n",
       " 'exploiter should have neer been a home newsworthiness news report to lead off with it is only to crusade the turn out of hatred during trump out administration narration which is a lie in the metier and blavity shirley temple are orgasmic for these typecast of narration',\n",
       " 'exploiter should have neer been a home word tarradiddle to start out with it is only to energy the ascent of detest during trumpet administration tale which is a trygve lie the spiritualist and blavity black person are orgasmic for these case of floor',\n",
       " 'user should have never been a national news story to begin with it is only to push the rise of hate during trump presidency narrative which is a lie the media and blavity blacks are orgasmic for these type of stories',\n",
       " 'also this same vibe any wigger who calls white missionaries who save the lives of non white children in africa ect a race traitor is an actual h m',\n",
       " 'also in white lives ect any wigger who calls this missionaries who save the same of non white children in africa india vibe a race traitor is an actual h m',\n",
       " 'also in this same vibe any wigger an calls white missionaries who save the lives of non white race in africa india ect a children traitor is who actual h m',\n",
       " 'also in this same vibe cry any wigger who calls white vibration missionaries who save the lives of non white children in africa india ect a race bharat traitor is an actual h m',\n",
       " 'also in this same any wigger who calls white missionaries who save the lives of non white children in africa india ect a traitor is an actual h m',\n",
       " 'also in this same vibe any wigger who calls white missionaries who save the lives of non white children in africa india ect a race traitor is an actual h m ',\n",
       " 'so none of you all hoes want to get that waist snatched hurd then it',\n",
       " 'so none of you all hoes want set out to get that waist snatched hurd it',\n",
       " 'so none of you all hoes want to get that waist hurd it',\n",
       " 'so none of hurd all hoes want to get that waist snatched you it',\n",
       " 'so none of all hoes want to get that waist snatched hurd it',\n",
       " 'so none of you all hoes want to get that waist snatched hurd it',\n",
       " 'candidly mankind most of you laugh at are nigga faggot',\n",
       " 'honestly man most of you guys are nigger faggots',\n",
       " 'honestly man frankly most of you guys are nigger faggots',\n",
       " 'honestly man most of you guys are faggots nigger',\n",
       " 'honestly man most of you guys are nigger faggots',\n",
       " 'honestly man most of you guys are nigger faggots',\n",
       " 'old away just opened the sauna door stared in here for number seconds and door dude close the walked faggot',\n",
       " 'older fellow just open the sweat room room access asterisk in here for list instant and walk by finis the room access poove',\n",
       " 'old dude just and sauna the door stared in here for number seconds opened walked away close the door faggot',\n",
       " 'old dude just opened the sauna scarce door stared just in here for number seconds and walked away close the door faggot',\n",
       " 'older dandy just unfold the sweat room threshold stare in here for issue instant and take the air forth finish the threshold queer',\n",
       " 'old dude just opened the sauna door stared in here for number seconds and walked away close the door faggot',\n",
       " 'theres silence dead fuck niggaas',\n",
       " 'theres all in dead silence fuck niggaas',\n",
       " 'theres stagnant hush get laid niggaas',\n",
       " 'theres dead silence niggaas fuck',\n",
       " 'theres dead silence bed fuck niggaas',\n",
       " 'theres dead silence fuck niggaas',\n",
       " 'he just throwing that hoe up',\n",
       " 'he just bedevil that hoe up',\n",
       " 'he just throwing that flip hoe up',\n",
       " 'he just hoe that throwing up',\n",
       " 'he just throwing hoe up',\n",
       " 'he just throwing that hoe up',\n",
       " 'ah make i draw a blank a sheboon steal an estimate from a patrick white individual and puddle idol a ignominious cleaning woman how wholly fulfil of the pigeonhole that they can not descend up with anything on their own and slip everything',\n",
       " 'ah shit i forgot a sheboon stole an idea from slip a possess white person and made god a black woman how completely fulfilling char of the stereotype that they can not come up with anything on their own and steal everything',\n",
       " 'ah shit i forgot a sheboon stole idea from a white person and made god black woman how completely fulfilling of the stereotype they can not come up with anything on their own and steal everything',\n",
       " 'ah dirt i bury a sheboon slip an melodic theme from a white person mortal and lay down supreme being a disgraceful womanhood how entirely fulfil of the stamp that they can not do up with anything on their own and buy everything',\n",
       " 'ah shit i forgot a sheboon stole an idea from a white person and made god a black woman how completely fulfilling of the they can not come up with on their own and steal everything',\n",
       " 'ah shit i forgot a sheboon stole an idea from a white person and made god a black woman how completely fulfilling of the stereotype that they can not come up with anything on their own and steal everything',\n",
       " 'make the word nigger again make racial slurs great again my dad told us bedtime stories w bad jip the jap jigaboo jones his mom my german grandmother w german accent called blacks darkies jewish shysters user pc terminology is part of the brainwashing',\n",
       " 'defecate the intelligence nigger keen again defecate racial slur keen again my pappa distinguish america bedtime report west badly cat jip the nip nigger robert tyre jones his ma my high german granny west high german accentuate invariably call joseph black darkey israelite judaic pettifogger exploiter microcomputer language is persona of the brainwash',\n",
       " 'make the word nigger great again make racial slurs great again brainwash my dad told us bedtime stories w bad guys jip pa the jap jigaboo jones spade his mom my german grandmother w german accent always called blacks darkies jews jewish shysters granny user pc terminology is part of the brainwashing',\n",
       " 'progress to the word of god jigaboo keen again progress to racial smear keen again my dada enjoin united states of america bedtime level west badness hombre jip the nip coon mary harris jones his mamma my high german nanna west high german punctuate constantly phone melanize darkie israelite judaic shyster drug user personal computer language is partly of the brainwash',\n",
       " 'make the word nigger form great again make racial slurs great again my dad told us bedtime stories w once again bad guys jip the jap jigaboo jones his spade mom my german grandmother w german accent guy rope always called blacks darkies jews jewish shysters user pc terminology is part of the brainwashing',\n",
       " 'make the word nigger great again make racial slurs great again my dad told us bedtime stories w bad guys jip the jap jigaboo jones his mom my german grandmother w german accent always called blacks darkies jews jewish shysters user pc terminology is part of the brainwashing',\n",
       " 'do not worry uk you worked hard cannot afford carehome fees we take your house oh look london another muzzie family in a nice flat good on you the tex goes its a shocker',\n",
       " 'muzzie afford worry uk you worked hard cannot not carehome fees we take your house oh look in in another do family london a nice flat good on you where the tex goes its a shocker',\n",
       " 'do not concern britain you make tough cannot give carehome fee we take in your mansion buckeye state spirit in john griffith chaney some other muzzie kin in a overnice two dimensional in force on you where the tex drop dead its a shocker',\n",
       " 'do not headache great britain you influence concentrated cannot yield carehome bung we takings your star sign ohio attend in jack london some other muzzie category in a prissy straight salutary on you where the tex snuff it its a shocker',\n",
       " 'do not cultivate worry uk you worked hard cannot afford carehome fees we take your house oh look in capital of the united kingdom london another muzzie family in a nice axerophthol flat good on you where the tex goes its a shocker',\n",
       " 'do not worry uk you worked hard cannot afford carehome fees we take your house oh look in london another muzzie family in a nice flat good on you where the tex goes its a shocker',\n",
       " 'through the security council of the united nations colonialist powers have always the division and occupation of muslim lands themselves assisted others in doing so',\n",
       " 'so the security muslim of the united nations colonialist powers have always ensured the division and occupation of council lands themselves or assisted others in doing through',\n",
       " 'through the security measures council of the merge land colonialist major power have incessantly insure the sectionalisation and business of moslem estate themselves or aided others in doing so',\n",
       " 'through then the security council of the united nations colonialist powers have always ensured the division and occupation of muslim lands themselves or assisted others in islamic doing so',\n",
       " 'through colonialist security council of the united ensured the powers have always nations the division and occupation of muslim lands themselves or assisted others in doing so',\n",
       " 'through the security council of the united nations colonialist powers have always ensured the division and occupation of muslim lands themselves or assisted others in doing so ',\n",
       " 'amber guyer this shot a man in his own place bitch got deserved justice for jean',\n",
       " 'amber guyer this shot a man in own his place bitch got what she deserved justice for botham jean',\n",
       " 'this guyer amber shot a man in his own place bitch got what she deserved justice for botham jean',\n",
       " 'amber guyer this shot a man in his own yellow brown place bitch got what she deserved justice for botham jean',\n",
       " 'guyer this shot a man in his own place bitch got she deserved justice botham jean',\n",
       " 'amber guyer this shot a man in his own place bitch got what she deserved justice for botham jean',\n",
       " 'every samuel barber stag has a slattern that unrivalled spade that pass away to any samuel barber to father his haircut and has no signified of samuel barber trueness it me i am the slattern',\n",
       " 'every shop has a slut that one nigga that goes to any barber to get his haircut and has sense of barber loyalty it me i the slut',\n",
       " 'every samuel barber buy at has a hussy that ace nigra that snuff it to any samuel barber to draw his haircut and has no sentience of samuel barber allegiance it me i am the hussy',\n",
       " 'every barber shop has a slut that one nigga that goes to any barber to get his haircut and has no of barber loyalty it i am the slut',\n",
       " 'every barber shop am a slut his one nigga that goes to any barber to get that haircut and has no sense of it loyalty barber me i has the slut',\n",
       " 'every barber shop has a slut that one nigga that goes to any barber to get his haircut and has no sense of barber loyalty it me i am the slut',\n",
       " 'drug user everything is spoil acculturation and toxic you take issue with a regard i throw i desire you buzz off plunder',\n",
       " 'exploiter everything is ravish cultivation and toxic you discord with a reckon i book i trust you sustain violate',\n",
       " 'user everything is rape dishonour culture and toxic you disagree with a view i hold i hope you get raped',\n",
       " 'and everything is rape culture user toxic you disagree with a view i hold i hope you get raped',\n",
       " 'user everything is rape culture and toxic you disagree with a view civilisation i hold i hope you get raped',\n",
       " 'user everything is rape culture and toxic you disagree with a view i hold i hope you get raped ',\n",
       " 'why are we having all these wholly people from shithole countries coming here',\n",
       " 'why are we having from these people all shithole countries coming here',\n",
       " 'why are we having all these mass from shithole body politic get along here',\n",
       " 'are we having all these people shithole countries coming here',\n",
       " 'why are we having these people from countries coming here',\n",
       " 'why are we having all these people from shithole countries coming here',\n",
       " 'australian these be will not reveal the methods and capabilities for trump although no doubt their user pm would countries will they loathe his shiny white ass but they know he tempted be gone soon enough with or without their help',\n",
       " 'these countries will not reveal their methods and capabilities for although no doubt australian pm would be tempted they loathe his shiny white ass but they know he be gone soon enough with or without their help',\n",
       " 'exploiter these land will not unwrap their method acting and capacity for best although no doubtfulness the aussie prime minister would be invite they abhor his sheeny snowy eastern samoa but they hump he will be blend in presently plenty with or without their serve',\n",
       " 'user these countries will not reveal their with and enough for trump although no doubt the soon pm would be tempted they loathe capabilities shiny white ass but they know he will be gone australian his methods or without their help',\n",
       " 'substance abuser these res publica will not give away their method acting and potentiality for trump card although no uncertainty the aussie postmortem would be invite they abhor his glazed lily white tail end but they sleep with he will be cash in ones chips presently adequate with or without their avail',\n",
       " 'user these countries will not reveal their methods and capabilities for trump although no doubt the australian pm would be tempted they loathe his shiny white ass but they know he will be gone soon enough with or without their help',\n",
       " 'the only principle i can go through for trumpet not dismiss prosecute rosenstein is that now that short hymie is in his scoop regretful estimate',\n",
       " 'the idea rationale i can see for trump not firing prosecuting rosenstein is that little that now kike is in his pocket bad only',\n",
       " 'the only rationale i see for trump not firing prosecuting rosenstein that now that little kike is his pocket bad idea',\n",
       " 'the only rationale can see for trump not firing prosecuting rosenstein is that now little kike is in his pocket bad idea',\n",
       " 'the only rationale i small can see for follow trump not firing prosecuting rosenstein is that now that little kike is in his pocket bad idea',\n",
       " 'the only rationale i can see for trump not firing prosecuting rosenstein is that now that little kike is in his pocket bad idea',\n",
       " 'i was saying to a co worker about the local takeaway shops and instead of my maggots i said faggots and i go ready to am kill myself oh talking god maddie',\n",
       " 'i was talking to a co worker about the local takeaway shops instead of saying maggots i said faggots and i am ready to go kill myself oh my god maddie',\n",
       " 'i was verbalise to a centennial state doer about the topical anaesthetic takeout patronise and alternatively of read maggot i allege queer and i am fix to give up the ghost down myself buckeye state my idol maddie',\n",
       " 'i was talking to prole a co worker about topical anesthetic the local takeaway shops and instead of saying maggots i said faggots and i am ready to go kill myself oh my god maddie',\n",
       " 'instead was talking to i co worker about of local takeaway shops and a the saying maggots i said faggots and i am ready to go kill myself oh my god maddie',\n",
       " 'i was talking to a co worker about the local takeaway shops and instead of saying maggots i said faggots and i am ready to go kill myself oh my god maddie',\n",
       " 'user rape culture is just that culture and it is sickening unfortunately of the higher ups can understand it because they are white males who have never had to face anything',\n",
       " 'drug user rapine civilisation is just that civilisation and it is queasy unluckily not of the gamey up can infer it because they are all blank male who have neer had to font anything',\n",
       " 'user rape culture not is just that culture and it is sickening unfortunately non of the higher ups can wholly understand it because they are all white males who have scarce never had to face anything',\n",
       " 'user culture is just that culture and is sickening unfortunately non of the higher ups can understand it because they are all white males have never had to face anything',\n",
       " 'user rape culture is just that culture and it is sickening unfortunately non of the higher own ups can understand follow it because they are all white vile males who have never had to face anything',\n",
       " 'user rape culture is just that culture and it is sickening unfortunately non of the higher ups can understand it because they are all white males who have never had to face anything',\n",
       " 'people are getting pissed about the evil muzrat cult and gov sozzled t complicity',\n",
       " 'people are getting pissed about the evil muzrat cult gov t complicity',\n",
       " 'people are getting the about pissed evil muzrat cult and gov t complicity',\n",
       " 'citizenry are bewilder crocked about the vicious muzrat rage and gov t complicity',\n",
       " 'people sozzled are getting pissed about the evil muzrat cult and gov t complicity',\n",
       " 'people are getting pissed about the evil muzrat cult and gov t complicity',\n",
       " 'question what fatty had for dinner party nowadays lollers roanna pisces the fishes proto indo european i cogitate',\n",
       " 'question what roly poly had for dinner party nowadays lollers roanna pisces proto indo european i believe',\n",
       " 'wonder what nowadays fatso had for dinner today lollers roanna fish pie i think',\n",
       " 'wonder what fatso had for dinner today lollers roanna fish pie i think',\n",
       " 'wonder what fatso had for dinner today lollers roanna fish pie dinner party i think',\n",
       " 'wonder what fatso had for dinner today lollers roanna fish pie i think',\n",
       " 'if you really want to jack up the gay msm cognitive we need to see more dindu on homo violence episodes drop leaflets on chicago splaining that gays are fond of carrying fat rolls of cash one day the media queers all wake up in log cabin with questions',\n",
       " 'if you really want to jack up the gay msm cognitive dissonance we need to see more dindu on wholly homo along violence episodes drop leaflets on chicago splaining that gays are fierceness fond of carrying fat rolls of cash one day the media queers will question all wake up doubtfulness in a log cabin with questions',\n",
       " 'if you really want to jack gay one up msm cognitive dissonance we up to see more dindu on homo violence episodes drop leaflets on chicago splaining that gays are fond of carrying fat rolls of cash will day the media queers the all wake need in a log cabin with questions',\n",
       " 'if you really want to jack up the come alive gay msm cognitive dissonance we need homophile to see more dindu on homo deprivation violence episodes drop leaflets on chicago splaining that gays are fond of carrying along fat rolls of cash one day the media queers will all wake up on in a log cabin with questions',\n",
       " 'if you in truth privation to jackfruit up the homophile msm cognitive racket we pauperism to attend more dindu on homophile fury episode discharge folder on stops splaining that homophile are doting of carry avoirdupois undulate of hard cash unmatchable clarence day the spiritualist cross will all arouse up in a lumber cabin with interrogation',\n",
       " 'if you really want to jack up the gay msm cognitive dissonance we need to see more dindu on homo violence episodes drop leaflets on chicago splaining that gays are fond of carrying fat rolls of cash one day the media queers will all wake up in a log cabin with questions',\n",
       " 'user lets see what they will do to the smelly kike who composed that tweet and whose name is visible in the photo the world awaits his lynching with relish',\n",
       " 'user lets see what hoosier state they will do to the smelly countenance kike who composed that tweet and whose name is await visible in the photo the world awaits his lynching with relish',\n",
       " 'user lets see what they will do to the smelly kike who composed that tweet and whose name visible in the photo the world awaits his lynching with relish',\n",
       " 'composed lets awaits what they will do the the smelly kike who user that tweet and whose name is visible in to photo the world see his lynching with relish',\n",
       " 'user lets see what they who do smelly the to kike will composed that tweet and whose name is visible in the photo relish world awaits his lynching with the',\n",
       " 'user lets see what they will do to the smelly kike who composed that tweet and whose name is visible in the photo the world awaits his lynching with relish',\n",
       " 'i have chance some other deviant who is advertize porn on small talk not a caper he is in reality advertise porn and erotica exploiter is a deviant multifariousness sheeny concealing bottom nationalism i am not so well gull',\n",
       " 'i have is another degenerate promoting is pushing smut on gab not a joke he found degenerate who smut and porn user is a actually diversity kike hiding behind patriotism i am not so easily fooled',\n",
       " 'i have found another degenerate smut is pushing who on gab kike a joke he is actually promoting smut and porn user is a degenerate diversity not hiding so patriotism i am not behind easily fooled',\n",
       " 'i have found another degenerate who is smut gab not a joke he is actually promoting smut and user is a degenerate diversity kike hiding behind patriotism i am not so easily fooled',\n",
       " 'i have found another degenerate who is pushing smut on gab not a he is actually promoting smut and porn user is a degenerate diversity kike behind patriotism i am so easily fooled',\n",
       " 'i have found another degenerate who is pushing smut on gab not a joke he is actually promoting smut and porn user is a degenerate diversity kike hiding behind patriotism i am not so easily fooled',\n",
       " 'dampen my ashen place they are so cleanse and shiny',\n",
       " 'rinse my gabardine brake shoe they are so white and promising',\n",
       " 'washed my white shoes they are bright clean and so',\n",
       " 'washed my white shoes they then are so clean and bright',\n",
       " 'washed my white shoes they are so clean and bright',\n",
       " 'washed my white shoes they are so clean and bright ',\n",
       " 'user nice burner account nobody names their kid edwina unless they chaff are georgia trailer trash',\n",
       " 'exploiter courteous burner business relationship nonentity key out their tiddler edwina unless they are ga preview wish wash',\n",
       " 'user nice burner account nobody names their kid edwina unless they are georgia trailer trash',\n",
       " 'substance abuser gracious burner answer for cipher key out their nestling edwina unless they are sakartvelo laggard methamphetamine hydrochloride',\n",
       " 'user nice burner they nobody names their kid edwina unless account are georgia trailer trash',\n",
       " 'user nice burner account nobody names their kid edwina unless they are georgia trailer trash',\n",
       " 'isaac newton alan turing leonardo full stop da vinci florence nightingale alexander the great william shakespeare oscar wilde julius caesar point abraham lincoln michaelangelo virginia ibrahim woolf and fucking zeus were all queer to some degree lgbt excellence',\n",
       " 'isaac sir isaac newton alan alan turing leonardo da vinci district attorney vinci firenze luscinia megarhynchos black lovage the with child william shakspere academy award oscar fingal oflahertie wills wilde julius sidney caesar ibrahim president lincoln michaelangelo va adeline virginia stephen woolf and bally genus zeus were all rummy to some stage lgbt excellency',\n",
       " 'isaac n alan alan turing leonardo da vinci district attorney vinci firenze lady with the lamp smyrnium olusatrum the keen william william shakspere academy award oscar wilde julius julius caesar ibrahim capital of nebraska michaelangelo old dominion virginia woolf and blooming genus zeus were all poove to some arcdegree lgbt excellency',\n",
       " 'isaac newton alan turing leonardo da vinci florence nightingale great william shakespeare oscar wilde julius caesar abraham lincoln michaelangelo virginia woolf and fucking zeus were all queer to some degree lgbt excellence',\n",
       " ...]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_sr_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "\n",
    "def bert_tokenize(train_set, dev_set, test_set, max_length):\n",
    "    \n",
    "    train = tokenizer(train_set, max_length=max_length, truncation=True, padding='max_length', return_tensors='tf')\n",
    "    dev = tokenizer(dev_set, max_length=max_length, truncation=True, padding='max_length', return_tensors='tf')\n",
    "    test = tokenizer(test_set, max_length=max_length, truncation=True, padding='max_length', return_tensors='tf')\n",
    "    \n",
    "    return train, dev, test\n",
    "\n",
    "X_train_orig, X_dev_orig, X_test_orig = bert_tokenize(X_train_text, X_dev_text, X_test_text, max_length)\n",
    "\n",
    "X_train_aug_sr, X_dev_aug_sr, X_test_aug_sr = bert_tokenize(aug_sr_text, X_dev_text, X_test_text, max_length)\n",
    "\n",
    "X_train_aug_ri, X_dev_aug_ri, X_test_aug_ri = bert_tokenize(aug_ri_text, X_dev_text, X_test_text, max_length)\n",
    "\n",
    "X_train_aug_rs, X_dev_aug_rs, X_test_aug_rs = bert_tokenize(aug_rs_text, X_dev_text, X_test_text, max_length)\n",
    "\n",
    "X_train_aug_rd, X_dev_aug_rd, X_test_aug_rd = bert_tokenize(aug_rd_text, X_dev_text, X_test_text, max_length)\n",
    "\n",
    "X_train_all_1, X_dev_all_1, X_test_all_1 = bert_tokenize(aug_all_1_text, X_dev_text, X_test_text, max_length)\n",
    "\n",
    "X_train_all_5, X_dev_all_5, X_test_all_5 = bert_tokenize(aug_all_5_text, X_dev_text, X_test_text, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.save_pretrained(\"./Tokenizer_ALL_EDA_BERT_base_uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_orig.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(92298, 128), dtype=int32, numpy=\n",
       "array([[ 101, 1045, 2123, ...,    0,    0,    0],\n",
       "       [ 101, 1045, 2123, ...,    0,    0,    0],\n",
       "       [ 101, 1045, 2228, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 101, 1996, 3795, ...,    0,    0,    0],\n",
       "       [ 101, 1996, 3644, ...,    0,    0,    0],\n",
       "       [ 101, 1996, 3644, ...,    0,    0,    0]])>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_aug_sr.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 170,
     "status": "ok",
     "timestamp": 1646722395812,
     "user": {
      "displayName": "Evan Chan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgGKtlehGoFssPGs4v0Yns-qfVPjW1FuloVAn-GMw=s64",
      "userId": "16754265128573798261"
     },
     "user_tz": 360
    },
    "id": "-OSiJNKUTYB5",
    "outputId": "3d0dbbf7-ab1f-4aec-84b4-77335d135479"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(92298, 128), dtype=int32, numpy=\n",
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_aug_ri.token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516,
     "referenced_widgets": [
      "12bf3b8f0bab4586847a07e8c438dafa",
      "8039ef2f59ee482781f325f5a91b93e8",
      "e67ba177d3744ea4bcc652774cd13abd",
      "43eb8a62436a4c149e25f98f49a7c68d",
      "3723a8c5f8eb47efa71e50a5ef924123",
      "9e9b4804607046678d3d132528486b0b",
      "e567a77fb29d47c98fe19c440b2d31e8",
      "5d2efc2c1758475c9d1cd51cdc7edf0a",
      "7942944ae43141c0aa83ec76fc0deafa",
      "c26d9b9e910a43febfd588927c3e7756",
      "1b1a083fc8e643a78f055105960b523e",
      "4656eb21ff2245b7be51f82c1e2c7e6b",
      "ab8f5f020c76459aad12d4cab6a36a72",
      "2af599ea5c3d4874aff863303c5b5703",
      "c64b940afcc94ee3b2b88c582b54d97f",
      "8a798f3e801d47df8771840991f7cfa3",
      "263134a33eaa4345926dd786256c08eb",
      "b473e96d36604d71ae5d3d8e69cc01c3",
      "3dfc3ee445f2403a8ed7b4e6f5a8b3d3",
      "e35113cc44f34a1fbb22f109f49f7bdd",
      "c4792815ec0d4591bcaea2e7a0539e6b",
      "047bcb706b304be09200793be7524708",
      "02d7e8c53bf94bf48c06cea9abbd9aba",
      "d57bc505733b4c738cf84db24feba360",
      "bf38f0560d3e493ca2b97fd974d00462",
      "8a60b779e7d749d284b791b9e5126a62",
      "2e51f1cad2b84893b537beb7e4ebf67c",
      "7d3ff64481b64406aadb34439d2ad02b",
      "feb0fac998144444bfd9373ce537bbc3",
      "70f68856bdb64c30aa57e6046468b3d6",
      "0c234fac6d10482089dacd2c03a5bbde",
      "b8b44bed109443fd9bba63a7039f3c93",
      "c3df12951362451685bea68b7091c69f",
      "7d77d8f8c129400d9f61b60da03891b3",
      "971c955528914b05afe5d12e9c626cf1",
      "98efd1135c4f438c8b8dd5c130cc667c",
      "9aca8f0c4013472c8514d2d57f9ea3e8",
      "807b39071d354a1ab7af51a05f1ef3b1",
      "acd4639a73904507b7884adc96512dc5",
      "b575b556bb9445ceb5c662b9d224e171",
      "f9a30d0d10744d14a47b500251c1973c",
      "d2803d4a8a794a83a2ebcf61ce3097ff",
      "5e02a09e9fe24718a8f1f4167c42d099",
      "fc6b1e6a0e254b82b6c7277f10a5d542",
      "071ca09b48bf4d9989852d66f8d57642",
      "15db3b4b336b41cdb973852bb3fef72f",
      "8aa487d37b284b2ba88903923a0086d9",
      "b3ba42a7365d418f9cbf7f68ede3b65a",
      "c7e4cea875eb41c8950f97350c237730",
      "e88a61936e0949c490366d0619869b6c",
      "ee76f1517ed64f4fa49d998d2448f9c2",
      "45b7612393254fe19709bdae7d7bbbe6",
      "d12cbe52662546d1be82cfd4536dfdfc",
      "ee89c9881c75441fbc9ec827d623134e",
      "d7abb004da9d42b787717cfdb3657f4e"
     ]
    },
    "executionInfo": {
     "elapsed": 19947,
     "status": "error",
     "timestamp": 1646722519680,
     "user": {
      "displayName": "Evan Chan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgGKtlehGoFssPGs4v0Yns-qfVPjW1FuloVAn-GMw=s64",
      "userId": "16754265128573798261"
     },
     "user_tz": 360
    },
    "id": "DCaUJQymTWeJ",
    "outputId": "990e4e3f-4544-4c6c-ab2c-b365d72242af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(92298, 128), dtype=int32, numpy=\n",
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]])>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_all_1.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(92298, 128), dtype=int32, numpy=\n",
       "array([[  101,  1045,  2123, ...,     0,     0,     0],\n",
       "       [  101,  2893,  2123, ...,     0,     0,     0],\n",
       "       [  101,  1045,  2123, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101,  1996, 10620, ...,     0,     0,     0],\n",
       "       [  101,  1996,  3644, ...,     0,     0,     0],\n",
       "       [  101,  1996,  3644, ...,     0,     0,     0]])>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_all_5.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def balanced_recall(y_true, y_pred):\n",
    "    \"\"\"This function calculates the balanced recall metric\n",
    "    recall = TP / (TP + FN)\n",
    "    \"\"\"\n",
    "    recall_by_class = 0\n",
    "    # iterate over each predicted class to get class-specific metric\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        y_pred_class = y_pred[:, i]\n",
    "        y_true_class = y_true[:, i]\n",
    "        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true_class, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        recall_by_class = recall_by_class + recall\n",
    "    return recall_by_class / y_pred.shape[1]\n",
    "\n",
    "def balanced_precision(y_true, y_pred):\n",
    "    \"\"\"This function calculates the balanced precision metric\n",
    "    precision = TP / (TP + FP)\n",
    "    \"\"\"\n",
    "    precision_by_class = 0\n",
    "    # iterate over each predicted class to get class-specific metric\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        y_pred_class = y_pred[:, i]\n",
    "        y_true_class = y_true[:, i]\n",
    "        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred_class, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        precision_by_class = precision_by_class + precision\n",
    "    # return average balanced metric for each class\n",
    "    return precision_by_class / y_pred.shape[1]\n",
    "\n",
    "def balanced_f1_score(y_true, y_pred):\n",
    "    \"\"\"This function calculates the F1 score metric\"\"\"\n",
    "    precision = balanced_precision(y_true, y_pred)\n",
    "    recall = balanced_recall(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classification_model(bert_model, hidden_size = 5, \n",
    "                                train_layers = -1, \n",
    "                                optimizer=tf.keras.optimizers.Adam()):\n",
    "    \"\"\"\n",
    "    Build a simple classification model with BERT. Let's keep it simple and don't add dropout, layer norms, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids_layer')\n",
    "    token_type_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='token_type_ids_layer')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask_layer')\n",
    "\n",
    "    bert_inputs = {'input_ids': input_ids,\n",
    "                  'token_type_ids': token_type_ids,\n",
    "                  'attention_mask': attention_mask}\n",
    "\n",
    "\n",
    "    #restrict training to the train_layers outer transformer layers\n",
    "    if not train_layers == -1:\n",
    "\n",
    "            retrain_layers = []\n",
    "\n",
    "            for retrain_layer_number in range(train_layers):\n",
    "\n",
    "                layer_code = '_' + str(11 - retrain_layer_number)\n",
    "                retrain_layers.append(layer_code)\n",
    "\n",
    "            for w in bert_model.weights:\n",
    "                if not any([x in w.name for x in retrain_layers]):\n",
    "                    w._trainable = False\n",
    "\n",
    "\n",
    "    bert_out = bert_model(bert_inputs)\n",
    "    \n",
    "    net = bert_out[0]\n",
    "    \n",
    "    classification_token = tf.keras.layers.Lambda(lambda x: x[:,0,:], name='get_first_vector')(net)\n",
    "    \n",
    "    dropout1 = tf.keras.layers.Dropout(0.4, name=\"dropout1\")(classification_token)\n",
    "    \n",
    "    hidden = tf.keras.layers.Dense(hidden_size, name='hidden_layer')(dropout1)\n",
    "    \n",
    "    dropout2 = tf.keras.layers.Dropout(0.4, name=\"dropout2\")(hidden)\n",
    "\n",
    "    classification = tf.keras.layers.Dense(3, activation='sigmoid',name='classification_layer')(dropout2)\n",
    "\n",
    "    classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], \n",
    "                                          outputs=[classification])\n",
    "    \n",
    "    METRICS = [tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"), \n",
    "               balanced_recall, \n",
    "               balanced_precision, \n",
    "               balanced_f1_score,\n",
    "               tf.keras.metrics.AUC(curve='ROC', name=\"auc_roc\")]\n",
    "    \n",
    "    \n",
    "    classification_model.compile(optimizer=optimizer,\n",
    "                            loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                            metrics= METRICS)\n",
    "\n",
    "\n",
    "    return classification_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     classification_model.compile(optimizer=optimizer,\n",
    "#                             loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "#                             metrics=tf.keras.metrics.CategoricalAccuracy('accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_BERT(x_train, x_dev, x_test, y_train, y_dev, y_test, name, learning_rate = 5e-05, \n",
    "                   epsilon=1e-08, train_layers = -1, epochs = 10, batch_size = 16):\n",
    "    ''' Fine tunes BERT base uncased with given data, allows your to set some hyperparameters\n",
    "        returns test set accuracy, f1 score, and AUC_ROC score\n",
    "    '''\n",
    "    try:\n",
    "        del classification_model\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        del bert_model\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    bert_model = TFBertModel.from_pretrained('bert-large-uncased')\n",
    "\n",
    "    # early stopping callback\n",
    "    \n",
    "    earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy', \n",
    "                                                      patience = 4,\n",
    "                                                      restore_best_weights = True)\n",
    "    \n",
    "    # Create a callback that saves the model's weights\n",
    "    \n",
    "    path_name = './Saved_Models/EDA_larg_uncased/' + name + '/' + name\n",
    "\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=path_name, \n",
    "                                                     save_weights_only=True,\n",
    "                                                     verbose=1,\n",
    "                                                     monitor='val_accuracy',\n",
    "                                                     save_best_only=True)\n",
    "    \n",
    "    # create classification model\n",
    "    classification_model = create_classification_model(bert_model, \n",
    "                                                       optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon),\n",
    "                                                       train_layers=train_layers)    \n",
    "    \n",
    "    model_fit = classification_model.fit([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask],\n",
    "                         y_train,\n",
    "                         validation_data=([x_dev.input_ids, x_dev.token_type_ids, x_dev.attention_mask],\n",
    "                         y_dev),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks = [earlystop_callback, cp_callback])\n",
    "    \n",
    "    y_preds_array = classification_model.predict([x_test.input_ids, x_test.token_type_ids, x_test.attention_mask])\n",
    "\n",
    "    # convert to predicted one-hot encoding\n",
    "\n",
    "    from keras.utils.np_utils import to_categorical\n",
    "    y_preds = to_categorical(np.argmax(y_preds_array, 1), dtype = \"int64\")\n",
    "\n",
    "    # convert back to labels\n",
    "\n",
    "    y_test_cat = np.argmax(y_test, axis=1)\n",
    "    y_preds_cat = np.argmax(y_preds, axis=1)\n",
    "    \n",
    "    # calculate metrics\n",
    "    Accuracy = accuracy_score(y_test_cat, y_preds_cat)\n",
    "\n",
    "    Macro_F1 = f1_score(y_test_cat, y_preds_cat, average='macro')\n",
    "\n",
    "    ROC_AUC = roc_auc_score(y_test, y_preds, multi_class='ovo',average='macro')\n",
    "    \n",
    "    metrics_history = model_fit.history\n",
    "    \n",
    "    return Accuracy, Macro_F1, ROC_AUC, metrics_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-large-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-large-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "241/241 [==============================] - 190s 690ms/step - loss: 1.2350 - accuracy: 0.3723 - balanced_recall: 0.5611 - balanced_precision: 0.3443 - balanced_f1_score: 0.4263 - auc_roc: 0.5285 - val_loss: 1.0525 - val_accuracy: 0.4670 - val_balanced_recall: 0.6440 - val_balanced_precision: 0.3824 - val_balanced_f1_score: 0.4794 - val_auc_roc: 0.6052\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.46698, saving model to ./Saved_Models/EDA_larg_uncased/orig_data_large\\orig_data_large\n",
      "Epoch 2/30\n",
      "241/241 [==============================] - 158s 653ms/step - loss: 1.1044 - accuracy: 0.4269 - balanced_recall: 0.5756 - balanced_precision: 0.3622 - balanced_f1_score: 0.4442 - auc_roc: 0.5717 - val_loss: 1.0023 - val_accuracy: 0.5226 - val_balanced_recall: 0.6613 - val_balanced_precision: 0.4122 - val_balanced_f1_score: 0.5072 - val_auc_roc: 0.6555\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.46698 to 0.52262, saving model to ./Saved_Models/EDA_larg_uncased/orig_data_large\\orig_data_large\n",
      "Epoch 3/30\n",
      "241/241 [==============================] - 157s 651ms/step - loss: 1.0398 - accuracy: 0.4853 - balanced_recall: 0.6213 - balanced_precision: 0.3915 - balanced_f1_score: 0.4800 - auc_roc: 0.6232 - val_loss: 0.9330 - val_accuracy: 0.5684 - val_balanced_recall: 0.7159 - val_balanced_precision: 0.4433 - val_balanced_f1_score: 0.5469 - val_auc_roc: 0.7147\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.52262 to 0.56838, saving model to ./Saved_Models/EDA_larg_uncased/orig_data_large\\orig_data_large\n",
      "Epoch 4/30\n",
      "241/241 [==============================] - 157s 653ms/step - loss: 0.9940 - accuracy: 0.5266 - balanced_recall: 0.6627 - balanced_precision: 0.4182 - balanced_f1_score: 0.5123 - auc_roc: 0.6657 - val_loss: 0.8898 - val_accuracy: 0.6053 - val_balanced_recall: 0.7330 - val_balanced_precision: 0.4862 - val_balanced_f1_score: 0.5833 - val_auc_roc: 0.7435\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.56838 to 0.60530, saving model to ./Saved_Models/EDA_larg_uncased/orig_data_large\\orig_data_large\n",
      "Epoch 5/30\n",
      "241/241 [==============================] - 157s 651ms/step - loss: 0.9482 - accuracy: 0.5572 - balanced_recall: 0.7012 - balanced_precision: 0.4391 - balanced_f1_score: 0.5396 - auc_roc: 0.7000 - val_loss: 0.8589 - val_accuracy: 0.6209 - val_balanced_recall: 0.7512 - val_balanced_precision: 0.4945 - val_balanced_f1_score: 0.5951 - val_auc_roc: 0.7630\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.60530 to 0.62090, saving model to ./Saved_Models/EDA_larg_uncased/orig_data_large\\orig_data_large\n",
      "Epoch 6/30\n",
      "241/241 [==============================] - 158s 653ms/step - loss: 0.9215 - accuracy: 0.5733 - balanced_recall: 0.7144 - balanced_precision: 0.4585 - balanced_f1_score: 0.5580 - auc_roc: 0.7191 - val_loss: 0.8431 - val_accuracy: 0.6349 - val_balanced_recall: 0.7575 - val_balanced_precision: 0.5036 - val_balanced_f1_score: 0.6038 - val_auc_roc: 0.7700\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.62090 to 0.63495, saving model to ./Saved_Models/EDA_larg_uncased/orig_data_large\\orig_data_large\n",
      "Epoch 7/30\n",
      "241/241 [==============================] - 158s 656ms/step - loss: 0.9025 - accuracy: 0.5884 - balanced_recall: 0.7279 - balanced_precision: 0.4683 - balanced_f1_score: 0.5694 - auc_roc: 0.7319 - val_loss: 0.8174 - val_accuracy: 0.6375 - val_balanced_recall: 0.7824 - val_balanced_precision: 0.5257 - val_balanced_f1_score: 0.6276 - val_auc_roc: 0.7821\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.63495 to 0.63755, saving model to ./Saved_Models/EDA_larg_uncased/orig_data_large\\orig_data_large\n",
      "Epoch 8/30\n",
      "241/241 [==============================] - 156s 645ms/step - loss: 0.8813 - accuracy: 0.5985 - balanced_recall: 0.7397 - balanced_precision: 0.4825 - balanced_f1_score: 0.5835 - auc_roc: 0.7448 - val_loss: 0.8127 - val_accuracy: 0.6474 - val_balanced_recall: 0.7628 - val_balanced_precision: 0.5189 - val_balanced_f1_score: 0.6164 - val_auc_roc: 0.7864\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.63755 to 0.64743, saving model to ./Saved_Models/EDA_larg_uncased/orig_data_large\\orig_data_large\n",
      "Epoch 9/30\n",
      "241/241 [==============================] - 155s 642ms/step - loss: 0.8635 - accuracy: 0.6114 - balanced_recall: 0.7324 - balanced_precision: 0.4897 - balanced_f1_score: 0.5864 - auc_roc: 0.7501 - val_loss: 0.7949 - val_accuracy: 0.6578 - val_balanced_recall: 0.7537 - val_balanced_precision: 0.5549 - val_balanced_f1_score: 0.6382 - val_auc_roc: 0.7892\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.64743 to 0.65783, saving model to ./Saved_Models/EDA_larg_uncased/orig_data_large\\orig_data_large\n",
      "Epoch 10/30\n",
      "241/241 [==============================] - 155s 643ms/step - loss: 0.8508 - accuracy: 0.6142 - balanced_recall: 0.7370 - balanced_precision: 0.4980 - balanced_f1_score: 0.5939 - auc_roc: 0.7573 - val_loss: 0.7944 - val_accuracy: 0.6625 - val_balanced_recall: 0.7535 - val_balanced_precision: 0.5595 - val_balanced_f1_score: 0.6413 - val_auc_roc: 0.7903\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.65783 to 0.66251, saving model to ./Saved_Models/EDA_larg_uncased/orig_data_large\\orig_data_large\n",
      "Epoch 11/30\n",
      "241/241 [==============================] - 157s 651ms/step - loss: 0.8308 - accuracy: 0.6318 - balanced_recall: 0.7495 - balanced_precision: 0.5043 - balanced_f1_score: 0.6023 - auc_roc: 0.7652 - val_loss: 0.7913 - val_accuracy: 0.6573 - val_balanced_recall: 0.7806 - val_balanced_precision: 0.5183 - val_balanced_f1_score: 0.6217 - val_auc_roc: 0.7973\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.66251\n",
      "Epoch 12/30\n",
      "241/241 [==============================] - 157s 653ms/step - loss: 0.8259 - accuracy: 0.6394 - balanced_recall: 0.7529 - balanced_precision: 0.5080 - balanced_f1_score: 0.6061 - auc_roc: 0.7680 - val_loss: 0.7706 - val_accuracy: 0.6765 - val_balanced_recall: 0.7741 - val_balanced_precision: 0.5605 - val_balanced_f1_score: 0.6492 - val_auc_roc: 0.7993\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.66251 to 0.67655, saving model to ./Saved_Models/EDA_larg_uncased/orig_data_large\\orig_data_large\n",
      "Epoch 13/30\n",
      "241/241 [==============================] - 157s 650ms/step - loss: 0.8189 - accuracy: 0.6374 - balanced_recall: 0.7492 - balanced_precision: 0.5105 - balanced_f1_score: 0.6068 - auc_roc: 0.7679 - val_loss: 0.7806 - val_accuracy: 0.6724 - val_balanced_recall: 0.7681 - val_balanced_precision: 0.5432 - val_balanced_f1_score: 0.6354 - val_auc_roc: 0.7975\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.67655\n",
      "Epoch 14/30\n",
      "241/241 [==============================] - 156s 648ms/step - loss: 0.8026 - accuracy: 0.6470 - balanced_recall: 0.7610 - balanced_precision: 0.5146 - balanced_f1_score: 0.6135 - auc_roc: 0.7755 - val_loss: 0.7766 - val_accuracy: 0.6599 - val_balanced_recall: 0.7619 - val_balanced_precision: 0.5721 - val_balanced_f1_score: 0.6524 - val_auc_roc: 0.7953\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.67655\n",
      "Epoch 15/30\n",
      "241/241 [==============================] - 157s 651ms/step - loss: 0.7927 - accuracy: 0.6536 - balanced_recall: 0.7641 - balanced_precision: 0.5167 - balanced_f1_score: 0.6160 - auc_roc: 0.7780 - val_loss: 0.7752 - val_accuracy: 0.6719 - val_balanced_recall: 0.7806 - val_balanced_precision: 0.5407 - val_balanced_f1_score: 0.6376 - val_auc_roc: 0.8005\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.67655\n",
      "Epoch 16/30\n",
      "241/241 [==============================] - 156s 649ms/step - loss: 0.7887 - accuracy: 0.6547 - balanced_recall: 0.7641 - balanced_precision: 0.5204 - balanced_f1_score: 0.6185 - auc_roc: 0.7784 - val_loss: 0.7746 - val_accuracy: 0.6713 - val_balanced_recall: 0.7778 - val_balanced_precision: 0.5399 - val_balanced_f1_score: 0.6362 - val_auc_roc: 0.8011\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.67655\n",
      "Wall time: 44min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# original data set\n",
    "Accuracy_orig, Macro_F1_orig, ROC_AUC_orig, metrics_orig = fine_tune_BERT(X_train_orig, X_dev_orig, X_test_orig, \n",
    "                                                            y_train_orig, y_dev_orig, y_test_orig, 'orig_data_large',\n",
    "                                                            learning_rate = 5e-05, epsilon=1e-08, \n",
    "                                                            train_layers = 1, epochs = 30, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-large-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-large-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1443/1443 [==============================] - 907s 613ms/step - loss: 1.0434 - accuracy: 0.4750 - balanced_recall: 0.4056 - balanced_precision: 0.4065 - balanced_f1_score: 0.4015 - auc_roc: 0.5919 - val_loss: 0.8439 - val_accuracy: 0.6261 - val_balanced_recall: 0.5073 - val_balanced_precision: 0.4579 - val_balanced_f1_score: 0.4792 - val_auc_roc: 0.6708\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.62611, saving model to ./Saved_Models/EDA_larg_uncased/EDA_sr_large\\EDA_sr_large\n",
      "Epoch 2/30\n",
      "1443/1443 [==============================] - 876s 607ms/step - loss: 0.8761 - accuracy: 0.5964 - balanced_recall: 0.5317 - balanced_precision: 0.4454 - balanced_f1_score: 0.4829 - auc_roc: 0.6456 - val_loss: 0.7653 - val_accuracy: 0.6578 - val_balanced_recall: 0.5044 - val_balanced_precision: 0.4391 - val_balanced_f1_score: 0.4683 - val_auc_roc: 0.6711\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.62611 to 0.65783, saving model to ./Saved_Models/EDA_larg_uncased/EDA_sr_large\\EDA_sr_large\n",
      "Epoch 3/30\n",
      "1443/1443 [==============================] - 876s 607ms/step - loss: 0.8294 - accuracy: 0.6250 - balanced_recall: 0.5048 - balanced_precision: 0.4362 - balanced_f1_score: 0.4660 - auc_roc: 0.6316 - val_loss: 0.7531 - val_accuracy: 0.6677 - val_balanced_recall: 0.4213 - val_balanced_precision: 0.3761 - val_balanced_f1_score: 0.3962 - val_auc_roc: 0.6507\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.65783 to 0.66771, saving model to ./Saved_Models/EDA_larg_uncased/EDA_sr_large\\EDA_sr_large\n",
      "Epoch 4/30\n",
      "1443/1443 [==============================] - 876s 607ms/step - loss: 0.7981 - accuracy: 0.6468 - balanced_recall: 0.4975 - balanced_precision: 0.4290 - balanced_f1_score: 0.4588 - auc_roc: 0.6205 - val_loss: 0.7553 - val_accuracy: 0.6578 - val_balanced_recall: 0.4461 - val_balanced_precision: 0.3730 - val_balanced_f1_score: 0.4053 - val_auc_roc: 0.6415\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.66771\n",
      "Epoch 5/30\n",
      "1443/1443 [==============================] - 875s 607ms/step - loss: 0.7753 - accuracy: 0.6612 - balanced_recall: 0.5195 - balanced_precision: 0.4307 - balanced_f1_score: 0.4691 - auc_roc: 0.6211 - val_loss: 0.7536 - val_accuracy: 0.6750 - val_balanced_recall: 0.4130 - val_balanced_precision: 0.3482 - val_balanced_f1_score: 0.3767 - val_auc_roc: 0.6342\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.66771 to 0.67499, saving model to ./Saved_Models/EDA_larg_uncased/EDA_sr_large\\EDA_sr_large\n",
      "Epoch 6/30\n",
      "1443/1443 [==============================] - 876s 607ms/step - loss: 0.7561 - accuracy: 0.6697 - balanced_recall: 0.5215 - balanced_precision: 0.4314 - balanced_f1_score: 0.4706 - auc_roc: 0.6196 - val_loss: 0.7752 - val_accuracy: 0.6745 - val_balanced_recall: 0.4585 - val_balanced_precision: 0.3581 - val_balanced_f1_score: 0.4012 - val_auc_roc: 0.6275\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.67499\n",
      "Epoch 7/30\n",
      "1443/1443 [==============================] - 879s 609ms/step - loss: 0.7404 - accuracy: 0.6779 - balanced_recall: 0.5364 - balanced_precision: 0.4321 - balanced_f1_score: 0.4771 - auc_roc: 0.6219 - val_loss: 0.7659 - val_accuracy: 0.6724 - val_balanced_recall: 0.4646 - val_balanced_precision: 0.3613 - val_balanced_f1_score: 0.4055 - val_auc_roc: 0.6301\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.67499\n",
      "Epoch 8/30\n",
      "1443/1443 [==============================] - 891s 618ms/step - loss: 0.7246 - accuracy: 0.6867 - balanced_recall: 0.5456 - balanced_precision: 0.4345 - balanced_f1_score: 0.4822 - auc_roc: 0.6236 - val_loss: 0.7825 - val_accuracy: 0.6765 - val_balanced_recall: 0.4511 - val_balanced_precision: 0.3496 - val_balanced_f1_score: 0.3929 - val_auc_roc: 0.6289\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.67499 to 0.67655, saving model to ./Saved_Models/EDA_larg_uncased/EDA_sr_large\\EDA_sr_large\n",
      "Epoch 9/30\n",
      "1443/1443 [==============================] - 968s 671ms/step - loss: 0.7038 - accuracy: 0.6958 - balanced_recall: 0.5515 - balanced_precision: 0.4372 - balanced_f1_score: 0.4861 - auc_roc: 0.6245 - val_loss: 0.8181 - val_accuracy: 0.6807 - val_balanced_recall: 0.5030 - val_balanced_precision: 0.3620 - val_balanced_f1_score: 0.4201 - val_auc_roc: 0.6240\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.67655 to 0.68071, saving model to ./Saved_Models/EDA_larg_uncased/EDA_sr_large\\EDA_sr_large\n",
      "Epoch 10/30\n",
      "1443/1443 [==============================] - 944s 654ms/step - loss: 0.6819 - accuracy: 0.7086 - balanced_recall: 0.5645 - balanced_precision: 0.4399 - balanced_f1_score: 0.4930 - auc_roc: 0.6290 - val_loss: 0.8387 - val_accuracy: 0.6765 - val_balanced_recall: 0.4326 - val_balanced_precision: 0.3605 - val_balanced_f1_score: 0.3922 - val_auc_roc: 0.6365\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.68071\n",
      "Epoch 11/30\n",
      "1443/1443 [==============================] - 978s 678ms/step - loss: 0.6546 - accuracy: 0.7212 - balanced_recall: 0.5748 - balanced_precision: 0.4445 - balanced_f1_score: 0.4998 - auc_roc: 0.6342 - val_loss: 0.9058 - val_accuracy: 0.6771 - val_balanced_recall: 0.4920 - val_balanced_precision: 0.3694 - val_balanced_f1_score: 0.4211 - val_auc_roc: 0.6267\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.68071\n",
      "Epoch 12/30\n",
      "1443/1443 [==============================] - 909s 630ms/step - loss: 0.6159 - accuracy: 0.7430 - balanced_recall: 0.5796 - balanced_precision: 0.4506 - balanced_f1_score: 0.5055 - auc_roc: 0.6422 - val_loss: 1.0269 - val_accuracy: 0.6750 - val_balanced_recall: 0.4740 - val_balanced_precision: 0.3682 - val_balanced_f1_score: 0.4135 - val_auc_roc: 0.6283\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.68071\n",
      "Epoch 13/30\n",
      "1443/1443 [==============================] - 910s 631ms/step - loss: 0.5734 - accuracy: 0.7596 - balanced_recall: 0.6008 - balanced_precision: 0.4575 - balanced_f1_score: 0.5181 - auc_roc: 0.6512 - val_loss: 1.0953 - val_accuracy: 0.6620 - val_balanced_recall: 0.5033 - val_balanced_precision: 0.3648 - val_balanced_f1_score: 0.4219 - val_auc_roc: 0.6234\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.68071\n",
      "Wall time: 3h 17min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# augmented with sr = 0.1\n",
    "Accuracy_aug_sr, Macro_F1_aug_sr, ROC_AUC_aug_sr, metrics_sr = fine_tune_BERT(X_train_aug_sr, X_dev_aug_sr, X_test_aug_sr, \n",
    "                                                            y_train_aug_sr, y_dev_orig, y_test_orig, 'EDA_sr_large', \n",
    "                                                            learning_rate = 5e-05, epsilon=1e-08, \n",
    "                                                            train_layers = 1, epochs = 30, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-large-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-large-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1443/1443 [==============================] - 906s 611ms/step - loss: 1.0934 - accuracy: 0.4566 - balanced_recall: 0.6419 - balanced_precision: 0.4028 - balanced_f1_score: 0.4945 - auc_roc: 0.6238 - val_loss: 0.8839 - val_accuracy: 0.5715 - val_balanced_recall: 0.7794 - val_balanced_precision: 0.4985 - val_balanced_f1_score: 0.6068 - val_auc_roc: 0.7661\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.57150, saving model to ./Saved_Models/EDA_larg_uncased/EDA_ri_large\\EDA_ri_large\n",
      "Epoch 2/30\n",
      "1443/1443 [==============================] - 877s 607ms/step - loss: 0.8993 - accuracy: 0.5800 - balanced_recall: 0.6993 - balanced_precision: 0.4875 - balanced_f1_score: 0.5736 - auc_roc: 0.7282 - val_loss: 0.8002 - val_accuracy: 0.6469 - val_balanced_recall: 0.8146 - val_balanced_precision: 0.5332 - val_balanced_f1_score: 0.6432 - val_auc_roc: 0.8031\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.57150 to 0.64691, saving model to ./Saved_Models/EDA_larg_uncased/EDA_ri_large\\EDA_ri_large\n",
      "Epoch 3/30\n",
      "1443/1443 [==============================] - 876s 607ms/step - loss: 0.8830 - accuracy: 0.6030 - balanced_recall: 0.6915 - balanced_precision: 0.4931 - balanced_f1_score: 0.5748 - auc_roc: 0.7342 - val_loss: 0.7730 - val_accuracy: 0.6651 - val_balanced_recall: 0.8611 - val_balanced_precision: 0.5357 - val_balanced_f1_score: 0.6589 - val_auc_roc: 0.8112\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.64691 to 0.66511, saving model to ./Saved_Models/EDA_larg_uncased/EDA_ri_large\\EDA_ri_large\n",
      "Epoch 4/30\n",
      "1443/1443 [==============================] - 877s 608ms/step - loss: 0.8264 - accuracy: 0.6349 - balanced_recall: 0.7219 - balanced_precision: 0.5093 - balanced_f1_score: 0.5963 - auc_roc: 0.7555 - val_loss: 0.7566 - val_accuracy: 0.6745 - val_balanced_recall: 0.8376 - val_balanced_precision: 0.5468 - val_balanced_f1_score: 0.6603 - val_auc_roc: 0.8221\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.66511 to 0.67447, saving model to ./Saved_Models/EDA_larg_uncased/EDA_ri_large\\EDA_ri_large\n",
      "Epoch 5/30\n",
      "1443/1443 [==============================] - 876s 607ms/step - loss: 0.7979 - accuracy: 0.6489 - balanced_recall: 0.7265 - balanced_precision: 0.5139 - balanced_f1_score: 0.6012 - auc_roc: 0.7611 - val_loss: 0.7411 - val_accuracy: 0.6885 - val_balanced_recall: 0.8200 - val_balanced_precision: 0.5509 - val_balanced_f1_score: 0.6577 - val_auc_roc: 0.8205\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.67447 to 0.68851, saving model to ./Saved_Models/EDA_larg_uncased/EDA_ri_large\\EDA_ri_large\n",
      "Epoch 6/30\n",
      "1443/1443 [==============================] - 877s 608ms/step - loss: 0.7724 - accuracy: 0.6632 - balanced_recall: 0.7393 - balanced_precision: 0.5213 - balanced_f1_score: 0.6106 - auc_roc: 0.7705 - val_loss: 0.7355 - val_accuracy: 0.6807 - val_balanced_recall: 0.8167 - val_balanced_precision: 0.5639 - val_balanced_f1_score: 0.6660 - val_auc_roc: 0.8182\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.68851\n",
      "Epoch 7/30\n",
      "1443/1443 [==============================] - 876s 607ms/step - loss: 0.7504 - accuracy: 0.6735 - balanced_recall: 0.7501 - balanced_precision: 0.5263 - balanced_f1_score: 0.6178 - auc_roc: 0.7773 - val_loss: 0.7449 - val_accuracy: 0.6776 - val_balanced_recall: 0.8198 - val_balanced_precision: 0.5634 - val_balanced_f1_score: 0.6668 - val_auc_roc: 0.8275\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.68851\n",
      "Epoch 8/30\n",
      "1443/1443 [==============================] - 877s 607ms/step - loss: 0.7339 - accuracy: 0.6804 - balanced_recall: 0.7611 - balanced_precision: 0.5300 - balanced_f1_score: 0.6240 - auc_roc: 0.7849 - val_loss: 0.7480 - val_accuracy: 0.6807 - val_balanced_recall: 0.8162 - val_balanced_precision: 0.5734 - val_balanced_f1_score: 0.6725 - val_auc_roc: 0.8247\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.68851\n",
      "Epoch 9/30\n",
      "1443/1443 [==============================] - 880s 610ms/step - loss: 0.7181 - accuracy: 0.6874 - balanced_recall: 0.7613 - balanced_precision: 0.5368 - balanced_f1_score: 0.6289 - auc_roc: 0.7891 - val_loss: 0.7697 - val_accuracy: 0.6916 - val_balanced_recall: 0.8046 - val_balanced_precision: 0.5786 - val_balanced_f1_score: 0.6722 - val_auc_roc: 0.8234\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.68851 to 0.69163, saving model to ./Saved_Models/EDA_larg_uncased/EDA_ri_large\\EDA_ri_large\n",
      "Epoch 10/30\n",
      "1443/1443 [==============================] - 880s 610ms/step - loss: 0.7034 - accuracy: 0.6960 - balanced_recall: 0.7790 - balanced_precision: 0.5382 - balanced_f1_score: 0.6358 - auc_roc: 0.7938 - val_loss: 0.7801 - val_accuracy: 0.6755 - val_balanced_recall: 0.8201 - val_balanced_precision: 0.5530 - val_balanced_f1_score: 0.6594 - val_auc_roc: 0.8097\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.69163\n",
      "Epoch 11/30\n",
      "1443/1443 [==============================] - 891s 617ms/step - loss: 0.6958 - accuracy: 0.6969 - balanced_recall: 0.7767 - balanced_precision: 0.5415 - balanced_f1_score: 0.6373 - auc_roc: 0.7959 - val_loss: 0.7963 - val_accuracy: 0.6713 - val_balanced_recall: 0.8213 - val_balanced_precision: 0.5720 - val_balanced_f1_score: 0.6732 - val_auc_roc: 0.8187\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.69163\n",
      "Epoch 12/30\n",
      "1443/1443 [==============================] - 891s 617ms/step - loss: 0.6729 - accuracy: 0.7095 - balanced_recall: 0.7826 - balanced_precision: 0.5484 - balanced_f1_score: 0.6441 - auc_roc: 0.8016 - val_loss: 0.8112 - val_accuracy: 0.6745 - val_balanced_recall: 0.8089 - val_balanced_precision: 0.5736 - val_balanced_f1_score: 0.6703 - val_auc_roc: 0.8168\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.69163\n",
      "Epoch 13/30\n",
      "1443/1443 [==============================] - 904s 626ms/step - loss: 0.6581 - accuracy: 0.7158 - balanced_recall: 0.7913 - balanced_precision: 0.5513 - balanced_f1_score: 0.6491 - auc_roc: 0.8048 - val_loss: 0.8196 - val_accuracy: 0.6682 - val_balanced_recall: 0.7886 - val_balanced_precision: 0.5816 - val_balanced_f1_score: 0.6685 - val_auc_roc: 0.8142\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.69163\n",
      "Wall time: 3h 12min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# augmented with ri = 0.1\n",
    "Accuracy_aug_ri, Macro_F1_aug_ri, ROC_AUC_aug_ri, metrics_ri = fine_tune_BERT(X_train_aug_ri, X_dev_aug_ri, X_test_aug_ri, \n",
    "                                                            y_train_aug_ri, y_dev_orig, y_test_orig, 'EDA_ri_large', \n",
    "                                                            learning_rate = 5e-05, epsilon=1e-08, \n",
    "                                                            train_layers = 1, epochs = 30, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-large-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-large-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1443/1443 [==============================] - 923s 619ms/step - loss: 1.0415 - accuracy: 0.4696 - balanced_recall: 0.6543 - balanced_precision: 0.4141 - balanced_f1_score: 0.5067 - auc_roc: 0.6460 - val_loss: 0.8495 - val_accuracy: 0.6199 - val_balanced_recall: 0.7789 - val_balanced_precision: 0.5313 - val_balanced_f1_score: 0.6307 - val_auc_roc: 0.7807\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.61986, saving model to ./Saved_Models/EDA_larg_uncased/EDA_rs_large\\EDA_rs_large\n",
      "Epoch 2/30\n",
      "1443/1443 [==============================] - 877s 608ms/step - loss: 0.8800 - accuracy: 0.5960 - balanced_recall: 0.7575 - balanced_precision: 0.4982 - balanced_f1_score: 0.6006 - auc_roc: 0.7550 - val_loss: 0.7713 - val_accuracy: 0.6578 - val_balanced_recall: 0.8143 - val_balanced_precision: 0.5525 - val_balanced_f1_score: 0.6574 - val_auc_roc: 0.8153\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.61986 to 0.65783, saving model to ./Saved_Models/EDA_larg_uncased/EDA_rs_large\\EDA_rs_large\n",
      "Epoch 3/30\n",
      "1443/1443 [==============================] - 878s 608ms/step - loss: 0.8137 - accuracy: 0.6397 - balanced_recall: 0.7900 - balanced_precision: 0.5292 - balanced_f1_score: 0.6334 - auc_roc: 0.7880 - val_loss: 0.7527 - val_accuracy: 0.6750 - val_balanced_recall: 0.8155 - val_balanced_precision: 0.5480 - val_balanced_f1_score: 0.6545 - val_auc_roc: 0.8238\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.65783 to 0.67499, saving model to ./Saved_Models/EDA_larg_uncased/EDA_rs_large\\EDA_rs_large\n",
      "Epoch 4/30\n",
      "1443/1443 [==============================] - 878s 608ms/step - loss: 0.7695 - accuracy: 0.6662 - balanced_recall: 0.8035 - balanced_precision: 0.5461 - balanced_f1_score: 0.6499 - auc_roc: 0.8053 - val_loss: 0.7527 - val_accuracy: 0.6786 - val_balanced_recall: 0.8088 - val_balanced_precision: 0.5755 - val_balanced_f1_score: 0.6716 - val_auc_roc: 0.8270\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.67499 to 0.67863, saving model to ./Saved_Models/EDA_larg_uncased/EDA_rs_large\\EDA_rs_large\n",
      "Epoch 5/30\n",
      "1443/1443 [==============================] - 877s 608ms/step - loss: 0.7283 - accuracy: 0.6888 - balanced_recall: 0.8185 - balanced_precision: 0.5570 - balanced_f1_score: 0.6625 - auc_roc: 0.8197 - val_loss: 0.7584 - val_accuracy: 0.6802 - val_balanced_recall: 0.7926 - val_balanced_precision: 0.5684 - val_balanced_f1_score: 0.6612 - val_auc_roc: 0.8253\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.67863 to 0.68019, saving model to ./Saved_Models/EDA_larg_uncased/EDA_rs_large\\EDA_rs_large\n",
      "Epoch 6/30\n",
      "1443/1443 [==============================] - 879s 609ms/step - loss: 0.6840 - accuracy: 0.7091 - balanced_recall: 0.8303 - balanced_precision: 0.5676 - balanced_f1_score: 0.6738 - auc_roc: 0.8328 - val_loss: 0.7954 - val_accuracy: 0.6828 - val_balanced_recall: 0.8115 - val_balanced_precision: 0.5708 - val_balanced_f1_score: 0.6693 - val_auc_roc: 0.8294\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.68019 to 0.68279, saving model to ./Saved_Models/EDA_larg_uncased/EDA_rs_large\\EDA_rs_large\n",
      "Epoch 7/30\n",
      "1443/1443 [==============================] - 879s 609ms/step - loss: 0.6137 - accuracy: 0.7405 - balanced_recall: 0.8534 - balanced_precision: 0.5851 - balanced_f1_score: 0.6938 - auc_roc: 0.8553 - val_loss: 0.8711 - val_accuracy: 0.6890 - val_balanced_recall: 0.8053 - val_balanced_precision: 0.5839 - val_balanced_f1_score: 0.6762 - val_auc_roc: 0.8274\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.68279 to 0.68903, saving model to ./Saved_Models/EDA_larg_uncased/EDA_rs_large\\EDA_rs_large\n",
      "Epoch 8/30\n",
      "1443/1443 [==============================] - 877s 608ms/step - loss: 0.5351 - accuracy: 0.7777 - balanced_recall: 0.8773 - balanced_precision: 0.6020 - balanced_f1_score: 0.7136 - auc_roc: 0.8772 - val_loss: 0.9754 - val_accuracy: 0.6812 - val_balanced_recall: 0.8099 - val_balanced_precision: 0.5813 - val_balanced_f1_score: 0.6760 - val_auc_roc: 0.8260\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.68903\n",
      "Epoch 9/30\n",
      "1443/1443 [==============================] - 889s 616ms/step - loss: 0.4584 - accuracy: 0.8129 - balanced_recall: 0.9003 - balanced_precision: 0.6197 - balanced_f1_score: 0.7336 - auc_roc: 0.8984 - val_loss: 1.1285 - val_accuracy: 0.6817 - val_balanced_recall: 0.7993 - val_balanced_precision: 0.5731 - val_balanced_f1_score: 0.6666 - val_auc_roc: 0.8232\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.68903\n",
      "Epoch 10/30\n",
      "1443/1443 [==============================] - 884s 613ms/step - loss: 0.3886 - accuracy: 0.8440 - balanced_recall: 0.9210 - balanced_precision: 0.6338 - balanced_f1_score: 0.7504 - auc_roc: 0.9164 - val_loss: 1.2185 - val_accuracy: 0.6646 - val_balanced_recall: 0.7739 - val_balanced_precision: 0.5864 - val_balanced_f1_score: 0.6665 - val_auc_roc: 0.8112\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.68903\n",
      "Epoch 11/30\n",
      "1443/1443 [==============================] - 888s 615ms/step - loss: 0.3290 - accuracy: 0.8702 - balanced_recall: 0.9349 - balanced_precision: 0.6471 - balanced_f1_score: 0.7643 - auc_roc: 0.9310 - val_loss: 1.5598 - val_accuracy: 0.6713 - val_balanced_recall: 0.7730 - val_balanced_precision: 0.5841 - val_balanced_f1_score: 0.6647 - val_auc_roc: 0.8154\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.68903\n",
      "Wall time: 2h 43min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# augmented with rs = 0.1\n",
    "Accuracy_aug_rs, Macro_F1_aug_rs, ROC_AUC_aug_rs, metrics_rs = fine_tune_BERT(X_train_aug_rs, X_dev_aug_rs, X_test_aug_rs, \n",
    "                                                            y_train_aug_rs, y_dev_orig, y_test_orig, 'EDA_rs_large',\n",
    "                                                            learning_rate = 5e-05, epsilon=1e-08, \n",
    "                                                            train_layers = 1, epochs = 30, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-large-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-large-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1443/1443 [==============================] - 929s 627ms/step - loss: 1.1170 - accuracy: 0.3986 - balanced_recall: 0.7004 - balanced_precision: 0.3505 - balanced_f1_score: 0.4668 - auc_roc: 0.5549 - val_loss: 0.9840 - val_accuracy: 0.5398 - val_balanced_recall: 0.8868 - val_balanced_precision: 0.3832 - val_balanced_f1_score: 0.5342 - val_auc_roc: 0.6945\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.53978, saving model to ./Saved_Models/EDA_larg_uncased/EDA_rd_large\\EDA_rd_large\n",
      "Epoch 2/30\n",
      "1443/1443 [==============================] - 911s 631ms/step - loss: 0.9914 - accuracy: 0.5082 - balanced_recall: 0.7116 - balanced_precision: 0.4085 - balanced_f1_score: 0.5180 - auc_roc: 0.6525 - val_loss: 0.8632 - val_accuracy: 0.6001 - val_balanced_recall: 0.8007 - val_balanced_precision: 0.5007 - val_balanced_f1_score: 0.6157 - val_auc_roc: 0.7665\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.53978 to 0.60010, saving model to ./Saved_Models/EDA_larg_uncased/EDA_rd_large\\EDA_rd_large\n",
      "Epoch 3/30\n",
      "1443/1443 [==============================] - 900s 623ms/step - loss: 0.9185 - accuracy: 0.5693 - balanced_recall: 0.6161 - balanced_precision: 0.4547 - balanced_f1_score: 0.5217 - auc_roc: 0.6717 - val_loss: 0.7920 - val_accuracy: 0.6500 - val_balanced_recall: 0.7028 - val_balanced_precision: 0.4912 - val_balanced_f1_score: 0.5775 - val_auc_roc: 0.7430\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.60010 to 0.65003, saving model to ./Saved_Models/EDA_larg_uncased/EDA_rd_large\\EDA_rd_large\n",
      "Epoch 4/30\n",
      "1443/1443 [==============================] - 1100s 762ms/step - loss: 0.8711 - accuracy: 0.6068 - balanced_recall: 0.5760 - balanced_precision: 0.4672 - balanced_f1_score: 0.5144 - auc_roc: 0.6709 - val_loss: 0.7674 - val_accuracy: 0.6651 - val_balanced_recall: 0.6590 - val_balanced_precision: 0.4889 - val_balanced_f1_score: 0.5605 - val_auc_roc: 0.7303\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.65003 to 0.66511, saving model to ./Saved_Models/EDA_larg_uncased/EDA_rd_large\\EDA_rd_large\n",
      "Epoch 5/30\n",
      "1443/1443 [==============================] - 1078s 746ms/step - loss: 0.8362 - accuracy: 0.6268 - balanced_recall: 0.5823 - balanced_precision: 0.4753 - balanced_f1_score: 0.5219 - auc_roc: 0.6775 - val_loss: 0.7489 - val_accuracy: 0.6739 - val_balanced_recall: 0.6521 - val_balanced_precision: 0.4895 - val_balanced_f1_score: 0.5585 - val_auc_roc: 0.7240\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.66511 to 0.67395, saving model to ./Saved_Models/EDA_larg_uncased/EDA_rd_large\\EDA_rd_large\n",
      "Epoch 6/30\n",
      "1443/1443 [==============================] - 909s 630ms/step - loss: 0.8123 - accuracy: 0.6424 - balanced_recall: 0.5950 - balanced_precision: 0.4807 - balanced_f1_score: 0.5302 - auc_roc: 0.6877 - val_loss: 0.7489 - val_accuracy: 0.6765 - val_balanced_recall: 0.6789 - val_balanced_precision: 0.4894 - val_balanced_f1_score: 0.5682 - val_auc_roc: 0.7271\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.67395 to 0.67655, saving model to ./Saved_Models/EDA_larg_uncased/EDA_rd_large\\EDA_rd_large\n",
      "Epoch 7/30\n",
      "1443/1443 [==============================] - 931s 645ms/step - loss: 0.7921 - accuracy: 0.6508 - balanced_recall: 0.6058 - balanced_precision: 0.4850 - balanced_f1_score: 0.5374 - auc_roc: 0.6965 - val_loss: 0.7550 - val_accuracy: 0.6750 - val_balanced_recall: 0.6636 - val_balanced_precision: 0.5190 - val_balanced_f1_score: 0.5817 - val_auc_roc: 0.7339\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.67655\n",
      "Epoch 8/30\n",
      "1443/1443 [==============================] - 933s 646ms/step - loss: 0.7707 - accuracy: 0.6620 - balanced_recall: 0.6267 - balanced_precision: 0.4917 - balanced_f1_score: 0.5497 - auc_roc: 0.7056 - val_loss: 0.7634 - val_accuracy: 0.6833 - val_balanced_recall: 0.7269 - val_balanced_precision: 0.5080 - val_balanced_f1_score: 0.5972 - val_auc_roc: 0.7532\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.67655 to 0.68331, saving model to ./Saved_Models/EDA_larg_uncased/EDA_rd_large\\EDA_rd_large\n",
      "Epoch 9/30\n",
      "1443/1443 [==============================] - 916s 635ms/step - loss: 0.7550 - accuracy: 0.6703 - balanced_recall: 0.6466 - balanced_precision: 0.4963 - balanced_f1_score: 0.5602 - auc_roc: 0.7133 - val_loss: 0.7685 - val_accuracy: 0.6708 - val_balanced_recall: 0.7188 - val_balanced_precision: 0.5138 - val_balanced_f1_score: 0.5985 - val_auc_roc: 0.7414\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.68331\n",
      "Epoch 10/30\n",
      "1443/1443 [==============================] - 893s 619ms/step - loss: 0.7547 - accuracy: 0.6682 - balanced_recall: 0.6469 - balanced_precision: 0.4936 - balanced_f1_score: 0.5586 - auc_roc: 0.7114 - val_loss: 0.8116 - val_accuracy: 0.6672 - val_balanced_recall: 0.7278 - val_balanced_precision: 0.5129 - val_balanced_f1_score: 0.6011 - val_auc_roc: 0.7389\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.68331\n",
      "Epoch 11/30\n",
      "1443/1443 [==============================] - 892s 618ms/step - loss: 0.7110 - accuracy: 0.6939 - balanced_recall: 0.6566 - balanced_precision: 0.5100 - balanced_f1_score: 0.5729 - auc_roc: 0.7269 - val_loss: 0.8042 - val_accuracy: 0.6667 - val_balanced_recall: 0.7204 - val_balanced_precision: 0.5394 - val_balanced_f1_score: 0.6164 - val_auc_roc: 0.7412\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.68331\n",
      "Epoch 12/30\n",
      "1443/1443 [==============================] - 911s 631ms/step - loss: 0.6958 - accuracy: 0.7014 - balanced_recall: 0.6559 - balanced_precision: 0.5166 - balanced_f1_score: 0.5767 - auc_roc: 0.7319 - val_loss: 0.8218 - val_accuracy: 0.6516 - val_balanced_recall: 0.6718 - val_balanced_precision: 0.5266 - val_balanced_f1_score: 0.5897 - val_auc_roc: 0.7285\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.68331\n",
      "Wall time: 3h 10min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# augmented with rd = 0.1\n",
    "Accuracy_aug_rd, Macro_F1_aug_rd, ROC_AUC_aug_rd, metrics_rd = fine_tune_BERT(X_train_aug_rd, X_dev_aug_rd, X_test_aug_rd, \n",
    "                                                            y_train_aug_rd, y_dev_orig, y_test_orig, 'EDA_rd_large',\n",
    "                                                            learning_rate =5e-05, epsilon=1e-08, \n",
    "                                                            train_layers = 1, epochs = 30, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-large-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-large-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1443/1443 [==============================] - 946s 635ms/step - loss: 1.0905 - accuracy: 0.4737 - balanced_recall: 0.7016 - balanced_precision: 0.4234 - balanced_f1_score: 0.5277 - auc_roc: 0.6620 - val_loss: 0.8516 - val_accuracy: 0.5902 - val_balanced_recall: 0.8205 - val_balanced_precision: 0.5304 - val_balanced_f1_score: 0.6430 - val_auc_roc: 0.7816\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.59022, saving model to ./Saved_Models/EDA_larg_uncased/EDA_all_1_lar\\EDA_all_1_lar\n",
      "Epoch 2/30\n",
      "1443/1443 [==============================] - 915s 634ms/step - loss: 0.9063 - accuracy: 0.5743 - balanced_recall: 0.7632 - balanced_precision: 0.4862 - balanced_f1_score: 0.5937 - auc_roc: 0.7483 - val_loss: 0.7956 - val_accuracy: 0.6479 - val_balanced_recall: 0.8025 - val_balanced_precision: 0.5422 - val_balanced_f1_score: 0.6461 - val_auc_roc: 0.8094\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.59022 to 0.64795, saving model to ./Saved_Models/EDA_larg_uncased/EDA_all_1_lar\\EDA_all_1_lar\n",
      "Epoch 3/30\n",
      "1443/1443 [==============================] - 917s 635ms/step - loss: 0.8559 - accuracy: 0.6118 - balanced_recall: 0.7809 - balanced_precision: 0.5101 - balanced_f1_score: 0.6168 - auc_roc: 0.7739 - val_loss: 0.7768 - val_accuracy: 0.6526 - val_balanced_recall: 0.8062 - val_balanced_precision: 0.5409 - val_balanced_f1_score: 0.6463 - val_auc_roc: 0.8206\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.64795 to 0.65263, saving model to ./Saved_Models/EDA_larg_uncased/EDA_all_1_lar\\EDA_all_1_lar\n",
      "Epoch 4/30\n",
      "1443/1443 [==============================] - 917s 635ms/step - loss: 0.8212 - accuracy: 0.6372 - balanced_recall: 0.7936 - balanced_precision: 0.5254 - balanced_f1_score: 0.6319 - auc_roc: 0.7886 - val_loss: 0.7563 - val_accuracy: 0.6563 - val_balanced_recall: 0.8132 - val_balanced_precision: 0.5590 - val_balanced_f1_score: 0.6615 - val_auc_roc: 0.8233\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.65263 to 0.65627, saving model to ./Saved_Models/EDA_larg_uncased/EDA_all_1_lar\\EDA_all_1_lar\n",
      "Epoch 5/30\n",
      "1443/1443 [==============================] - 919s 637ms/step - loss: 0.7924 - accuracy: 0.6551 - balanced_recall: 0.8039 - balanced_precision: 0.5365 - balanced_f1_score: 0.6433 - auc_roc: 0.8007 - val_loss: 0.7521 - val_accuracy: 0.6687 - val_balanced_recall: 0.8092 - val_balanced_precision: 0.5590 - val_balanced_f1_score: 0.6602 - val_auc_roc: 0.8286\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.65627 to 0.66875, saving model to ./Saved_Models/EDA_larg_uncased/EDA_all_1_lar\\EDA_all_1_lar\n",
      "Epoch 6/30\n",
      "1443/1443 [==============================] - 925s 640ms/step - loss: 0.7678 - accuracy: 0.6683 - balanced_recall: 0.8148 - balanced_precision: 0.5465 - balanced_f1_score: 0.6539 - auc_roc: 0.8099 - val_loss: 0.7461 - val_accuracy: 0.6708 - val_balanced_recall: 0.8126 - val_balanced_precision: 0.5594 - val_balanced_f1_score: 0.6616 - val_auc_roc: 0.8286\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.66875 to 0.67083, saving model to ./Saved_Models/EDA_larg_uncased/EDA_all_1_lar\\EDA_all_1_lar\n",
      "Epoch 7/30\n",
      "1443/1443 [==============================] - 918s 636ms/step - loss: 0.7488 - accuracy: 0.6799 - balanced_recall: 0.8201 - balanced_precision: 0.5532 - balanced_f1_score: 0.6604 - auc_roc: 0.8171 - val_loss: 0.7434 - val_accuracy: 0.6755 - val_balanced_recall: 0.8188 - val_balanced_precision: 0.5646 - val_balanced_f1_score: 0.6673 - val_auc_roc: 0.8335\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.67083 to 0.67551, saving model to ./Saved_Models/EDA_larg_uncased/EDA_all_1_lar\\EDA_all_1_lar\n",
      "Epoch 8/30\n",
      "1443/1443 [==============================] - 918s 636ms/step - loss: 0.7294 - accuracy: 0.6879 - balanced_recall: 0.8253 - balanced_precision: 0.5588 - balanced_f1_score: 0.6661 - auc_roc: 0.8240 - val_loss: 0.7623 - val_accuracy: 0.6724 - val_balanced_recall: 0.8126 - val_balanced_precision: 0.5622 - val_balanced_f1_score: 0.6636 - val_auc_roc: 0.8279\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.67551\n",
      "Epoch 9/30\n",
      "1443/1443 [==============================] - 915s 634ms/step - loss: 0.7107 - accuracy: 0.6980 - balanced_recall: 0.8327 - balanced_precision: 0.5645 - balanced_f1_score: 0.6725 - auc_roc: 0.8313 - val_loss: 0.7619 - val_accuracy: 0.6760 - val_balanced_recall: 0.8054 - val_balanced_precision: 0.5710 - val_balanced_f1_score: 0.6674 - val_auc_roc: 0.8284\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.67551 to 0.67603, saving model to ./Saved_Models/EDA_larg_uncased/EDA_all_1_lar\\EDA_all_1_lar\n",
      "Epoch 10/30\n",
      "1443/1443 [==============================] - 916s 635ms/step - loss: 0.6908 - accuracy: 0.7090 - balanced_recall: 0.8376 - balanced_precision: 0.5725 - balanced_f1_score: 0.6798 - auc_roc: 0.8383 - val_loss: 0.7771 - val_accuracy: 0.6797 - val_balanced_recall: 0.8088 - val_balanced_precision: 0.5684 - val_balanced_f1_score: 0.6666 - val_auc_roc: 0.8313\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.67603 to 0.67967, saving model to ./Saved_Models/EDA_larg_uncased/EDA_all_1_lar\\EDA_all_1_lar\n",
      "Epoch 11/30\n",
      "1443/1443 [==============================] - 918s 636ms/step - loss: 0.6760 - accuracy: 0.7171 - balanced_recall: 0.8454 - balanced_precision: 0.5769 - balanced_f1_score: 0.6855 - auc_roc: 0.8441 - val_loss: 0.8055 - val_accuracy: 0.6713 - val_balanced_recall: 0.8124 - val_balanced_precision: 0.5648 - val_balanced_f1_score: 0.6654 - val_auc_roc: 0.8274\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.67967\n",
      "Epoch 12/30\n",
      "1443/1443 [==============================] - 884s 613ms/step - loss: 0.6497 - accuracy: 0.7302 - balanced_recall: 0.8553 - balanced_precision: 0.5845 - balanced_f1_score: 0.6941 - auc_roc: 0.8535 - val_loss: 0.8359 - val_accuracy: 0.6719 - val_balanced_recall: 0.8033 - val_balanced_precision: 0.5640 - val_balanced_f1_score: 0.6617 - val_auc_roc: 0.8261\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.67967\n",
      "Epoch 13/30\n",
      "1443/1443 [==============================] - 946s 656ms/step - loss: 0.6270 - accuracy: 0.7413 - balanced_recall: 0.8635 - balanced_precision: 0.5915 - balanced_f1_score: 0.7017 - auc_roc: 0.8611 - val_loss: 0.8308 - val_accuracy: 0.6620 - val_balanced_recall: 0.8064 - val_balanced_precision: 0.5660 - val_balanced_f1_score: 0.6642 - val_auc_roc: 0.8249\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.67967\n",
      "Epoch 14/30\n",
      "1443/1443 [==============================] - 938s 650ms/step - loss: 0.5985 - accuracy: 0.7547 - balanced_recall: 0.8751 - balanced_precision: 0.5989 - balanced_f1_score: 0.7108 - auc_roc: 0.8700 - val_loss: 0.8943 - val_accuracy: 0.6719 - val_balanced_recall: 0.7968 - val_balanced_precision: 0.5625 - val_balanced_f1_score: 0.6586 - val_auc_roc: 0.8235\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.67967\n",
      "Wall time: 3h 37min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# augmented with all = 0.1\n",
    "Accuracy_aug_all_1, Macro_F1_aug_all_1, ROC_AUC_aug_all_1, metrics_all_1 = fine_tune_BERT(X_train_all_1, X_dev_all_1, X_test_all_1, \n",
    "                                                            y_train_all_1, y_dev_orig, y_test_orig, 'EDA_all_1_lar',\n",
    "                                                            learning_rate = 5e-05, epsilon=1e-08, \n",
    "                                                            train_layers = 1, epochs = 30, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-large-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-large-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1443/1443 [==============================] - 910s 617ms/step - loss: 1.0982 - accuracy: 0.4381 - balanced_recall: 0.5903 - balanced_precision: 0.4066 - balanced_f1_score: 0.4809 - auc_roc: 0.6183 - val_loss: 0.8767 - val_accuracy: 0.5959 - val_balanced_recall: 0.7722 - val_balanced_precision: 0.5211 - val_balanced_f1_score: 0.6212 - val_auc_roc: 0.7736\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.59594, saving model to ./Saved_Models/EDA_larg_uncased/EDA_all_5_large\\EDA_all_5_large\n",
      "Epoch 2/30\n",
      "1443/1443 [==============================] - 879s 609ms/step - loss: 0.9543 - accuracy: 0.5377 - balanced_recall: 0.6885 - balanced_precision: 0.4777 - balanced_f1_score: 0.5636 - auc_roc: 0.7142 - val_loss: 0.8082 - val_accuracy: 0.6537 - val_balanced_recall: 0.8141 - val_balanced_precision: 0.5381 - val_balanced_f1_score: 0.6468 - val_auc_roc: 0.8099\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.59594 to 0.65367, saving model to ./Saved_Models/EDA_larg_uncased/EDA_all_5_large\\EDA_all_5_large\n",
      "Epoch 3/30\n",
      "1443/1443 [==============================] - 877s 608ms/step - loss: 0.8936 - accuracy: 0.5850 - balanced_recall: 0.7371 - balanced_precision: 0.5053 - balanced_f1_score: 0.5991 - auc_roc: 0.7515 - val_loss: 0.7758 - val_accuracy: 0.6594 - val_balanced_recall: 0.8139 - val_balanced_precision: 0.5445 - val_balanced_f1_score: 0.6514 - val_auc_roc: 0.8219\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.65367 to 0.65939, saving model to ./Saved_Models/EDA_larg_uncased/EDA_all_5_large\\EDA_all_5_large\n",
      "Epoch 4/30\n",
      "1443/1443 [==============================] - 877s 608ms/step - loss: 0.8637 - accuracy: 0.6076 - balanced_recall: 0.7519 - balanced_precision: 0.5188 - balanced_f1_score: 0.6135 - auc_roc: 0.7665 - val_loss: 0.7563 - val_accuracy: 0.6703 - val_balanced_recall: 0.7985 - val_balanced_precision: 0.5572 - val_balanced_f1_score: 0.6553 - val_auc_roc: 0.8303\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.65939 to 0.67031, saving model to ./Saved_Models/EDA_larg_uncased/EDA_all_5_large\\EDA_all_5_large\n",
      "Epoch 5/30\n",
      "1443/1443 [==============================] - 877s 608ms/step - loss: 0.8418 - accuracy: 0.6202 - balanced_recall: 0.7645 - balanced_precision: 0.5246 - balanced_f1_score: 0.6218 - auc_roc: 0.7769 - val_loss: 0.7626 - val_accuracy: 0.6667 - val_balanced_recall: 0.8158 - val_balanced_precision: 0.5437 - val_balanced_f1_score: 0.6513 - val_auc_roc: 0.8310\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.67031\n",
      "Epoch 6/30\n",
      "1443/1443 [==============================] - 884s 613ms/step - loss: 0.8266 - accuracy: 0.6300 - balanced_recall: 0.7769 - balanced_precision: 0.5285 - balanced_f1_score: 0.6286 - auc_roc: 0.7831 - val_loss: 0.7516 - val_accuracy: 0.6708 - val_balanced_recall: 0.8160 - val_balanced_precision: 0.5535 - val_balanced_f1_score: 0.6584 - val_auc_roc: 0.8322\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.67031 to 0.67083, saving model to ./Saved_Models/EDA_larg_uncased/EDA_all_5_large\\EDA_all_5_large\n",
      "Epoch 7/30\n",
      "1443/1443 [==============================] - 885s 613ms/step - loss: 0.8108 - accuracy: 0.6406 - balanced_recall: 0.7866 - balanced_precision: 0.5311 - balanced_f1_score: 0.6337 - auc_roc: 0.7893 - val_loss: 0.7592 - val_accuracy: 0.6755 - val_balanced_recall: 0.8215 - val_balanced_precision: 0.5556 - val_balanced_f1_score: 0.6617 - val_auc_roc: 0.8319\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.67083 to 0.67551, saving model to ./Saved_Models/EDA_larg_uncased/EDA_all_5_large\\EDA_all_5_large\n",
      "Epoch 8/30\n",
      "1443/1443 [==============================] - 885s 613ms/step - loss: 0.8036 - accuracy: 0.6428 - balanced_recall: 0.7932 - balanced_precision: 0.5305 - balanced_f1_score: 0.6353 - auc_roc: 0.7908 - val_loss: 0.7544 - val_accuracy: 0.6760 - val_balanced_recall: 0.8373 - val_balanced_precision: 0.5390 - val_balanced_f1_score: 0.6545 - val_auc_roc: 0.8315\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.67551 to 0.67603, saving model to ./Saved_Models/EDA_larg_uncased/EDA_all_5_large\\EDA_all_5_large\n",
      "Epoch 9/30\n",
      "1443/1443 [==============================] - 885s 613ms/step - loss: 0.7877 - accuracy: 0.6506 - balanced_recall: 0.7988 - balanced_precision: 0.5327 - balanced_f1_score: 0.6387 - auc_roc: 0.7950 - val_loss: 0.7640 - val_accuracy: 0.6760 - val_balanced_recall: 0.8171 - val_balanced_precision: 0.5621 - val_balanced_f1_score: 0.6649 - val_auc_roc: 0.8318\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.67603\n",
      "Epoch 10/30\n",
      "1443/1443 [==============================] - 886s 614ms/step - loss: 0.7766 - accuracy: 0.6581 - balanced_recall: 0.8034 - balanced_precision: 0.5359 - balanced_f1_score: 0.6425 - auc_roc: 0.7989 - val_loss: 0.7596 - val_accuracy: 0.6750 - val_balanced_recall: 0.8147 - val_balanced_precision: 0.5655 - val_balanced_f1_score: 0.6665 - val_auc_roc: 0.8338\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.67603\n",
      "Epoch 11/30\n",
      "1443/1443 [==============================] - 886s 614ms/step - loss: 0.7659 - accuracy: 0.6631 - balanced_recall: 0.8079 - balanced_precision: 0.5378 - balanced_f1_score: 0.6452 - auc_roc: 0.8027 - val_loss: 0.7685 - val_accuracy: 0.6755 - val_balanced_recall: 0.8047 - val_balanced_precision: 0.5575 - val_balanced_f1_score: 0.6575 - val_auc_roc: 0.8331\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.67603\n",
      "Epoch 12/30\n",
      "1443/1443 [==============================] - 886s 614ms/step - loss: 0.7629 - accuracy: 0.6640 - balanced_recall: 0.8094 - balanced_precision: 0.5368 - balanced_f1_score: 0.6450 - auc_roc: 0.8028 - val_loss: 0.7617 - val_accuracy: 0.6729 - val_balanced_recall: 0.8134 - val_balanced_precision: 0.5579 - val_balanced_f1_score: 0.6607 - val_auc_roc: 0.8322\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.67603\n",
      "Wall time: 2h 58min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# augmented with all = 0.5\n",
    "Accuracy_aug_all_5, Macro_F1_aug_all_5, ROC_AUC_aug_all_5, metrics_all_5 = fine_tune_BERT(X_train_all_5, X_dev_all_5, X_test_all_5, \n",
    "                                                            y_train_all_5, y_dev_orig, y_test_orig, 'EDA_all_5_large',\n",
    "                                                            learning_rate = 5e-05, epsilon=1e-08, \n",
    "                                                            train_layers = 1, epochs = 30, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6963078523140925,\n",
       " 0.684745493480006,\n",
       " 0.7639715463789166,\n",
       " {'loss': [1.2350019216537476,\n",
       "   1.1044189929962158,\n",
       "   1.039764165878296,\n",
       "   0.9939859509468079,\n",
       "   0.9481862187385559,\n",
       "   0.9214847087860107,\n",
       "   0.9025112390518188,\n",
       "   0.8812642097473145,\n",
       "   0.863515317440033,\n",
       "   0.8507573008537292,\n",
       "   0.8308382034301758,\n",
       "   0.8259435892105103,\n",
       "   0.8188992142677307,\n",
       "   0.8026054501533508,\n",
       "   0.7927019596099854,\n",
       "   0.7887464165687561],\n",
       "  'accuracy': [0.3722940981388092,\n",
       "   0.42689982056617737,\n",
       "   0.48534095287323,\n",
       "   0.5266202688217163,\n",
       "   0.557173490524292,\n",
       "   0.5732951760292053,\n",
       "   0.588441789150238,\n",
       "   0.5985178351402283,\n",
       "   0.6113892197608948,\n",
       "   0.6141844987869263,\n",
       "   0.6318013668060303,\n",
       "   0.6394071578979492,\n",
       "   0.6373919248580933,\n",
       "   0.6470129489898682,\n",
       "   0.6535786390304565,\n",
       "   0.6547487378120422],\n",
       "  'balanced_recall': [0.5611453652381897,\n",
       "   0.5756309628486633,\n",
       "   0.6212756037712097,\n",
       "   0.6626681089401245,\n",
       "   0.7011670470237732,\n",
       "   0.7143974900245667,\n",
       "   0.7278687953948975,\n",
       "   0.7397183179855347,\n",
       "   0.7323708534240723,\n",
       "   0.7370441555976868,\n",
       "   0.749464213848114,\n",
       "   0.7529025077819824,\n",
       "   0.7491657137870789,\n",
       "   0.7609905004501343,\n",
       "   0.7640836834907532,\n",
       "   0.7640645503997803],\n",
       "  'balanced_precision': [0.3443257510662079,\n",
       "   0.3621605932712555,\n",
       "   0.391535222530365,\n",
       "   0.4181511402130127,\n",
       "   0.4391366243362427,\n",
       "   0.45847105979919434,\n",
       "   0.4683148264884949,\n",
       "   0.48253971338272095,\n",
       "   0.4896879494190216,\n",
       "   0.4980260729789734,\n",
       "   0.5042990446090698,\n",
       "   0.5080097913742065,\n",
       "   0.5105346441268921,\n",
       "   0.514594316482544,\n",
       "   0.5167423486709595,\n",
       "   0.5204375982284546],\n",
       "  'balanced_f1_score': [0.4262637197971344,\n",
       "   0.44416967034339905,\n",
       "   0.4799627959728241,\n",
       "   0.512337327003479,\n",
       "   0.5396087169647217,\n",
       "   0.5579839944839478,\n",
       "   0.5694193840026855,\n",
       "   0.5835153460502625,\n",
       "   0.586395263671875,\n",
       "   0.5939074158668518,\n",
       "   0.6023446321487427,\n",
       "   0.6060656309127808,\n",
       "   0.6067549586296082,\n",
       "   0.6134887337684631,\n",
       "   0.6160497665405273,\n",
       "   0.618486225605011],\n",
       "  'auc_roc': [0.5285490155220032,\n",
       "   0.571692168712616,\n",
       "   0.6231919527053833,\n",
       "   0.6656566858291626,\n",
       "   0.7000254392623901,\n",
       "   0.7191440463066101,\n",
       "   0.731918215751648,\n",
       "   0.744792640209198,\n",
       "   0.7501001358032227,\n",
       "   0.7572970986366272,\n",
       "   0.7652295231819153,\n",
       "   0.7679980397224426,\n",
       "   0.7678985595703125,\n",
       "   0.7754861116409302,\n",
       "   0.7780461311340332,\n",
       "   0.7783997058868408],\n",
       "  'val_loss': [1.0525295734405518,\n",
       "   1.0022722482681274,\n",
       "   0.9330152273178101,\n",
       "   0.8898382782936096,\n",
       "   0.8589461445808411,\n",
       "   0.8431177735328674,\n",
       "   0.8173788189888,\n",
       "   0.8126696348190308,\n",
       "   0.7948660850524902,\n",
       "   0.7943974137306213,\n",
       "   0.7912697196006775,\n",
       "   0.7705674171447754,\n",
       "   0.7805948257446289,\n",
       "   0.7765754461288452,\n",
       "   0.775195300579071,\n",
       "   0.7746443748474121],\n",
       "  'val_accuracy': [0.46697866916656494,\n",
       "   0.5226209163665771,\n",
       "   0.568382740020752,\n",
       "   0.6053042411804199,\n",
       "   0.6209048628807068,\n",
       "   0.6349453926086426,\n",
       "   0.6375455260276794,\n",
       "   0.6474258899688721,\n",
       "   0.65782630443573,\n",
       "   0.6625065207481384,\n",
       "   0.6573063135147095,\n",
       "   0.6765470504760742,\n",
       "   0.672386884689331,\n",
       "   0.6599063873291016,\n",
       "   0.6718668937683105,\n",
       "   0.6713468432426453],\n",
       "  'val_balanced_recall': [0.6440398693084717,\n",
       "   0.661262035369873,\n",
       "   0.7159295678138733,\n",
       "   0.7330010533332825,\n",
       "   0.7512069940567017,\n",
       "   0.7575249075889587,\n",
       "   0.7824280858039856,\n",
       "   0.7627630829811096,\n",
       "   0.753669023513794,\n",
       "   0.7534973621368408,\n",
       "   0.7806198000907898,\n",
       "   0.7740883231163025,\n",
       "   0.7680513858795166,\n",
       "   0.7618517875671387,\n",
       "   0.7805618643760681,\n",
       "   0.7778332829475403],\n",
       "  'val_balanced_precision': [0.3824191391468048,\n",
       "   0.41219204664230347,\n",
       "   0.4432932138442993,\n",
       "   0.4861607849597931,\n",
       "   0.4944585859775543,\n",
       "   0.503643274307251,\n",
       "   0.5256858468055725,\n",
       "   0.5189100503921509,\n",
       "   0.5548856258392334,\n",
       "   0.5595336556434631,\n",
       "   0.518297016620636,\n",
       "   0.5605312585830688,\n",
       "   0.5432179570198059,\n",
       "   0.5720852017402649,\n",
       "   0.5407402515411377,\n",
       "   0.5398974418640137],\n",
       "  'val_balanced_f1_score': [0.47938650846481323,\n",
       "   0.5071719884872437,\n",
       "   0.5468742847442627,\n",
       "   0.5833229422569275,\n",
       "   0.5951305031776428,\n",
       "   0.6037857532501221,\n",
       "   0.6275781989097595,\n",
       "   0.6163646578788757,\n",
       "   0.638207733631134,\n",
       "   0.641305685043335,\n",
       "   0.6216852068901062,\n",
       "   0.6492084264755249,\n",
       "   0.6353707909584045,\n",
       "   0.6524214744567871,\n",
       "   0.6376376748085022,\n",
       "   0.6362173557281494],\n",
       "  'val_auc_roc': [0.6052205562591553,\n",
       "   0.6554856300354004,\n",
       "   0.7147119045257568,\n",
       "   0.7435359954833984,\n",
       "   0.7629656195640564,\n",
       "   0.7700000405311584,\n",
       "   0.7820687294006348,\n",
       "   0.7864206433296204,\n",
       "   0.7892406582832336,\n",
       "   0.7902766466140747,\n",
       "   0.7973363995552063,\n",
       "   0.7992877960205078,\n",
       "   0.797543466091156,\n",
       "   0.7953284382820129,\n",
       "   0.8004627227783203,\n",
       "   0.8010916113853455]})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Accuracy_orig, Macro_F1_orig, ROC_AUC_orig, metrics_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6989079563182528,\n",
       " 0.6758681084374049,\n",
       " 0.7669763868547642,\n",
       " {'loss': [1.0434372425079346,\n",
       "   0.8761473298072815,\n",
       "   0.8293533325195312,\n",
       "   0.7980645298957825,\n",
       "   0.7752760648727417,\n",
       "   0.7560654282569885,\n",
       "   0.7403826117515564,\n",
       "   0.7245800495147705,\n",
       "   0.7037743330001831,\n",
       "   0.6819196939468384,\n",
       "   0.6545854210853577,\n",
       "   0.6159468293190002,\n",
       "   0.5733673572540283],\n",
       "  'accuracy': [0.47498321533203125,\n",
       "   0.5964376330375671,\n",
       "   0.6250298023223877,\n",
       "   0.6467528939247131,\n",
       "   0.6612386107444763,\n",
       "   0.6696678400039673,\n",
       "   0.6778694987297058,\n",
       "   0.6866670846939087,\n",
       "   0.6957789063453674,\n",
       "   0.7086068987846375,\n",
       "   0.721229076385498,\n",
       "   0.7429630160331726,\n",
       "   0.7596264481544495],\n",
       "  'balanced_recall': [0.40563106536865234,\n",
       "   0.5316692590713501,\n",
       "   0.5047776103019714,\n",
       "   0.49748098850250244,\n",
       "   0.5195180773735046,\n",
       "   0.5215157866477966,\n",
       "   0.5363613963127136,\n",
       "   0.5456236004829407,\n",
       "   0.5514771938323975,\n",
       "   0.5644664168357849,\n",
       "   0.5748149752616882,\n",
       "   0.5796377658843994,\n",
       "   0.6008470058441162],\n",
       "  'balanced_precision': [0.4064735472202301,\n",
       "   0.4454438388347626,\n",
       "   0.43617862462997437,\n",
       "   0.42902588844299316,\n",
       "   0.43074414134025574,\n",
       "   0.43140506744384766,\n",
       "   0.43209731578826904,\n",
       "   0.43445053696632385,\n",
       "   0.4372493028640747,\n",
       "   0.43994325399398804,\n",
       "   0.44448018074035645,\n",
       "   0.4506281316280365,\n",
       "   0.45745110511779785],\n",
       "  'balanced_f1_score': [0.4015403389930725,\n",
       "   0.4828692078590393,\n",
       "   0.4660087525844574,\n",
       "   0.4588490128517151,\n",
       "   0.4691218435764313,\n",
       "   0.4705859124660492,\n",
       "   0.4771121144294739,\n",
       "   0.4821532964706421,\n",
       "   0.4861432909965515,\n",
       "   0.492962509393692,\n",
       "   0.49976757168769836,\n",
       "   0.5055434703826904,\n",
       "   0.5180624723434448],\n",
       "  'auc_roc': [0.5919449329376221,\n",
       "   0.6456250548362732,\n",
       "   0.6315693259239197,\n",
       "   0.6204530000686646,\n",
       "   0.6211197376251221,\n",
       "   0.619644820690155,\n",
       "   0.6218732595443726,\n",
       "   0.6235845685005188,\n",
       "   0.6244639754295349,\n",
       "   0.6289613246917725,\n",
       "   0.6341566443443298,\n",
       "   0.6422239542007446,\n",
       "   0.651189386844635],\n",
       "  'val_loss': [0.8438993096351624,\n",
       "   0.7653411030769348,\n",
       "   0.7531068921089172,\n",
       "   0.7553134560585022,\n",
       "   0.7536023855209351,\n",
       "   0.7752088904380798,\n",
       "   0.7658812999725342,\n",
       "   0.7824642062187195,\n",
       "   0.8181111812591553,\n",
       "   0.8387115001678467,\n",
       "   0.9057763814926147,\n",
       "   1.0269229412078857,\n",
       "   1.0953023433685303],\n",
       "  'val_accuracy': [0.6261050701141357,\n",
       "   0.65782630443573,\n",
       "   0.6677067279815674,\n",
       "   0.65782630443573,\n",
       "   0.6749870181083679,\n",
       "   0.6744669675827026,\n",
       "   0.672386884689331,\n",
       "   0.6765470504760742,\n",
       "   0.6807072162628174,\n",
       "   0.6765470504760742,\n",
       "   0.6770671010017395,\n",
       "   0.6749870181083679,\n",
       "   0.6619864702224731],\n",
       "  'val_balanced_recall': [0.507338285446167,\n",
       "   0.5043650269508362,\n",
       "   0.42130839824676514,\n",
       "   0.4461445212364197,\n",
       "   0.412957102060318,\n",
       "   0.45850005745887756,\n",
       "   0.46456074714660645,\n",
       "   0.4511304795742035,\n",
       "   0.5029586553573608,\n",
       "   0.4325806796550751,\n",
       "   0.4920175075531006,\n",
       "   0.4740122854709625,\n",
       "   0.5032930374145508],\n",
       "  'val_balanced_precision': [0.4579435884952545,\n",
       "   0.439104825258255,\n",
       "   0.3760665953159332,\n",
       "   0.37300387024879456,\n",
       "   0.34818658232688904,\n",
       "   0.35805290937423706,\n",
       "   0.3613280951976776,\n",
       "   0.3495645523071289,\n",
       "   0.3620074391365051,\n",
       "   0.3605384826660156,\n",
       "   0.36938461661338806,\n",
       "   0.3682357668876648,\n",
       "   0.36482784152030945],\n",
       "  'val_balanced_f1_score': [0.4792060852050781,\n",
       "   0.4682948887348175,\n",
       "   0.39615029096603394,\n",
       "   0.4052836298942566,\n",
       "   0.37669795751571655,\n",
       "   0.40115299820899963,\n",
       "   0.4054832458496094,\n",
       "   0.3929429352283478,\n",
       "   0.4200747311115265,\n",
       "   0.39221322536468506,\n",
       "   0.42105022072792053,\n",
       "   0.4134921431541443,\n",
       "   0.42189714312553406],\n",
       "  'val_auc_roc': [0.6707668304443359,\n",
       "   0.6711480617523193,\n",
       "   0.6507282257080078,\n",
       "   0.6415140628814697,\n",
       "   0.6342334747314453,\n",
       "   0.627493679523468,\n",
       "   0.630108118057251,\n",
       "   0.6288655400276184,\n",
       "   0.6240329742431641,\n",
       "   0.6364552974700928,\n",
       "   0.6266679167747498,\n",
       "   0.628333568572998,\n",
       "   0.6234449744224548]})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Accuracy_aug_sr, Macro_F1_aug_sr, ROC_AUC_aug_sr, metrics_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy_aug_ri, Macro_F1_aug_ri, ROC_AUC_aug_ri, metrics_ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy_aug_rs, Macro_F1_aug_rs, ROC_AUC_aug_rs, metrics_rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy_aug_rd, Macro_F1_aug_rd, ROC_AUC_aug_rd, metrics_rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy_aug_all_1, Macro_F1_aug_all_1, ROC_AUC_aug_all_1, metrics_all_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy_aug_all_5, Macro_F1_aug_all_5, ROC_AUC_aug_all_5, metrics_all_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_name_list = ['Original Data', 'Augmented SR 0.1', 'Augmented RI 0.1', \n",
    "                   'Augmented RS 0.1', 'Augmented RD 0.1', 'Augmented All 0.1', 'Augmented All 0.5']\n",
    "\n",
    "acc_list = [Accuracy_orig, Accuracy_aug_sr, Accuracy_aug_ri, Accuracy_aug_rs, \n",
    "            Accuracy_aug_rd, Accuracy_aug_all_1, Accuracy_aug_all_5]\n",
    "\n",
    "macro_f1_list = [Macro_F1_orig, Macro_F1_aug_sr, Macro_F1_aug_ri, Macro_F1_aug_rs, \n",
    "                 Macro_F1_aug_rd, Macro_F1_aug_all_1, Macro_F1_aug_all_5]\n",
    "\n",
    "roc_auc_list = [ROC_AUC_orig, ROC_AUC_aug_sr, ROC_AUC_aug_ri, ROC_AUC_aug_rs, \n",
    "                ROC_AUC_aug_rd, ROC_AUC_aug_all_1, ROC_AUC_aug_all_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {'Trial Name' : trial_name_list, 'Test Accuracy Score' : acc_list, \n",
    "               'Test Macro F1 Score' : macro_f1_list, 'Test ROC AUC Score' : roc_auc_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trial Name</th>\n",
       "      <th>Test Accuracy Score</th>\n",
       "      <th>Test Macro F1 Score</th>\n",
       "      <th>Test ROC AUC Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Original Data</td>\n",
       "      <td>0.696308</td>\n",
       "      <td>0.684745</td>\n",
       "      <td>0.763972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Augmented SR 0.1</td>\n",
       "      <td>0.698908</td>\n",
       "      <td>0.675868</td>\n",
       "      <td>0.766976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Augmented RI 0.1</td>\n",
       "      <td>0.699428</td>\n",
       "      <td>0.687185</td>\n",
       "      <td>0.767257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Augmented RS 0.1</td>\n",
       "      <td>0.683827</td>\n",
       "      <td>0.675714</td>\n",
       "      <td>0.758049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Augmented RD 0.1</td>\n",
       "      <td>0.695788</td>\n",
       "      <td>0.682732</td>\n",
       "      <td>0.764686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Augmented All 0.1</td>\n",
       "      <td>0.679147</td>\n",
       "      <td>0.666963</td>\n",
       "      <td>0.754397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Augmented All 0.5</td>\n",
       "      <td>0.704108</td>\n",
       "      <td>0.688736</td>\n",
       "      <td>0.769812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Trial Name  Test Accuracy Score  Test Macro F1 Score  \\\n",
       "0      Original Data             0.696308             0.684745   \n",
       "1   Augmented SR 0.1             0.698908             0.675868   \n",
       "2   Augmented RI 0.1             0.699428             0.687185   \n",
       "3   Augmented RS 0.1             0.683827             0.675714   \n",
       "4   Augmented RD 0.1             0.695788             0.682732   \n",
       "5  Augmented All 0.1             0.679147             0.666963   \n",
       "6  Augmented All 0.5             0.704108             0.688736   \n",
       "\n",
       "   Test ROC AUC Score  \n",
       "0            0.763972  \n",
       "1            0.766976  \n",
       "2            0.767257  \n",
       "3            0.758049  \n",
       "4            0.764686  \n",
       "5            0.754397  \n",
       "6            0.769812  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(result_dict)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('./Saved_Models/EDA_base_uncased/All_DA_BERT_large_uncased.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>balanced_recall</th>\n",
       "      <th>balanced_precision</th>\n",
       "      <th>balanced_f1_score</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_balanced_recall</th>\n",
       "      <th>val_balanced_precision</th>\n",
       "      <th>val_balanced_f1_score</th>\n",
       "      <th>val_auc_roc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.235002</td>\n",
       "      <td>0.372294</td>\n",
       "      <td>0.561145</td>\n",
       "      <td>0.344326</td>\n",
       "      <td>0.426264</td>\n",
       "      <td>0.528549</td>\n",
       "      <td>1.052530</td>\n",
       "      <td>0.466979</td>\n",
       "      <td>0.644040</td>\n",
       "      <td>0.382419</td>\n",
       "      <td>0.479387</td>\n",
       "      <td>0.605221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.104419</td>\n",
       "      <td>0.426900</td>\n",
       "      <td>0.575631</td>\n",
       "      <td>0.362161</td>\n",
       "      <td>0.444170</td>\n",
       "      <td>0.571692</td>\n",
       "      <td>1.002272</td>\n",
       "      <td>0.522621</td>\n",
       "      <td>0.661262</td>\n",
       "      <td>0.412192</td>\n",
       "      <td>0.507172</td>\n",
       "      <td>0.655486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.039764</td>\n",
       "      <td>0.485341</td>\n",
       "      <td>0.621276</td>\n",
       "      <td>0.391535</td>\n",
       "      <td>0.479963</td>\n",
       "      <td>0.623192</td>\n",
       "      <td>0.933015</td>\n",
       "      <td>0.568383</td>\n",
       "      <td>0.715930</td>\n",
       "      <td>0.443293</td>\n",
       "      <td>0.546874</td>\n",
       "      <td>0.714712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.993986</td>\n",
       "      <td>0.526620</td>\n",
       "      <td>0.662668</td>\n",
       "      <td>0.418151</td>\n",
       "      <td>0.512337</td>\n",
       "      <td>0.665657</td>\n",
       "      <td>0.889838</td>\n",
       "      <td>0.605304</td>\n",
       "      <td>0.733001</td>\n",
       "      <td>0.486161</td>\n",
       "      <td>0.583323</td>\n",
       "      <td>0.743536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.948186</td>\n",
       "      <td>0.557173</td>\n",
       "      <td>0.701167</td>\n",
       "      <td>0.439137</td>\n",
       "      <td>0.539609</td>\n",
       "      <td>0.700025</td>\n",
       "      <td>0.858946</td>\n",
       "      <td>0.620905</td>\n",
       "      <td>0.751207</td>\n",
       "      <td>0.494459</td>\n",
       "      <td>0.595131</td>\n",
       "      <td>0.762966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.921485</td>\n",
       "      <td>0.573295</td>\n",
       "      <td>0.714397</td>\n",
       "      <td>0.458471</td>\n",
       "      <td>0.557984</td>\n",
       "      <td>0.719144</td>\n",
       "      <td>0.843118</td>\n",
       "      <td>0.634945</td>\n",
       "      <td>0.757525</td>\n",
       "      <td>0.503643</td>\n",
       "      <td>0.603786</td>\n",
       "      <td>0.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.902511</td>\n",
       "      <td>0.588442</td>\n",
       "      <td>0.727869</td>\n",
       "      <td>0.468315</td>\n",
       "      <td>0.569419</td>\n",
       "      <td>0.731918</td>\n",
       "      <td>0.817379</td>\n",
       "      <td>0.637546</td>\n",
       "      <td>0.782428</td>\n",
       "      <td>0.525686</td>\n",
       "      <td>0.627578</td>\n",
       "      <td>0.782069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.881264</td>\n",
       "      <td>0.598518</td>\n",
       "      <td>0.739718</td>\n",
       "      <td>0.482540</td>\n",
       "      <td>0.583515</td>\n",
       "      <td>0.744793</td>\n",
       "      <td>0.812670</td>\n",
       "      <td>0.647426</td>\n",
       "      <td>0.762763</td>\n",
       "      <td>0.518910</td>\n",
       "      <td>0.616365</td>\n",
       "      <td>0.786421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.863515</td>\n",
       "      <td>0.611389</td>\n",
       "      <td>0.732371</td>\n",
       "      <td>0.489688</td>\n",
       "      <td>0.586395</td>\n",
       "      <td>0.750100</td>\n",
       "      <td>0.794866</td>\n",
       "      <td>0.657826</td>\n",
       "      <td>0.753669</td>\n",
       "      <td>0.554886</td>\n",
       "      <td>0.638208</td>\n",
       "      <td>0.789241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.850757</td>\n",
       "      <td>0.614184</td>\n",
       "      <td>0.737044</td>\n",
       "      <td>0.498026</td>\n",
       "      <td>0.593907</td>\n",
       "      <td>0.757297</td>\n",
       "      <td>0.794397</td>\n",
       "      <td>0.662507</td>\n",
       "      <td>0.753497</td>\n",
       "      <td>0.559534</td>\n",
       "      <td>0.641306</td>\n",
       "      <td>0.790277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.830838</td>\n",
       "      <td>0.631801</td>\n",
       "      <td>0.749464</td>\n",
       "      <td>0.504299</td>\n",
       "      <td>0.602345</td>\n",
       "      <td>0.765230</td>\n",
       "      <td>0.791270</td>\n",
       "      <td>0.657306</td>\n",
       "      <td>0.780620</td>\n",
       "      <td>0.518297</td>\n",
       "      <td>0.621685</td>\n",
       "      <td>0.797336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.825944</td>\n",
       "      <td>0.639407</td>\n",
       "      <td>0.752903</td>\n",
       "      <td>0.508010</td>\n",
       "      <td>0.606066</td>\n",
       "      <td>0.767998</td>\n",
       "      <td>0.770567</td>\n",
       "      <td>0.676547</td>\n",
       "      <td>0.774088</td>\n",
       "      <td>0.560531</td>\n",
       "      <td>0.649208</td>\n",
       "      <td>0.799288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.818899</td>\n",
       "      <td>0.637392</td>\n",
       "      <td>0.749166</td>\n",
       "      <td>0.510535</td>\n",
       "      <td>0.606755</td>\n",
       "      <td>0.767899</td>\n",
       "      <td>0.780595</td>\n",
       "      <td>0.672387</td>\n",
       "      <td>0.768051</td>\n",
       "      <td>0.543218</td>\n",
       "      <td>0.635371</td>\n",
       "      <td>0.797543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.802605</td>\n",
       "      <td>0.647013</td>\n",
       "      <td>0.760991</td>\n",
       "      <td>0.514594</td>\n",
       "      <td>0.613489</td>\n",
       "      <td>0.775486</td>\n",
       "      <td>0.776575</td>\n",
       "      <td>0.659906</td>\n",
       "      <td>0.761852</td>\n",
       "      <td>0.572085</td>\n",
       "      <td>0.652421</td>\n",
       "      <td>0.795328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.792702</td>\n",
       "      <td>0.653579</td>\n",
       "      <td>0.764084</td>\n",
       "      <td>0.516742</td>\n",
       "      <td>0.616050</td>\n",
       "      <td>0.778046</td>\n",
       "      <td>0.775195</td>\n",
       "      <td>0.671867</td>\n",
       "      <td>0.780562</td>\n",
       "      <td>0.540740</td>\n",
       "      <td>0.637638</td>\n",
       "      <td>0.800463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.788746</td>\n",
       "      <td>0.654749</td>\n",
       "      <td>0.764065</td>\n",
       "      <td>0.520438</td>\n",
       "      <td>0.618486</td>\n",
       "      <td>0.778400</td>\n",
       "      <td>0.774644</td>\n",
       "      <td>0.671347</td>\n",
       "      <td>0.777833</td>\n",
       "      <td>0.539897</td>\n",
       "      <td>0.636217</td>\n",
       "      <td>0.801092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  accuracy  balanced_recall  balanced_precision  \\\n",
       "0   1.235002  0.372294         0.561145            0.344326   \n",
       "1   1.104419  0.426900         0.575631            0.362161   \n",
       "2   1.039764  0.485341         0.621276            0.391535   \n",
       "3   0.993986  0.526620         0.662668            0.418151   \n",
       "4   0.948186  0.557173         0.701167            0.439137   \n",
       "5   0.921485  0.573295         0.714397            0.458471   \n",
       "6   0.902511  0.588442         0.727869            0.468315   \n",
       "7   0.881264  0.598518         0.739718            0.482540   \n",
       "8   0.863515  0.611389         0.732371            0.489688   \n",
       "9   0.850757  0.614184         0.737044            0.498026   \n",
       "10  0.830838  0.631801         0.749464            0.504299   \n",
       "11  0.825944  0.639407         0.752903            0.508010   \n",
       "12  0.818899  0.637392         0.749166            0.510535   \n",
       "13  0.802605  0.647013         0.760991            0.514594   \n",
       "14  0.792702  0.653579         0.764084            0.516742   \n",
       "15  0.788746  0.654749         0.764065            0.520438   \n",
       "\n",
       "    balanced_f1_score   auc_roc  val_loss  val_accuracy  val_balanced_recall  \\\n",
       "0            0.426264  0.528549  1.052530      0.466979             0.644040   \n",
       "1            0.444170  0.571692  1.002272      0.522621             0.661262   \n",
       "2            0.479963  0.623192  0.933015      0.568383             0.715930   \n",
       "3            0.512337  0.665657  0.889838      0.605304             0.733001   \n",
       "4            0.539609  0.700025  0.858946      0.620905             0.751207   \n",
       "5            0.557984  0.719144  0.843118      0.634945             0.757525   \n",
       "6            0.569419  0.731918  0.817379      0.637546             0.782428   \n",
       "7            0.583515  0.744793  0.812670      0.647426             0.762763   \n",
       "8            0.586395  0.750100  0.794866      0.657826             0.753669   \n",
       "9            0.593907  0.757297  0.794397      0.662507             0.753497   \n",
       "10           0.602345  0.765230  0.791270      0.657306             0.780620   \n",
       "11           0.606066  0.767998  0.770567      0.676547             0.774088   \n",
       "12           0.606755  0.767899  0.780595      0.672387             0.768051   \n",
       "13           0.613489  0.775486  0.776575      0.659906             0.761852   \n",
       "14           0.616050  0.778046  0.775195      0.671867             0.780562   \n",
       "15           0.618486  0.778400  0.774644      0.671347             0.777833   \n",
       "\n",
       "    val_balanced_precision  val_balanced_f1_score  val_auc_roc  \n",
       "0                 0.382419               0.479387     0.605221  \n",
       "1                 0.412192               0.507172     0.655486  \n",
       "2                 0.443293               0.546874     0.714712  \n",
       "3                 0.486161               0.583323     0.743536  \n",
       "4                 0.494459               0.595131     0.762966  \n",
       "5                 0.503643               0.603786     0.770000  \n",
       "6                 0.525686               0.627578     0.782069  \n",
       "7                 0.518910               0.616365     0.786421  \n",
       "8                 0.554886               0.638208     0.789241  \n",
       "9                 0.559534               0.641306     0.790277  \n",
       "10                0.518297               0.621685     0.797336  \n",
       "11                0.560531               0.649208     0.799288  \n",
       "12                0.543218               0.635371     0.797543  \n",
       "13                0.572085               0.652421     0.795328  \n",
       "14                0.540740               0.637638     0.800463  \n",
       "15                0.539897               0.636217     0.801092  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_org_df = pd.DataFrame(metrics_orig)\n",
    "\n",
    "metrics_org_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_list = [metrics_orig, metrics_sr, metrics_ri, metrics_rs, metrics_rd, metrics_all_1, metrics_all_5]\n",
    "name_list = ['fit_metrics_orig.csv', 'fit_metrics_sr.csv', 'fit_metrics_ri.csv', 'fit_metrics_rs.csv', 'fit_metrics_rd.csv', 'fit_metrics_all_1.csv', 'fit_metrics_all_5.csv']\n",
    "\n",
    "i = 0\n",
    "for m in metrics_list:\n",
    "    df = pd.DataFrame(m)\n",
    "    df.to_csv(name_list[i])\n",
    "    i += 1\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOeNGBZaTc3vi1iokDn5dgn",
   "collapsed_sections": [],
   "name": "Data_Processing",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02d7e8c53bf94bf48c06cea9abbd9aba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bf38f0560d3e493ca2b97fd974d00462",
       "IPY_MODEL_8a60b779e7d749d284b791b9e5126a62",
       "IPY_MODEL_2e51f1cad2b84893b537beb7e4ebf67c"
      ],
      "layout": "IPY_MODEL_d57bc505733b4c738cf84db24feba360"
     }
    },
    "047bcb706b304be09200793be7524708": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "071ca09b48bf4d9989852d66f8d57642": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8aa487d37b284b2ba88903923a0086d9",
       "IPY_MODEL_b3ba42a7365d418f9cbf7f68ede3b65a",
       "IPY_MODEL_c7e4cea875eb41c8950f97350c237730"
      ],
      "layout": "IPY_MODEL_15db3b4b336b41cdb973852bb3fef72f"
     }
    },
    "0c234fac6d10482089dacd2c03a5bbde": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12bf3b8f0bab4586847a07e8c438dafa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e67ba177d3744ea4bcc652774cd13abd",
       "IPY_MODEL_43eb8a62436a4c149e25f98f49a7c68d",
       "IPY_MODEL_3723a8c5f8eb47efa71e50a5ef924123"
      ],
      "layout": "IPY_MODEL_8039ef2f59ee482781f325f5a91b93e8"
     }
    },
    "15db3b4b336b41cdb973852bb3fef72f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b1a083fc8e643a78f055105960b523e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "263134a33eaa4345926dd786256c08eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2af599ea5c3d4874aff863303c5b5703": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b473e96d36604d71ae5d3d8e69cc01c3",
      "placeholder": "​",
      "style": "IPY_MODEL_263134a33eaa4345926dd786256c08eb",
      "value": "Downloading: 100%"
     }
    },
    "2e51f1cad2b84893b537beb7e4ebf67c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3df12951362451685bea68b7091c69f",
      "placeholder": "​",
      "style": "IPY_MODEL_b8b44bed109443fd9bba63a7039f3c93",
      "value": " 226k/226k [00:00&lt;00:00, 3.21MB/s]"
     }
    },
    "3723a8c5f8eb47efa71e50a5ef924123": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b1a083fc8e643a78f055105960b523e",
      "placeholder": "​",
      "style": "IPY_MODEL_c26d9b9e910a43febfd588927c3e7756",
      "value": " 299/299 [00:00&lt;00:00, 5.18kB/s]"
     }
    },
    "3dfc3ee445f2403a8ed7b4e6f5a8b3d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "43eb8a62436a4c149e25f98f49a7c68d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7942944ae43141c0aa83ec76fc0deafa",
      "max": 299,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5d2efc2c1758475c9d1cd51cdc7edf0a",
      "value": 299
     }
    },
    "45b7612393254fe19709bdae7d7bbbe6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4656eb21ff2245b7be51f82c1e2c7e6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2af599ea5c3d4874aff863303c5b5703",
       "IPY_MODEL_c64b940afcc94ee3b2b88c582b54d97f",
       "IPY_MODEL_8a798f3e801d47df8771840991f7cfa3"
      ],
      "layout": "IPY_MODEL_ab8f5f020c76459aad12d4cab6a36a72"
     }
    },
    "5d2efc2c1758475c9d1cd51cdc7edf0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5e02a09e9fe24718a8f1f4167c42d099": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "70f68856bdb64c30aa57e6046468b3d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7942944ae43141c0aa83ec76fc0deafa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d3ff64481b64406aadb34439d2ad02b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7d77d8f8c129400d9f61b60da03891b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_98efd1135c4f438c8b8dd5c130cc667c",
       "IPY_MODEL_9aca8f0c4013472c8514d2d57f9ea3e8",
       "IPY_MODEL_807b39071d354a1ab7af51a05f1ef3b1"
      ],
      "layout": "IPY_MODEL_971c955528914b05afe5d12e9c626cf1"
     }
    },
    "8039ef2f59ee482781f325f5a91b93e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "807b39071d354a1ab7af51a05f1ef3b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc6b1e6a0e254b82b6c7277f10a5d542",
      "placeholder": "​",
      "style": "IPY_MODEL_5e02a09e9fe24718a8f1f4167c42d099",
      "value": " 112/112 [00:00&lt;00:00, 2.56kB/s]"
     }
    },
    "8a60b779e7d749d284b791b9e5126a62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c234fac6d10482089dacd2c03a5bbde",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_70f68856bdb64c30aa57e6046468b3d6",
      "value": 231508
     }
    },
    "8a798f3e801d47df8771840991f7cfa3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_047bcb706b304be09200793be7524708",
      "placeholder": "​",
      "style": "IPY_MODEL_c4792815ec0d4591bcaea2e7a0539e6b",
      "value": " 790/790 [00:00&lt;00:00, 14.6kB/s]"
     }
    },
    "8aa487d37b284b2ba88903923a0086d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee76f1517ed64f4fa49d998d2448f9c2",
      "placeholder": "​",
      "style": "IPY_MODEL_e88a61936e0949c490366d0619869b6c",
      "value": "Downloading: 100%"
     }
    },
    "971c955528914b05afe5d12e9c626cf1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98efd1135c4f438c8b8dd5c130cc667c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b575b556bb9445ceb5c662b9d224e171",
      "placeholder": "​",
      "style": "IPY_MODEL_acd4639a73904507b7884adc96512dc5",
      "value": "Downloading: 100%"
     }
    },
    "9aca8f0c4013472c8514d2d57f9ea3e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d2803d4a8a794a83a2ebcf61ce3097ff",
      "max": 112,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f9a30d0d10744d14a47b500251c1973c",
      "value": 112
     }
    },
    "9e9b4804607046678d3d132528486b0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ab8f5f020c76459aad12d4cab6a36a72": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "acd4639a73904507b7884adc96512dc5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b3ba42a7365d418f9cbf7f68ede3b65a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d12cbe52662546d1be82cfd4536dfdfc",
      "max": 437996231,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_45b7612393254fe19709bdae7d7bbbe6",
      "value": 437996231
     }
    },
    "b473e96d36604d71ae5d3d8e69cc01c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b575b556bb9445ceb5c662b9d224e171": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b8b44bed109443fd9bba63a7039f3c93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf38f0560d3e493ca2b97fd974d00462": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_feb0fac998144444bfd9373ce537bbc3",
      "placeholder": "​",
      "style": "IPY_MODEL_7d3ff64481b64406aadb34439d2ad02b",
      "value": "Downloading: 100%"
     }
    },
    "c26d9b9e910a43febfd588927c3e7756": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c3df12951362451685bea68b7091c69f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4792815ec0d4591bcaea2e7a0539e6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c64b940afcc94ee3b2b88c582b54d97f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e35113cc44f34a1fbb22f109f49f7bdd",
      "max": 790,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3dfc3ee445f2403a8ed7b4e6f5a8b3d3",
      "value": 790
     }
    },
    "c7e4cea875eb41c8950f97350c237730": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d7abb004da9d42b787717cfdb3657f4e",
      "placeholder": "​",
      "style": "IPY_MODEL_ee89c9881c75441fbc9ec827d623134e",
      "value": " 418M/418M [00:11&lt;00:00, 37.0MB/s]"
     }
    },
    "d12cbe52662546d1be82cfd4536dfdfc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2803d4a8a794a83a2ebcf61ce3097ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d57bc505733b4c738cf84db24feba360": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7abb004da9d42b787717cfdb3657f4e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e35113cc44f34a1fbb22f109f49f7bdd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e567a77fb29d47c98fe19c440b2d31e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e67ba177d3744ea4bcc652774cd13abd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e567a77fb29d47c98fe19c440b2d31e8",
      "placeholder": "​",
      "style": "IPY_MODEL_9e9b4804607046678d3d132528486b0b",
      "value": "Downloading: 100%"
     }
    },
    "e88a61936e0949c490366d0619869b6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ee76f1517ed64f4fa49d998d2448f9c2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee89c9881c75441fbc9ec827d623134e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f9a30d0d10744d14a47b500251c1973c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fc6b1e6a0e254b82b6c7277f10a5d542": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "feb0fac998144444bfd9373ce537bbc3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
