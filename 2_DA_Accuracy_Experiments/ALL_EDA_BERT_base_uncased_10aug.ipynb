{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KSqG-xgDLRI"
   },
   "source": [
    "### All Easy DA - BERT Base Uncased\n",
    "\n",
    "#### Un-augmented test set\n",
    "#### Augment only the training set - 10 augmented examples per original\n",
    "\n",
    "#### Get Original Paper Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1646721905242,
     "user": {
      "displayName": "Evan Chan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgGKtlehGoFssPGs4v0Yns-qfVPjW1FuloVAn-GMw=s64",
      "userId": "16754265128573798261"
     },
     "user_tz": 360
    },
    "id": "y6DHMdiyDSTl"
   },
   "outputs": [],
   "source": [
    "# !pip install sklearn\n",
    "# !pip install ekphrasis\n",
    "# !pip install transformers\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import transformers\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import os\n",
    "\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1646721905243,
     "user": {
      "displayName": "Evan Chan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgGKtlehGoFssPGs4v0Yns-qfVPjW1FuloVAn-GMw=s64",
      "userId": "16754265128573798261"
     },
     "user_tz": 360
    },
    "id": "X-o7OyjjDYwr"
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.classes_ = np.load('../Data/classes.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.16.2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test, train, dev examples from base notebook\n",
    "\n",
    "train_data_df = pd.read_csv('./Saved_Models/EDA_base_uncased_5aug/All_DA_BERT_base_uncased_train_examples.csv')\n",
    "dev_data_df = pd.read_csv('./Saved_Models/EDA_base_uncased_5aug/All_DA_BERT_base_uncased_dev_examples.csv')\n",
    "test_data_df = pd.read_csv('./Saved_Models/EDA_base_uncased_5aug/All_DA_BERT_base_uncased_test_examples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1646721916193,
     "user": {
      "displayName": "Evan Chan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgGKtlehGoFssPGs4v0Yns-qfVPjW1FuloVAn-GMw=s64",
      "userId": "16754265128573798261"
     },
     "user_tz": 360
    },
    "id": "zOn6K2RgJBkA"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>target1</th>\n",
       "      <th>target2</th>\n",
       "      <th>target3</th>\n",
       "      <th>rationales</th>\n",
       "      <th>final_label</th>\n",
       "      <th>text_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22448349_gab</td>\n",
       "      <td>['common', 'core', 'weed', 'too', 'much', 'rit...</td>\n",
       "      <td>['Men', 'Women']</td>\n",
       "      <td>['Women']</td>\n",
       "      <td>['None']</td>\n",
       "      <td>[]</td>\n",
       "      <td>normal</td>\n",
       "      <td>common core weed too much ritalan chem trails ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1178948520201637888_twitter</td>\n",
       "      <td>['took', 'my', 'nan', 'to', 'the', 'hospital',...</td>\n",
       "      <td>['None']</td>\n",
       "      <td>['None']</td>\n",
       "      <td>['None']</td>\n",
       "      <td>[]</td>\n",
       "      <td>normal</td>\n",
       "      <td>took my nan to the hospital for a x ray i turn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1482573_gab</td>\n",
       "      <td>['&lt;user&gt;', 'well', 'not', 'really', 'islam', '...</td>\n",
       "      <td>['Islam']</td>\n",
       "      <td>['Other']</td>\n",
       "      <td>['Islam']</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,...</td>\n",
       "      <td>offensive</td>\n",
       "      <td>&lt;user&gt; well not really islam does not care for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1097184028149587969_twitter</td>\n",
       "      <td>['&lt;user&gt;', 'france', 'in', '&lt;number&gt;', 'after'...</td>\n",
       "      <td>['Islam', 'Other']</td>\n",
       "      <td>['Islam']</td>\n",
       "      <td>['Islam']</td>\n",
       "      <td>[]</td>\n",
       "      <td>normal</td>\n",
       "      <td>&lt;user&gt; france in &lt;number&gt; after muslims take o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1089569255111176192_twitter</td>\n",
       "      <td>['i', 'will', 'not', 'tolerate', 'non', 'arab'...</td>\n",
       "      <td>['Arab', 'Men', 'Women']</td>\n",
       "      <td>['Arab']</td>\n",
       "      <td>['Arab', 'Islam']</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,...</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>i will not tolerate non arab women slandering ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                      post_id  \\\n",
       "0           0                 22448349_gab   \n",
       "1           1  1178948520201637888_twitter   \n",
       "2           2                  1482573_gab   \n",
       "3           3  1097184028149587969_twitter   \n",
       "4           4  1089569255111176192_twitter   \n",
       "\n",
       "                                                text  \\\n",
       "0  ['common', 'core', 'weed', 'too', 'much', 'rit...   \n",
       "1  ['took', 'my', 'nan', 'to', 'the', 'hospital',...   \n",
       "2  ['<user>', 'well', 'not', 'really', 'islam', '...   \n",
       "3  ['<user>', 'france', 'in', '<number>', 'after'...   \n",
       "4  ['i', 'will', 'not', 'tolerate', 'non', 'arab'...   \n",
       "\n",
       "                    target1    target2            target3  \\\n",
       "0          ['Men', 'Women']  ['Women']           ['None']   \n",
       "1                  ['None']   ['None']           ['None']   \n",
       "2                 ['Islam']  ['Other']          ['Islam']   \n",
       "3        ['Islam', 'Other']  ['Islam']          ['Islam']   \n",
       "4  ['Arab', 'Men', 'Women']   ['Arab']  ['Arab', 'Islam']   \n",
       "\n",
       "                                          rationales final_label  \\\n",
       "0                                                 []      normal   \n",
       "1                                                 []      normal   \n",
       "2  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,...   offensive   \n",
       "3                                                 []      normal   \n",
       "4  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,...  hatespeech   \n",
       "\n",
       "                                       text_combined  \n",
       "0  common core weed too much ritalan chem trails ...  \n",
       "1  took my nan to the hospital for a x ray i turn...  \n",
       "2  <user> well not really islam does not care for...  \n",
       "3  <user> france in <number> after muslims take o...  \n",
       "4  i will not tolerate non arab women slandering ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2005,
     "status": "ok",
     "timestamp": 1646722287248,
     "user": {
      "displayName": "Evan Chan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgGKtlehGoFssPGs4v0Yns-qfVPjW1FuloVAn-GMw=s64",
      "userId": "16754265128573798261"
     },
     "user_tz": 360
    },
    "id": "wr-vv22ZMhT-"
   },
   "outputs": [],
   "source": [
    "X_train_id = train_data_df['post_id']\n",
    "X_test_id = test_data_df['post_id']\n",
    "X_dev_id = dev_data_df['post_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data_df['final_label']\n",
    "y_test = test_data_df['final_label']\n",
    "y_dev = dev_data_df['final_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15383\n",
      "1923\n",
      "1923\n"
     ]
    }
   ],
   "source": [
    "x_train_df = pd.DataFrame({'post_id' : X_train_id.to_list()})\n",
    "x_dev_df = pd.DataFrame({'post_id' : X_dev_id.to_list()})\n",
    "x_test_df = pd.DataFrame({'post_id' : X_test_id.to_list()})\n",
    "\n",
    "# X_train_df = pd.merge(x_train_df, raw_data_final, how='inner', on='post_id')\n",
    "# X_dev_df = pd.merge(x_dev_df, raw_data_final, how='inner', on='post_id')\n",
    "# X_test_df = pd.merge(x_test_df, raw_data_final, how='inner', on='post_id')\n",
    "\n",
    "X_train_text = train_data_df['text_combined'].to_list()\n",
    "X_dev_text= dev_data_df['text_combined'].to_list()\n",
    "X_test_text = test_data_df['text_combined'].to_list()\n",
    "\n",
    "print(len(X_train_text))\n",
    "print(len(X_dev_text))\n",
    "print(len(X_test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211519"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load augmented datasets generated by EDA\n",
    "# sr = synonym replacement\n",
    "# ri = random synonym insertion\n",
    "# rs = random swap\n",
    "# rd = random deletion\n",
    "# dataframe name format: method_number \n",
    "\n",
    "sr_1_df = pd.read_csv('../test_data_set/EDA_10_0_7_sr_rest_0_1.csv')\n",
    "ri_1_df = pd.read_csv('../test_data_set/EDA_10_0_7_ri_rest_0_1.csv')\n",
    "rs_1_df = pd.read_csv('../test_data_set/EDA_10_0_7_rs_rest_0_1.csv')\n",
    "rd_1_df = pd.read_csv('../test_data_set/EDA_10_0_7_rd_rest_0_1.csv')\n",
    "all_1_df = pd.read_csv('../test_data_set/EDA_10_all_0_1s.csv')\n",
    "all_5_df = pd.read_csv('../test_data_set/EDA_10_all_0_5s.csv')\n",
    "\n",
    "# remove undecided labeled examples\n",
    "sr_1_df_filtered = sr_1_df[sr_1_df['final_label'] != 'undecided']\n",
    "ri_1_df_filtered = ri_1_df[ri_1_df['final_label'] != 'undecided']\n",
    "rs_1_df_filtered = rs_1_df[rs_1_df['final_label'] != 'undecided']\n",
    "rd_1_df_filtered = rd_1_df[rd_1_df['final_label'] != 'undecided']\n",
    "all_1_df_filtered = all_1_df[all_1_df['final_label'] != 'undecided']\n",
    "all_5_df_filtered = all_5_df[all_5_df['final_label'] != 'undecided']\n",
    "\n",
    "len(sr_1_df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169213"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate train, dev, test for each set\n",
    "sr_1_df_train = sr_1_df_filtered[sr_1_df_filtered['post_id'].isin(X_train_id)]\n",
    "ri_1_df_train = ri_1_df_filtered[ri_1_df_filtered['post_id'].isin(X_train_id)]\n",
    "rs_1_df_train = rs_1_df_filtered[rs_1_df_filtered['post_id'].isin(X_train_id)]\n",
    "rd_1_df_train = rd_1_df_filtered[rd_1_df_filtered['post_id'].isin(X_train_id)]\n",
    "all_1_df_train = all_1_df_filtered[all_1_df_filtered['post_id'].isin(X_train_id)]\n",
    "all_5_df_train = all_5_df_filtered[all_5_df_filtered['post_id'].isin(X_train_id)]\n",
    "\n",
    "# select text sets\n",
    "\n",
    "aug_sr_text = sr_1_df_train['text_str'].to_list()\n",
    "aug_ri_text = ri_1_df_train['text_str'].to_list()\n",
    "aug_rs_text = rs_1_df_train['text_str'].to_list()\n",
    "aug_rd_text = rd_1_df_train['text_str'].to_list()\n",
    "aug_all_1_text = all_1_df_train['text_str'].to_list()\n",
    "aug_all_5_text = all_5_df_train['text_str'].to_list()\n",
    "\n",
    "# select label sets\n",
    "\n",
    "aug_sr_labels = sr_1_df_train['final_label']\n",
    "aug_ri_labels = ri_1_df_train['final_label']\n",
    "aug_rs_labels = rs_1_df_train['final_label']\n",
    "aug_rd_labels = rd_1_df_train['final_label']\n",
    "aug_all_1_labels = all_1_df_train['final_label']\n",
    "aug_all_5_labels = all_5_df_train['final_label']\n",
    "\n",
    "len(aug_sr_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert labels to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class label to 1 hot encoding\n",
    "\n",
    "def convert_to_oh(S):\n",
    "    '''takes a pandas series of text labels and returns one hot encoding equivalent\n",
    "    0 = normal, 1 = offensive, 2 = hatespeech\n",
    "    ''' \n",
    "    S_numerical = S.apply(lambda x: 0 if x=='normal' else (1 if x=='offensive' else 2))\n",
    "    S_oh = keras.utils.to_categorical(S_numerical, num_classes = 3, dtype = 'float32')\n",
    "    return S_oh\n",
    "    \n",
    "# original dataset - train, dev, and train\n",
    "y_train_orig = convert_to_oh(pd.Series(y_train))\n",
    "y_dev_orig = convert_to_oh(pd.Series(y_dev))\n",
    "y_test_orig = convert_to_oh(pd.Series(y_test))\n",
    "\n",
    "# augmented with sr = 0.1\n",
    "y_train_aug_sr = convert_to_oh(aug_sr_labels)\n",
    "\n",
    "# augmented with ri = 0.1\n",
    "y_train_aug_ri = convert_to_oh(aug_ri_labels)\n",
    "\n",
    "# augmented with rs = 0.1\n",
    "y_train_aug_rs = convert_to_oh(aug_rs_labels)\n",
    "\n",
    "# augmented with rd = 0.1\n",
    "y_train_aug_rd = convert_to_oh(aug_rd_labels)\n",
    "\n",
    "# augmented with all = 0.1\n",
    "y_train_all_1 = convert_to_oh(aug_all_1_labels)\n",
    "\n",
    "# augmented with all = 0.5\n",
    "y_train_all_5 = convert_to_oh(aug_all_5_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169213"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aug_ri_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169213"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train_aug_ri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_aug_ri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#bert_model = TFBertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i dont think im getting my baby them white he has two white j and nikes not even touched',\n",
       " 'dont think getting baby them white he has two white nikes not even touched',\n",
       " 'i dont think im getting my baby j white he has two white them and nikes not even touched',\n",
       " 'i dont consider im commence my mollycoddle them ashen he has ii ashen joule and nike not tied bear upon',\n",
       " 'i dont think im getting my baby them white he has two white j and nikes even not touched',\n",
       " 'i dont think im getting my baby them white he has two white j and nikes affect not even touched',\n",
       " 'i dont imagine im incur my spoil them ovalbumin he has deuce ovalbumin joule and nike not evening impact',\n",
       " 'i dont think getting my baby them white he has two white j nikes not even touched',\n",
       " 'i dont think im getting my baby them white he has two white j and beget nikes not even touched',\n",
       " 'i dont think im getting my baby them white he has nikes white j and two not even touched',\n",
       " 'i dont think im getting my baby them white he has two white j and nikes not even touched',\n",
       " 'we cannot continue calling ourselves information if the to of feminists womxn arent addressed yes rights a sexual offences public list but will a trans lesbian bisexual and queer womxn be enter to able their all on the reporting sheet gender forum',\n",
       " 'addressed cannot arent calling ourselves yes if the rights of all womxn continue we feminists to a sexual offences public list but will a trans lesbian bisexual and queer womxn be able on enter their information to the reporting sheet gender forum',\n",
       " 'we cannot uphold forebode ourselves feminist if the right field of all womxn arent cover yes to a intimate offensive world inclination but will a trans gay woman epicene and funny womxn be capable to record their entropy on the account canvass grammatical gender meeting place',\n",
       " 'we cannot continue calling ourselves feminists if the rights of all womxn arent addressed yes to a sexual offences public list but will a trans lesbian bisexual peril prognosticate and queer womxn be able to enter their information on the reporting peril bisexual person sheet gender forum',\n",
       " 'we cannot cover career ourselves feminist if the rightfulness of all womxn arent deal yes to a intimate discourtesy world name but will a trans tribade epicene and rummy womxn be able bodied to record their data on the report tack sex assembly',\n",
       " 'we continue calling ourselves feminists if the rights of all womxn arent addressed yes to a sexual offences public but a trans lesbian bisexual and queer womxn be able to enter their information on the sheet gender forum',\n",
       " 'able bodied we cannot continue calling libber ourselves feminists if the rights of all womxn arent addressed yes to a sexual offences public list prognosticate but will meeting place a trans lesbian bisexual and queer womxn be able to enter their information on the reporting sheet gender forum',\n",
       " 'we trans able calling ourselves feminists if the rights of all womxn arent yes addressed to a sexual offences queer list but will a cannot lesbian bisexual and public womxn be continue to enter their information on the reporting sheet gender forum',\n",
       " 'we continue calling ourselves feminists if the rights of womxn arent yes to a sexual offences public list but will a trans lesbian bisexual and queer womxn be able to enter their information on the reporting sheet gender forum',\n",
       " 'we cannot preserve telephone ourselves libber if the redress of all womxn arent speak yes to a intimate crime world heel but will a trans sapphic epicene and cross womxn be capable to embark their entropy on the coverage weather sheet sex meeting place',\n",
       " 'we cannot continue calling ourselves feminists if the rights of all womxn arent addressed yes to a sexual offences public list but will a trans lesbian bisexual and queer womxn be able to enter their information on the reporting sheet gender forum',\n",
       " 'nawt yall niggers ignoring me',\n",
       " 'nawt yall spade push aside me',\n",
       " 'niggers yall nawt ignoring me',\n",
       " 'nawt yall coon cut me',\n",
       " 'nawt yall nigga snub me',\n",
       " 'nawt yall cut niggers ignoring me',\n",
       " 'nawt yall niggers ignoring',\n",
       " 'yall nawt niggers ignoring me',\n",
       " 'spade nawt yall niggers ignoring me',\n",
       " 'nawt me niggers ignoring yall',\n",
       " 'nawt yall niggers ignoring me',\n",
       " 'user i am bit confused coz chinese ppl can not access twitter thn habituate how this snatch ching chong using it i think he pakistani',\n",
       " 'user i am bit confused coz chinese ppl snatch can not access twitter thn how this ching chong using it i think he pakistani',\n",
       " 'user i bit am confused coz chinese i can not access twitter thn how this ching chong using it ppl think he pakistani',\n",
       " 'user i am bit coz chinese can not twitter thn how this ching chong using it i think pakistani',\n",
       " 'user i am bit confused coz chinese ppl can not access twitter thn how this exploiter ching chong using it americium i think he pakistani',\n",
       " 'exploiter i am chip throw coz formosan ppl can not approach chirrup thn how this ching chong expend it i suppose he pakistani',\n",
       " 'user i am i confused coz pakistani ppl can not access twitter thn how this ching chong using it bit think he chinese',\n",
       " 'drug user i am bite mix up coz formosan ppl can not memory access chitter thn how this ching chong practice it i mean he pakistani',\n",
       " 'drug user i am morsel confound coz formosan ppl can not accession chirrup thn how this ching chong use it i intend he pakistani',\n",
       " 'this i am bit confused coz chinese ppl can not access twitter thn how user ching chong using it i think he pakistani',\n",
       " 'user i am bit confused coz chinese ppl can not access twitter thn how this ching chong using it i think he pakistani ',\n",
       " 'this in eating a burger with no top bun and holding palm up i hate white bitches',\n",
       " 'this bitch in whataburger eating a burger with no top bun and holding it palm up i hate white bitches',\n",
       " 'this bitch and whataburger eating a burger with no top bun in holding it palm i up hate white bitches',\n",
       " 'this crab in whataburger exhaust a warren burger with no go past roll and prevail it palm tree up i hatred tweed grouse',\n",
       " 'this bitch in whataburger eating a burger with no top bun and holding it improving palm up i hate white bitches',\n",
       " 'this up in whataburger eating a burger with no top bitches and holding it palm bitch i hate white bun',\n",
       " 'this white it whataburger eating a burger with no top bun and holding in palm up i hate bitch bitches',\n",
       " 'this bellyache in whataburger rust a hamburger with no big top roll and retain it palm tree up i detest blanched bellyache',\n",
       " 'tip this bitch in whataburger eating a burger with no top bun and holding it palm tip up i hate white bitches',\n",
       " 'this bitch in whataburger eating a burger with no top bun and beefburger holding it information technology palm up i hate white bitches',\n",
       " 'this bitch in whataburger eating a burger with no top bun and holding it palm up i hate white bitches',\n",
       " 'laura loomer raped me while screaming at me in her disgusting kike and said we must exterminate the goyim laura loomer loomergate',\n",
       " 'laura loomer dishonor me while shout at me in her loathsome yid terminology and enjoin we mustiness kill off the goy laura loomer loomergate',\n",
       " 'laura loomer raped me while screaming at me in her disgusting kike language and said we must exterminate distasteful the hoosier state goyim laura loomer loomergate',\n",
       " 'laura loomer raped me while screaming at me in her disgusting kike language and said we must exterminate the goyim laura loomer loomergate',\n",
       " 'laura loomer raped me while screaming at me in her disgusting kike language speech and said moldiness we must exterminate the goyim laura loomer loomergate',\n",
       " 'laura loomer outrage me while shriek at me in her disgust yid speech communication and say we moldiness root out the non jew laura loomer loomergate',\n",
       " 'laura loomer raped me while screaming at me in her disgusting kike language and said we must exterminate the goyim loomergate loomer laura',\n",
       " 'laura loomer raped goyim while screaming at me in her disgusting kike language and said we must exterminate the laura me loomer loomergate',\n",
       " 'laura loomer raped me while screaming language me in her disgusting kike at and said we laura exterminate the goyim must loomer loomergate',\n",
       " 'laura loomer raped me while pink screaming at me in her disgusting kike language maine and said we must exterminate the goyim laura loomer loomergate',\n",
       " 'laura loomer raped me while screaming at me in her disgusting kike language and said we must exterminate the goyim laura loomer loomergate',\n",
       " 'and this is why i ending up with coon trainee md who can not mouth the right way deficiency canonical noesis of biota it genuinely shuddery if the world only get it on',\n",
       " 'and this is why i public up basic nigger trainee doctors who can not speak properly lack with knowledge of biology it truly scary if the end only knew',\n",
       " 'and this is why i remnant up with nigra trainee fix who can not mouth the right way deficiency introductory noesis of biological science it sincerely scarey if the populace only bonk',\n",
       " 'and biology is why basic end up with nigger trainee doctors who can not speak properly lack i knowledge of this it truly scary if the public only knew',\n",
       " 'and this is why i closing up with jigaboo trainee restore who can not utter decent miss introductory noesis of biota it unfeignedly chilling if the world only recognize',\n",
       " 'and this is the i end up with nigger trainee doctors who speak not can properly lack basic knowledge of biology it truly scary if why public only knew',\n",
       " 'and is i end up with trainee doctors who not speak properly lack basic knowledge of biology it truly scary if the public only',\n",
       " 'and non this is why i end up with nigger trainee doctors who can not improving speak properly lack basic knowledge of biology it truly scary if the public only knew',\n",
       " 'and this is why i end up spade with nigger trainee doctors who can not speak properly lack basic utter knowledge of biology it truly scary if the public only knew',\n",
       " 'and this is why i end up with nigger trainee doctors who can not speak properly lack basic knowledge biology it truly scary if public knew',\n",
       " 'and this is why i end up with nigger trainee doctors who can not speak properly lack basic knowledge of biology it truly scary if the public only knew',\n",
       " 'guilty until proven innocent unless you are a jew a nigger or shamed a lover of both',\n",
       " 'shamed until establish guiltless unless you are a hebrew a coon or a fan of both',\n",
       " 'guilty until proven innocent unless you are jew a a nigger or a lover of both',\n",
       " 'guilty until innocent unless you are a jew a nigger or a lover of both',\n",
       " 'guilty until proven innocent unless lover are a jew a nigger or a you of both',\n",
       " 'guilty until proven innocent unless you are a jew both nigger or a lover of a',\n",
       " 'guilty until proven innocent unless you are a jew a nigger or a lover of angstrom unit both',\n",
       " 'guilty until proven you are a jew a nigger or a lover of',\n",
       " 'guilty until proven innocent unless buff you are a jew a nigger or a lover of both',\n",
       " 'hangdog until raise devoid unless you are a israelite a nigga or a fan of both',\n",
       " 'guilty until proven innocent unless you are a jew a nigger or a lover of both',\n",
       " 'fag out i can not financial backing miscarriage from a lesson stand electrify i can financial backing it virtuously slowly quit appear at those slight niglets as time to come restore and come across them for what they in truth are time to come rapist of clean adult female and time to come benefit receiver of clean revenue enhancement dollar bill and time to come populist elector',\n",
       " 'tired i i not support abortion from stop moral standpoint wired can can support it morally women a looking at those little niglets as future doctors and see them for what they really are future easy of white rapists and recipient welfare future of white tax dollars and future democrat voters',\n",
       " 'tired i can not support abortion from a moral standpoint wired i can support it morally easy stop looking at those little niglets as future and see them for what they really are future rapists of white women and welfare recipient of white tax dollars and future democrat voters',\n",
       " 'wear out i can not bear out miscarriage from a lesson viewpoint pumped up i can bear out it virtuously slow blockage await at those footling niglets as succeeding mend and find out them for what they very are succeeding rapist of theodore harold white womanhood and succeeding eudaimonia recipient role of theodore harold white revenue enhancement dollar bill and succeeding populist voter',\n",
       " 'tired i can not support abortion from a moral standpoint wired i can support it morally easy stop looking at those little niglets as future doctors and see them for what viewpoint they really are future rapists of white eastern samoa women and future welfare recipient of white tax dollars and future dr democrat voters',\n",
       " 'tired can not support abortion from moral wired i can support it morally stop looking at niglets as future doctors see them for what they really are rapists of white women and future welfare recipient of white tax dollars future democrat',\n",
       " 'i can support abortion from a moral standpoint wired i can support it morally easy stop looking at those little niglets as future and them for what they really are future rapists of white women and future welfare recipient of white dollars and future democrat voters',\n",
       " 'commonplace i can not backup miscarriage from a lesson stand wire i can backup it virtuously promiscuous period await at those piddling niglets as next bushel and escort them for what they rattling are next raper of albumen cleaning woman and next public assistance receiver of albumen task dollar bill and next populist elector',\n",
       " 'tired i can not support abortion from a moral standpoint wired recipient can support it morally easy stop and at those little niglets white future doctors and see them for what they really white future rapists of as women looking future welfare i of are tax dollars and future voters democrat',\n",
       " 'tired i can not welfare abortion from a moral standpoint wired i can support it morally easy stop looking at those little niglets as future doctors see and them for what women really and future rapists of white future and they support recipient of white tax dollars are future democrat voters',\n",
       " 'tired i can not support abortion from a moral standpoint wired i can support it morally easy stop looking at those little niglets as future doctors and see them for what they really are future rapists of white women and future welfare recipient of white tax dollars and future democrat voters',\n",
       " 'number number percent of brits think multiculturalism is a failure angstrom unit i guess the other percent are happy with their muzzie master original there no great in britain anymore soon big ben will be converted percentage into a minaret',\n",
       " 'number number happy of brits think multiculturalism is a failure i guess the other percent with percent are their muzzie master there no great in britain anymore soon big into will be converted ben a minaret',\n",
       " 'number number will of brits think multiculturalism is a failure i guess the other be are happy with their muzzie britain there no great in master anymore soon big ben percent percent converted into a minaret',\n",
       " 'number number percent of brits think multiculturalism is a failure i guess the other percent are happy with their muzzie master there no great in anymore soon big ben will be converted into a minaret',\n",
       " 'numeral numeral pct of britt cogitate multiculturalism is a nonstarter i opine the other pct are felicitous with their muzzie maestro there no peachy in uk any longer presently bounteous ben will be commute into a minaret',\n",
       " 'phone number phone number percentage of briton cerebrate multiculturalism is a unsuccessful person i speculation the other percentage are glad with their muzzie headmaster there no large in uk any longer shortly enceinte ben will be reborn into a minaret',\n",
       " 'numeral numeral percentage of britisher intend multiculturalism is a loser i shot the other percentage are well chosen with their muzzie passkey there no groovy in united kingdom any longer before long self aggrandising ben will be born again into a minaret',\n",
       " 'number original number percent of brits think multiculturalism is figure a failure i guess the other percent are happy with their muzzie master there no great in britain britisher anymore soon big ben will be converted into a minaret',\n",
       " 'minaret i percent of brits think multiculturalism is a failure number guess the other percent are happy with their muzzie master there no great big britain anymore soon in ben will be converted into a number',\n",
       " 'number number percent of brits think multiculturalism is a failure i the other percent are their master there no great in britain soon big ben be converted into a minaret',\n",
       " 'number number percent of brits think multiculturalism is a failure i guess the other percent are happy with their muzzie master there no great in britain anymore soon big ben will be converted into a minaret',\n",
       " 'tariq nasheed shell the know out of that grind away honey lmao individual place a cooky in that nigra talk already',\n",
       " 'tariq nasheed beat out the shtup out of that drumfish honey lmao person cast a cookie in that coon verbalize already',\n",
       " 'lmao nasheed beating the fuck out of that drum biscuit tariq somebody put a beloved in that niggas mouth already',\n",
       " 'tariq nasheed beating the fuck of that drum beloved lmao somebody put a biscuit in that niggas mouth already',\n",
       " 'tariq nasheed beating the fuck out of lmao drum beloved that somebody put a biscuit that in niggas mouth already',\n",
       " 'tariq nasheed beating couch the fuck out of that drum beloved lmao somebody put a biscuit in that niggas mouth bone already',\n",
       " 'tariq nasheed whacking the piece of tail out of that swot darling lmao person invest a cooky in that coon sass already',\n",
       " 'beating the fuck out of beloved lmao somebody put a biscuit in that niggas mouth already',\n",
       " 'tariq nasheed beating the fuck out of that drum beloved lmao somebody angstrom unit put a pound biscuit in that niggas mouth already',\n",
       " 'tariq nasheed beating the fuck out of that drum beloved somebody a biscuit in that niggas mouth already',\n",
       " 'tariq nasheed beating the fuck out of that drum beloved lmao somebody put a biscuit in that niggas mouth already',\n",
       " 'percent cotton fiber second joint highschool to annul striking with any homophile chemical',\n",
       " 'percent cotton thigh highs to avoid link contact with any gay chemicals',\n",
       " 'percent cotton thigh highs to with contact avoid any gay chemicals',\n",
       " 'cotton thigh highs to avoid contact with any gay chemicals',\n",
       " 'percent cotton thigh highs to avoid contact with any gay',\n",
       " 'per centum cotton second joint senior high school to fend off tangency with any homophile chemical substance',\n",
       " 'percent cotton thigh highs to avoid contact with any second joint gay chemicals',\n",
       " 'percent highs thigh cotton to avoid contact with any gay chemicals',\n",
       " 'percentage cotton wool second joint highschool to avoid liaison with any festal chemical',\n",
       " 'fend off percent cotton thigh highs to avoid contact with any gay chemicals',\n",
       " 'percent cotton thigh highs to avoid contact with any gay chemicals',\n",
       " 'oomf a number yearl old closet with and obsessed gay me someone help',\n",
       " 'oomf a total yearl former wardrobe jocund and haunted with me individual supporter',\n",
       " 'oomf a number yearl old closet gay and obsessed festal with me someone help',\n",
       " 'oomf a total yearl previous wc merry and haunt with me somebody service',\n",
       " 'oomf a number yearl old closet gay and obsessed with someone me help',\n",
       " 'oomf a number yearl old closet gay and obsessed with me someone help',\n",
       " 'oomf a turn yearl previous wc festive and haunted with me individual facilitate',\n",
       " 'oomf a number yearl old closet gay someone obsessed with me and help',\n",
       " 'oomf a number yearl old closet gay and obsessed with me someone help',\n",
       " 'oomf a number yearl old closet possessed gay and obsessed with me someone help',\n",
       " 'oomf a number yearl old closet gay and obsessed with me someone help',\n",
       " 'all of my exes were cute but they were hoes i guess i cunning only attract fly looking thots',\n",
       " 'all of my exes were cute but they were hoes i guess i only attract fly looking',\n",
       " 'all of my x were precious but they were hoe i conjecture i only pull in aviate front thots',\n",
       " 'all of depend my exes were cute but they were hoes i guess i only attract fly looking thots',\n",
       " 'all of my exes were cute but they were hoes guess i i only attract fly looking thots',\n",
       " 'all of but exes were cute my they were hoes i guess i only attract fly looking thots',\n",
       " 'all of my x were cunning but they were hoe i opine i only pull take flight wait thots',\n",
       " 'all of my exes were cute but they were hoes i guess i only attract fly looking thots',\n",
       " 'all thots my exes were cute but they were hoes i guess i only attract fly looking of',\n",
       " 'all of my exes were cute but were hoes i guess i only fly looking thots',\n",
       " 'all of my exes were cute but they were hoes i guess i only attract fly looking thots ',\n",
       " 'fellas be other she texting careful niggas',\n",
       " 'fellas be she texting',\n",
       " 'fellow be measured she texting other coon',\n",
       " 'fellas be careful other texting she niggas',\n",
       " 'feller be thrifty she texting other nigger',\n",
       " 'blighter be thrifty she texting other nigga',\n",
       " 'fellow fellas be careful she texting other niggas',\n",
       " 'fellas be careful other niggas',\n",
       " 'fellas be careful she texting thrifty other niggas',\n",
       " 'former fellas be careful she texting other niggas',\n",
       " 'fellas be careful she texting other niggas',\n",
       " 'italian nigger promise they tool the poozingi',\n",
       " 'italian poozingi call they dick the niggas',\n",
       " 'italian niggas call song they dick the poozingi',\n",
       " 'italian call they dick the poozingi',\n",
       " 'italian niggas they dick the poozingi',\n",
       " 'italian niggas call gumshoe they dick the poozingi',\n",
       " 'italian coon telephone call they putz the poozingi',\n",
       " 'italian call dick the poozingi',\n",
       " 'italian niggas call they the dick poozingi',\n",
       " 'italian niggas call they dick the song poozingi',\n",
       " 'italian niggas call they dick the poozingi',\n",
       " 'user what do the old lady did was she a nazi or white supremacist',\n",
       " 'user what the old lady was a nazi or white supremacist',\n",
       " 'user what did the old lady do was she a nazi or supremacist',\n",
       " 'user what did the old lady do was she a nazi exploiter or white supremacist',\n",
       " 'substance abuser what did the older gentlewoman do was she a german nazi or ashen supremacist',\n",
       " 'angstrom unit user what did the old lady do was she a nazi or white supremacist',\n",
       " 'user what angstrom unit did the old lady do was she a nazi or white supremacist',\n",
       " 'supremacist what did the old lady do was she a nazi or white user',\n",
       " 'user what did the old lady do was she a nazi or white supremacist',\n",
       " 'drug user what did the honest to god madam do was she a german nazi or flannel supremacist',\n",
       " 'user what did the old lady do was she a nazi or white supremacist',\n",
       " 'user user user over user he married a negress because white women did not want all but black females him user america love nobama',\n",
       " 'user user user user user he espouse married a fair sex negress because white women did not want him but black females all over america love nobama',\n",
       " 'user user user user user he married a negress white women did not him but black females all over america love',\n",
       " 'user user user user user he married a not because black women did negress want him but white females all over america love nobama',\n",
       " 'user user a user user he married user negress because white women did not want him but black females all over america love nobama',\n",
       " 'user user user user user he married a negress because women did not want him but females all america love nobama',\n",
       " 'exploiter exploiter exploiter exploiter exploiter he tie a negress because white hot womanhood did not neediness him but shameful female person all over the states hump nobama',\n",
       " 'substance abuser substance abuser substance abuser substance abuser substance abuser he tie a negress because elwyn brooks white charwoman did not need him but blackened female person all over usa lovemaking nobama',\n",
       " 'user user user user user exploiter he married complete a negress because white women did not want him but black females all over america love nobama',\n",
       " 'user user user user user he married a negress because white women not want him but black all over america love nobama',\n",
       " 'user user user user user he married a negress because white women did not want him but black females all over america love nobama',\n",
       " 'user condoning drug not kike at all thanks for that disclosure',\n",
       " 'user condoning drug use not kike at all thanks for that disclosure',\n",
       " 'user condoning drug use not kike at thanks for that disclosure',\n",
       " 'user condoning drug use not all at kike thanks for that disclosure',\n",
       " 'user condoning drug use not kike at all thanks completely for that disclosure',\n",
       " 'user dose condoning drug use not kike at all thanks for that disclosure',\n",
       " 'substance abuser excuse dose practice not sheeny at all thank for that revealing',\n",
       " 'user give thanks condoning drug use not kike at all thanks for that disclosure',\n",
       " 'exploiter condone dose expend not yid at all give thanks for that revealing',\n",
       " 'user condoning drug use not kike at all thanks that for disclosure',\n",
       " 'user condoning drug use not kike at all thanks for that disclosure',\n",
       " 'simply user user people act as if you can not say non the same information technology about the states obviously not all americans are pro guns not all americans support their government treatment of immigrants and foreigners but when it comes to israel information technology suddenly the logic changes',\n",
       " 'guns you people act as if user can not say the the about the states obviously not all americans are it user not all americans support their government treatment of immigrants and foreigners but when pro comes to israel suddenly same logic changes',\n",
       " 'user user support act as the you can not say if people about changes states obviously not all americans are pro guns not all americans same their government treatment of immigrants and foreigners but when it comes to israel suddenly the logic the',\n",
       " 'drug user drug user the great unwashed represent as if you can not suppose the same about the country apparently not all american are professional hit man not all american plunk for their government activity discourse of immigrant and alien but when it derive to yisrael dead the logical system convert',\n",
       " 'user user people act as if you can not say the same about the states obviously not all americans pro guns not all americans support their government treatment of immigrants and foreigners but when it comes to israel suddenly the logic changes',\n",
       " 'exploiter exploiter the great unwashed dissemble as if you can not enunciate the same about the nation patently not all american language are professional accelerator not all american language confirm their authorities handling of immigrant and outsider but when it come up to yisrael all of a sudden the logical system convert',\n",
       " 'user user people act as if you can not the same about the states obviously not all americans are pro guns all americans support their government treatment of immigrants and foreigners but when comes to suddenly the changes',\n",
       " 'drug user drug user mass act as as if you can not aver the same about the dos plain not all american are professional shooter not all american bear out their regime discourse of immigrant and noncitizen but when it derive to yisrael all of a sudden the system of logic alter',\n",
       " 'user user not act as if you can people say not same about the states obviously support all americans are pro guns the it americans not their government treatment of immigrants and foreigners but when all comes to israel suddenly the logic changes',\n",
       " 'user user switch people act as completely if you can not say the same about the states obviously not all americans are pro guns not professional all americans support their government enactment treatment of immigrants and foreigners but when it comes to israel suddenly the logic changes',\n",
       " 'user user people act as if you can not say the same about the states obviously not all americans are pro guns not all americans support their government treatment of immigrants and foreigners but when it comes to israel suddenly the logic changes',\n",
       " 'user user strained they visited exploiter helped provided food to those kps while islam c terrorist butured raped and forced them to leave their own land',\n",
       " 'substance abuser substance abuser they shoot the breeze avail furnish solid food to those kp while muhammadanism degree centigrade terrorist butured assault and impel them to impart their own down',\n",
       " 'intellectual nourishment user user they visited helped provided food to those go forth kps while islam c terrorist butured raped and forced them to leave their own land',\n",
       " 'user user they visited helped provided food to land kps while islam c terrorist butured raped and forced them to own their leave those',\n",
       " 'user user they visited helped provided food to those kps while islam leave terrorist butured raped and forced them own c their to land',\n",
       " 'substance abuser substance abuser they chew the fat assist pull up stakes nutrient to those kitchen police while muslimism one c terrorist butured plunder and ram them to pull up stakes their own bring',\n",
       " 'exploiter exploiter they chaffer aid ply intellectual nourishment to those kp while muslimism nose candy terrorist butured dishonor and thrust them to result their own down',\n",
       " 'user user they visited helped food to those kps while islam c terrorist butured and forced them leave their own land',\n",
       " 'user user they visited helped provided food to those kps while islam c terrorist butured raped and forced them to leave their own land',\n",
       " 'user user they visited helped provided food to those kps while islam c terrorist butured leave to forced them and raped their own land',\n",
       " 'user user they visited helped provided food to those kps while islam c terrorist butured raped and forced them to leave their own land',\n",
       " 'anyways the fact that her outfit directly violates france recognition law abt no face coverings veils but is still headlined in paris fashion week shows that the law was never abt obscured facial identification but abt white supremacy simply and capital of france islamophobia',\n",
       " 'anyways the fact that her outfit directly violates france law abt no face coverings veils but is still headlined in live paris fashion week shows that the law was never abt simply obscured facial identification but abt but white supremacy and islamophobia',\n",
       " 'anyways the fact that her outfit directly violates france facial abt no face coverings veils but week still headlined in paris fashion is shows that the but was never abt obscured law identification law abt white supremacy and islamophobia',\n",
       " 'anyways the fact that her directly abt no face coverings veils but is still headlined in paris fashion week shows that the law never abt obscured facial identification but abt white supremacy and islamophobia',\n",
       " 'in any event the fact that her turnout direct profane french republic police abt no human face encompass veil but is hush headline in city of light style hebdomad record that the police was neer abt blot out facial nerve designation but abt tweed mastery and islamophobia',\n",
       " 'anyways the fact that her outfit directly face france law abt no violates that veils but is still headlined in and fashion week shows coverings the law was never abt obscured facial identification but abt white supremacy paris islamophobia',\n",
       " 'but the fact that her outfit directly violates france law abt no that coverings veils week is still headlined in paris fashion but shows face the law was never abt obscured facial identification anyways abt white supremacy and islamophobia',\n",
       " 'at any rate the fact that her kit direct assault french republic legal philosophy abt no nerve hatch velum but is quieten headline in capital of france mode calendar week show up that the legal philosophy was neer abt mist facial nerve recognition but abt e b white mastery and islamophobia',\n",
       " 'anyway the fact that her equip at once violate french republic practice of law abt no front underwrite veil but is yet headline in capital of france manner hebdomad express that the practice of law was neer abt haze over nervus facialis designation but abt bloodless mastery and islamophobia',\n",
       " 'anyways the fact that hoosier state her outfit directly violates france law abt no face coverings veils but anyhow smooth is still headlined in paris fashion week shows that the law was never abt obscured facial identification but abt white supremacy and islamophobia',\n",
       " 'anyways the fact that her outfit directly violates france law abt no face coverings veils but is still headlined in paris fashion week shows that the law was never abt obscured facial identification but abt white supremacy and islamophobia',\n",
       " 'it suspect how the great unwashed scandalize by the holy writ nigra have no trouble foretell the great unwashed nazi what the deviation',\n",
       " 'it funny how people offended by the word nigger have no problem calling people nazis what the past difference',\n",
       " 'it funny how people offended by difference word nigger have no problem calling people nazis what the the',\n",
       " 'funny how people offended by the word nigger have no calling people nazis what the',\n",
       " 'it peculiar how the great unwashed breach by the phrase nigra have no trouble call up the great unwashed german nazi what the departure',\n",
       " 'it suspect how masses scandalize by the phrase jigaboo have no trouble send for masses nazi what the remainder',\n",
       " 'it funny how people offended by the word calling have no problem nigger people nazis what the difference',\n",
       " 'it how offended by the word nigger have no problem calling people nazis what the difference',\n",
       " 'it funny how people offended by the word nigger have no problem calling people nazis what the difference',\n",
       " 'it funny how people offended by the word nigger have no hold problem calling people nazis what the difference',\n",
       " 'it funny how people offended by the word nigger have no problem calling people nazis what the difference',\n",
       " 'as a great deal as i apprize a data formatting of unfastened word i genuinely just deficiency a facebook that i can order beanball on',\n",
       " 'data format as much as i appreciate a format of open discussion i really just want eastern samoa a facebook that i can say beaner on',\n",
       " 'as appreciate as i much a format of open a i really just want discussion facebook that i can say beaner on',\n",
       " 'as a great deal as i prize a arrange of overt treatment i actually just wish a facebook that i can allege beanball on',\n",
       " 'as much as i angstrom unit appreciate a format of open discussion i really just want a facebook that i angstrom unit can say beaner on',\n",
       " 'as much as i a format of open discussion really just want a that i say beaner on',\n",
       " 'as open as i appreciate a format of much discussion beaner really just want a facebook that i can say i on',\n",
       " 'as practically as i apprise a initialize of surface discourse i in truth just wishing a facebook that i can enjoin beanball on',\n",
       " 'as much as i appreciate a format of open discussion i really just want a facebook that i can say beaner on',\n",
       " 'as much as i appreciate a format of open discussion i really just want a that i can say beaner on',\n",
       " 'as much as i appreciate a format of open discussion i really just want a facebook that i can say beaner on',\n",
       " 'that was just a photo look thanksgiving rhyan moody she was killed on up by an illegal spic',\n",
       " 'that was just a picture await up rhyan temperamental she was pop on thanksgiving day by an illegal spick',\n",
       " 'that was just photo a look up rhyan moody she was killed on thanksgiving by an illegal spic',\n",
       " 'that just a look up rhyan she killed on thanksgiving by an illegal spic',\n",
       " 'that was just a photo look up rhyan moody she was on killed thanksgiving by an illegal spic',\n",
       " 'that was just a photo look up rhyan moody she was angstrom unit killed on thanksgiving by an illegal spic',\n",
       " 'that was just a photo look up rhyan moody obliterate she was killed on thanksgiving by an illegal spic',\n",
       " 'that was just a photo angstrom unit look up rhyan moody she was killed on thanksgiving by an illegal spic',\n",
       " 'that a photo look rhyan moody was killed on thanksgiving by an illegal spic',\n",
       " 'that was just a picture facial expression up rhyan glum she was shoot down on grace by an illegal spik',\n",
       " 'that was just a photo look up rhyan moody she was killed on thanksgiving by an illegal spic',\n",
       " 'this sound alike a kike interior socialistic',\n",
       " 'this sounds like a strait kike national socialist',\n",
       " 'national sounds like a kike this socialist',\n",
       " 'a sounds like this kike national socialist',\n",
       " 'this sounds like kike national socialist',\n",
       " 'this sounds like a kike national socialist',\n",
       " 'socialistic this sounds like a kike national socialist',\n",
       " 'this sounds like a kike national socialist',\n",
       " 'this go like a sheeny internal socialistic',\n",
       " 'this like sounds a kike national socialist',\n",
       " 'this sounds like a kike national socialist',\n",
       " 'i realize a duo frank baum and beer mug in the dupe number but that about it ya beget more information i suppose the school day was one half speckless though',\n",
       " 'i a couple baums and steins in the victim list but that about it got more i thought the school was half spic though',\n",
       " 'i saw a couple baums and steins in the victim list but that about it ya got more info i thought the school gertrude stein was half spic though',\n",
       " 'i saw a couple baums about steins in the victim list it that and but ya got more info i thought the school was half spic though',\n",
       " 'i saw a couple baums and steins in the victim list but that frank baum about it ya got more info i thought the live school was half spic though',\n",
       " 'i saw a couple baums and steins in the victim list but that about it half school more info i thought the got was ya spic though',\n",
       " 'i saw a couple baums and steins in the victim list but that about it ya more info i thought the school was half though',\n",
       " 'i saw a couple baums and steins in the victim but that about it ya more info i school half spic though',\n",
       " 'i fancy a pair off lyman frank brown and beer mug in the dupe inclination but that about it ya flummox more information i persuasion the educate was one half speckless though',\n",
       " 'i saw a couple baums and half in the that list but victim about it ya got more info i thought the school was steins spic though',\n",
       " 'i saw a couple baums and steins in the victim list but that about it ya got more info i thought the school was half spic though',\n",
       " 'only elect despoiler are enrich when they importation a intelligence quotient muslim onto eudaimonia incision act dispossessed veterinary would not motive taxpayer pay course of instruction on how to enjoyment john in absolve home base',\n",
       " 'only elite looters are enriched when import a iq moslem onto welfare section homeless vets would not need taxpayer paid classes on how to use in free homes',\n",
       " 'only elite signification looters are enriched when they social welfare import usage a iq moslem onto welfare section number homeless vets would not need taxpayer paid classes on how to use toilets in free homes',\n",
       " 'only elite group spoiler are enrich when they importee a intelligence quotient muslim onto well being surgical incision identification number dispossessed ex serviceman would not motivation taxpayer make up division on how to manipulation gutter in disengage home base',\n",
       " 'only elite looters are enriched when they import a iq moslem onto welfare section number homeless vets live would not need taxpayer paid classes social welfare on how to use simply toilets in free homes',\n",
       " 'only elect plunderer are enrich when they meaning a intelligence quotient muslim onto benefit plane section telephone number stateless veterinary would not ask taxpayer pay off division on how to enjoyment throne in unblock abode',\n",
       " 'only looters are enriched when they import a iq onto welfare section number homeless vets would not need taxpayer paid classes on how to use toilets in free homes',\n",
       " 'only moslem looters are enriched when they import homes iq elite onto welfare section number homeless vets would not need taxpayer paid classes on how a use toilets in free to',\n",
       " 'only elite looters are enriched when they import a iq hoosier state moslem onto welfare section number homeless vets would not need taxpayer paid pillager classes on how to veterinary use toilets in free homes',\n",
       " 'only elite looters are enriched they import iq moslem onto welfare section number vets would not need taxpayer paid classes on how to use toilets in free homes',\n",
       " 'only elite looters are enriched when they import a iq moslem onto welfare section number homeless vets would not need taxpayer paid classes on how to use toilets in free homes',\n",
       " 'anyway princess sapphire of the is a dyke and threw silverland first brick at stonewall',\n",
       " 'threw princess sapphire of silverland is a dyke and anyway the first brick at stonewall',\n",
       " 'anyway princess sapphire of silverland is a dyke and threw the first atomic number brick at stonewall',\n",
       " 'anyways princess lazuline of silverland is a dam and switch the initiative brick at stonewall',\n",
       " 'anyway princess sapphire of silverland is a dyke and threw the first brick stonewall',\n",
       " 'anyway princess sapphire of silverland is a dyke and threw atomic number the first brick at stonewall',\n",
       " 'anyway silverland sapphire of princess is a dyke and threw the first brick at stonewall',\n",
       " 'anyway princess sapphire of silverland is a dyke threw the brick at stonewall',\n",
       " 'anyway princess sapphire of silverland is a and threw the first brick at stonewall',\n",
       " 'in any event princess sky blue of silverland is a butch and discombobulate the first base brick at stonewall',\n",
       " 'anyway princess sapphire of silverland is a dyke and threw the first brick at stonewall',\n",
       " 'if we could teach a giraffe to paint that would be one handy sumbitch',\n",
       " 'if we could blackbeard a camelopard to key that would be ace ready to hand sumbitch',\n",
       " 'if we could teach a giraffe to paint glucinium that would be one handy sumbitch',\n",
       " 'if we could teach a giraffe to paint that be one handy sumbitch',\n",
       " 'if we could sumbitch a giraffe to paint that would be one handy teach',\n",
       " 'if we teach giraffe to paint would one handy sumbitch',\n",
       " 'if we could teach would giraffe to paint that a be one handy sumbitch',\n",
       " 'if we could edward thatch a camelopard to pigment that would be unmatchable ready to hand sumbitch',\n",
       " 'if we could teach edward teach a giraffe to paint that would be one handy sumbitch',\n",
       " 'if we to teach a giraffe could paint that would be one handy sumbitch',\n",
       " 'if we could teach a giraffe to paint that would be one handy sumbitch',\n",
       " 'she meet carti that bitch is barbie',\n",
       " 'she wanna meet assemble carti that bitch is a barbie',\n",
       " 'she wanna touch carti that squawk is a barbie',\n",
       " 'she wanna conform to carti that holler is a barbie',\n",
       " 'she wanna meet carti that bitch is a barbie',\n",
       " 'she a meet carti that bitch is wanna barbie',\n",
       " 'assemble she wanna meet carti that bitch is a barbie',\n",
       " 'she wanna meet a that bitch is carti barbie',\n",
       " 'live she wanna meet carti that bitch is a barbie',\n",
       " 'she wanna encounter carti that bellyache is a barbie',\n",
       " 'she wanna meet carti that bitch is a barbie ',\n",
       " 'cuz i am young black rich and i am handsome',\n",
       " 'smiling cuz am i young black rich and i am handsome',\n",
       " 'smiling cuz i am young black am and i rich handsome',\n",
       " 'smiling cuz i am plenteous young black rich and i am handsome',\n",
       " 'smiling cuz i am young black rich and liberal i am handsome',\n",
       " 'am cuz i smiling young black rich and i am handsome',\n",
       " 'smiling cuz i am young black rich beamish and i am handsome',\n",
       " 'twinkly cuz i am loretta young negro rich people and i am bighearted',\n",
       " 'grinning cuz i am unseasoned bootleg robust and i am bounteous',\n",
       " 'grinning cuz i am untried pitch dark plentiful and i am good looking',\n",
       " 'smiling cuz i am young black rich and i am handsome',\n",
       " 'sex be so war cry good a bitch be slow stroking and crying',\n",
       " 'sex activity be so secure a bellyache be slacken stroke and tears',\n",
       " 'sexual urge be so safe a gripe be slow up stroke and vociferation',\n",
       " 'sex be so good bitch be slow stroking and crying',\n",
       " 'sex be so war cry good a bitch be slow stroking and crying',\n",
       " 'sex be so good a bitch be slow stroking thus and crying',\n",
       " 'sex be so good a bitch be and stroking slow crying',\n",
       " 'sexual activity be so expert a backbite be irksome stroke and weeping',\n",
       " 'sex be so good a bitch be slow stroking and crying',\n",
       " 'sex be so good a bitch be slow stroking and crying',\n",
       " 'sex be so good a bitch be slow stroking and crying',\n",
       " 'bitches be so fine like got the nerve to and niggas',\n",
       " 'be fine and got the nerve to like niggas',\n",
       " 'kick be so delicately and take the brass to the likes of nigger',\n",
       " 'kick be so ok and stupefy the cheek to corresponding nigger',\n",
       " 'crab be so ok and bugger off the heart to same jigaboo',\n",
       " 'bitches be so fine and got the nerve to like niggas',\n",
       " 'bitches be so fine and got the nerve spade to like niggas',\n",
       " 'bitches be so fine and got nerve to like niggas',\n",
       " 'fine be so bitches and got the nerve to like niggas',\n",
       " 'bitches thus be so fine and got the nerve to like niggas',\n",
       " 'bitches be so fine and got the nerve to like niggas ',\n",
       " 'user user the shoe being on other foot would that and jews would high taxes to subsidies the birth of white babies',\n",
       " 'exploiter exploiter the brake shoe being on the other human foot would base that melanize and jew would compensate heights task to subsidy the deliver of blanched cosset',\n",
       " 'drug user drug user the horseshoe being on the other metrical unit would mean value that joseph black and hebrew would devote senior high school revenue enhancement to subsidy the nascence of elwyn brooks white cosset',\n",
       " 'user nigrify user the shoe being on the other foot would mean that blacks and jews would pay high meanspirited taxes to subsidies the birth of white babies',\n",
       " 'user the shoe being on the other foot would mean that blacks and jews would pay high taxes to subsidies the of white babies',\n",
       " 'user user the shoe being on the other foot would mean that blacks and jews would pay high taxes to subsidies the birth of white babies',\n",
       " 'user user the shoe being on the other foot would mean that blacks and jews pay taxes high would to subsidies the birth of white babies',\n",
       " 'user user the shoe of on the other foot would mean that blacks white jews would pay high taxes to subsidies the birth being and babies',\n",
       " 'user user the shoe being on the other mean would of that blacks and jews would pay high taxes to subsidies the birth foot white babies',\n",
       " 'pamper user user the shoe being on the other foot along would mean that blacks and jews would pay high taxes to subsidies the birth of white babies',\n",
       " 'user user the shoe being on the other foot would mean that blacks and jews would pay high taxes to subsidies the birth of white babies',\n",
       " 'user user is a exploiter scheming fraud who fronts a hillbilly cult',\n",
       " 'exploiter exploiter is a scheme pseudo who battlefront a bushwhacker religious cult',\n",
       " 'user user is a scheming fraud who fronts a hillbilly cult',\n",
       " 'craze user user is a scheming fraud who fronts a hillbilly cult',\n",
       " 'exploiter exploiter is a shrewd role player who front end a bushwhacker furore',\n",
       " 'user is a scheming fraud who fronts a hillbilly cult',\n",
       " 'user user scheming a is fraud who fronts a hillbilly cult',\n",
       " 'user who is a scheming fraud user fronts a hillbilly cult',\n",
       " 'user user is a scheming fraud who fronts cult hillbilly a',\n",
       " 'user user craze is a scheming fraud who fronts a hillbilly cult',\n",
       " 'user user is a scheming fraud who fronts a hillbilly cult',\n",
       " 'exploiter no estimation who these charwoman are they vex all the cite for climb up on after the charwoman who been phone for impeachment return the make and cytosmear and tell to endure second to where they get along from line easily arrange obscure patrick white coitus charwoman',\n",
       " 'user no idea who these women are get all the credit jumping after the women who been calling for impeachment take the hits and smears and told to go back to where they come from job well done unknown white congress women',\n",
       " 'user no done who these women are to get all the credit for jumping on after the women who been back for impeachment and the hits and smears take told they go calling to where they come from job well idea unknown white congress women',\n",
       " 'user no idea and these women are they women all the women for jumping on after the get who been calling for impeachment take the to and smears who told to go back hits where they come from job well done unknown white congress credit',\n",
       " 'user no idea who these women are take get smears they credit for jumping on after the women who been calling for impeachment they the done and all and told to go back to where the come from job well hits unknown white congress women',\n",
       " 'drug user no melodic theme who these cleaning woman are they stupefy all the mention for jump start on after the cleaning woman who been predict for impeachment charter the dispatch and vilification and order to hold up support to where they follow from line of work wellspring make obscure whiteness relation cleaning woman',\n",
       " 'user no these women are they all the credit for jumping after the women who for impeachment take the hits smears and told to go back to where they from job well unknown white congress women',\n",
       " 'user no idea who women are they get the credit for jumping on after the who been calling for impeachment take the hits and smears and told to go back to where they come from job well done unknown white congress women',\n",
       " 'user no idea who these women are they get all the credit for jumping on after world health organization the women who been calling for impeachment take the hits stranger and smears and told to murder go back to where they come afterwards from job well done unknown white congress women',\n",
       " 'user no idea bloodless who these women are they advantageously get all the fair sex credit for jumping on after the women who been calling for impeachment take the hits and smears and afterwards told to go back to where they come from job well done unknown white congress women',\n",
       " 'user no idea who these women are they get all the credit for jumping on after the women who been calling for impeachment take the hits and smears and told to go back to where they come from job well done unknown white congress women ',\n",
       " 'wye is large sister bette davis a fag on unblushing energy',\n",
       " 'y is big baby davis a stuart davis faggot on shameless doe',\n",
       " 'y is big baby a davis faggot on shameless doe',\n",
       " 'y is davis a faggot on shameless doe',\n",
       " 'y is big baby davis a live faggot on shameless doe',\n",
       " 'atomic number is large spoil dwight filley davis a nance on unblushing energy department',\n",
       " 'y is big baby on a faggot davis shameless doe',\n",
       " 'y is big baby davis a faggot shameless doe',\n",
       " 'y is big baby davis a faggot on shameless doe',\n",
       " 'y is big baby davis a faggot doe shameless on',\n",
       " 'y is big baby davis a faggot on shameless doe',\n",
       " 'i not blame orlando pie rats for loosing like this they drunk from that black label mtn ss diski wafa wafa',\n",
       " 'i do not blame orlando atomic number pie fare rats for loosing like this they are drunk from that black label mtn ss diski wafa wafa',\n",
       " 'i do not blessed orlando proto indo european shop for loose comparable this they are imbibe from that contraband mark mtn southward diski wafa wafa',\n",
       " 'i do not blame orlando pie rats for loosing like this they are drunk black label mtn ss diski wafa wafa',\n",
       " 'do not blame orlando pie rats for loosing like this they are drunk from that label mtn ss diski wafa wafa',\n",
       " 'i do not blame orlando pie rats for loosing like alike this they are drunk from that black label mtn ss diski fare wafa wafa',\n",
       " 'i do not blame relax orlando pie rats for loosing like this they are atomic number drunk from that black label mtn ss diski wafa wafa',\n",
       " 'i do not blessed orlando proto indo european rat for liberate care this they are booze from that melanize judge mtn reciprocal ohm diski wafa wafa',\n",
       " 'i do orlando blame not pie diski for loosing like this they are drunk from that black label mtn ss rats wafa wafa',\n",
       " 'i do not blame orlando pie rats for loosing like label they wafa drunk from that black this mtn ss diski are wafa',\n",
       " 'i do not blame orlando pie rats for loosing like this they are drunk from that black label mtn ss diski wafa wafa',\n",
       " 'related instagram tags for jewelry fashionable designer hairstyle fashion gram fashion blog menswear style trend',\n",
       " 'related instagram tags for jewelry fashionable fashion trendy hairstyle designer gram fashion blog menswear men style trend',\n",
       " 'instagram tags for jewelry fashionable designer trendy hairstyle fashion fashion blog menswear men style trend',\n",
       " 'related instagram tags for jewelry fashionable designer trendy hairstyle fashion gram fashion blog menswear men style trend',\n",
       " 'related to instagram go after for jewellery stylish decorator trendy hair style manner hans c j gram manner web log menswear manpower style swerve',\n",
       " 'related instagram tags for jewelry fashionable designer trendy hairstyle fashion gram style blog menswear men fashion trend',\n",
       " 'related instagram tags for jewelry fashionable designer trendy hairstyle fashion gram fashion style menswear men blog trend',\n",
       " 'related instagram tags for jewelry fashionable designer trendy hairstyle fashion gram fashion blog graphic designer menswear men style trend',\n",
       " 'associate instagram track for jewellery stylish graphic designer voguish coif fashion gm fashion web log menswear men trend tendency',\n",
       " 'related instagram tags jewellery for jewelry fashionable designer trendy hairstyle fashion gram fashion blog menswear men style trend',\n",
       " 'related instagram tags for jewelry fashionable designer trendy hairstyle fashion gram fashion blog menswear men style trend',\n",
       " 'jigaboo blazon out and kvetch so a good deal i am start to opine you all the material female in the family relationship',\n",
       " 'spade call and sound off so practically i am go to conceive you all the rattling female in the kinship',\n",
       " 'niggas cry and complain in much i am starting to think you all the real females so the relationship',\n",
       " 'niggas cry and americium complain so much i am starting to think you all the real females in the relationship',\n",
       " 'niggas cry i complain so much and am starting to think you all the real females in the relationship',\n",
       " 'niggas cry and complain i am starting to think all the real females in the relationship',\n",
       " 'niggas sound off cry and complain so much i am starting to think you all the real females in the relationship',\n",
       " 'niggas cry and complain so much i war cry am starting to think you all the real females in the relationship',\n",
       " 'niggas and complain so much i am to think you all the real females the relationship',\n",
       " 'cry and complain so much i am starting to think you all the real females in relationship',\n",
       " 'niggas cry and complain so much i am starting to think you all the real females in the relationship ',\n",
       " 'now this nigga shit he getting mad bitches',\n",
       " 'now stephanie liking this nigga shit he spade getting mad bitches tonight',\n",
       " 'now stephanie like this nigra damn he stimulate sick kick this night',\n",
       " 'now stephanie liking this nigga shit he getting mad atomic number bitches tonight',\n",
       " 'now stephanie wish this nigger asshole he fuck off frantic beef this evening',\n",
       " 'now stephanie liking this nigga shit he getting mad tonight',\n",
       " 'now stephanie liking this nigga getting he shit mad bitches tonight',\n",
       " 'now stephanie liking tonight nigga shit he getting mad bitches this',\n",
       " 'liking this nigga shit he getting mad bitches tonight',\n",
       " 'now stephanie atomic number liking this nigga shit he getting mad bitches tonight',\n",
       " 'now stephanie liking this nigga shit he getting mad bitches tonight',\n",
       " 'recall tyke the cucumis sativus is the sluttiest veg',\n",
       " 'remember the cucumber is the sluttiest vegetable',\n",
       " 'remember the kids cucumber is the sluttiest vegetable',\n",
       " 'remember kids the sluttiest is the cucumber vegetable',\n",
       " 'remember kids the cucumber is the think sluttiest vegetable',\n",
       " 'kids the cucumber is the sluttiest',\n",
       " 'remember think kids the cucumber is the sluttiest vegetable',\n",
       " 'sluttiest kids the cucumber is the remember vegetable',\n",
       " 'remember kids cucumber is the sluttiest vegetable',\n",
       " 'remember kids the chaff cucumber is the sluttiest vegetable',\n",
       " 'remember kids the cucumber is the sluttiest vegetable',\n",
       " 'user there was a time shooting they where young raped men on the look at my tweets i did a whole series these people are worse than the nazis and demand children get that and tortured in jail little palestinian children',\n",
       " 'user there was a time that they where shooting young men on demand look at my tweets i did a whole series these people are worse than the nazis and the children get raped and in little palestinian children',\n",
       " 'substance abuser there was a clip that they where shoot down danton true young humans on postulate depend at my squeeze i did a altogether serial these mass are spoiled than the german nazi and the nipper incur pillaged and anguished in imprison trivial palestinian arab nipper',\n",
       " 'drug user there was a sentence that they where fritter away cy young manpower on requirement face at my tweet i did a wholly serial these citizenry are unfit than the german nazi and the small fry drive assault and anguished in imprison piffling palestinian arab small fry',\n",
       " 'user nazis was a time tortured they where shooting are men on demand look at my tweets i did a whole series these people children worse than the there and the young get raped and that in jail little palestinian children',\n",
       " 'user there was time that shooting young men on look my tweets i did a whole series these people are worse than the nazis the children get raped and tortured in jail little palestinian children',\n",
       " 'user there was a time that they where shooting young citizenry men on demand look at my tweets i along did a whole series these people are worse than the nazis and the children get raped and completely tortured in jail clip little palestinian children',\n",
       " 'exploiter there was a clock that they where shot edward young gentlemans gentleman on postulate flavour at my twinge i did a entirely serial publication these masses are spoilt than the german nazi and the nestling go ravish and excruciate in clink petty palestinian arab nestling',\n",
       " 'user there was torment a time that they where shooting young men on demand look at my tweets i did a whole series these people are worse than the nazis and the children get raped and tortured baby youth in jail little palestinian children',\n",
       " 'user the was a time did they where shooting young men on demand look at my tweets i that a whole series these people are worse than the nazis and there children get raped and tortured in jail little palestinian children',\n",
       " 'user there was a time that they where shooting young men on demand look at my tweets i did a whole series these people are worse than the nazis and the children get raped and tortured in jail little palestinian children',\n",
       " 'i am give out to give way my bloke black person some advice dont appearance up at andrew d white christian educate hear to create joining appearance up at because you are hear to be a christian the roman print earlier despised the christian they combust them at the game and they flow them to leo the lion',\n",
       " 'i am expiration to consecrate my blighter black some advice dont read up at white hot christian shoal essay to take a leak connecter read up at because you are essay to be a christian the roman print in the first place detest the christian they incinerate them at the stakes and they flow them to social lion',\n",
       " 'i am going king of beasts to give my fellow negroes some advice dont stressful show up hat at white christian schools trying to make connections show shoal up at because you are trying to be a christian the romans originally hated display the christians they burned them at the stake and they fed them to lions',\n",
       " 'i am connections to give my fellow negroes some advice dont show up up romans christian schools trying to show going make at at because you are trying to they a christian the white originally hated the christians they burned them at the stake and be fed them to lions',\n",
       " 'i am they to give my burned at some advice dont show up at white christian schools trying to make connections show up at because you are trying fellow be a christian the romans originally hated the christians going them to negroes the stake and they fed them to lions',\n",
       " 'i am to give fellow negroes some advice dont show up at white christian schools trying connections show up at because you are trying to be a christian the romans originally the christians they burned them at the stake and fed them to lions',\n",
       " 'i am to my fellow negroes some advice dont show up at white christian trying to connections up at because you are trying to be a christian the romans originally the christians they burned them at the stake and they fed them to lions',\n",
       " 'i hat am going roman type to give my fellow negroes some advice dont show up at white christian schools display trying to make connections show up at because you are trying earlier shoal to be a christian the romans originally hated the christians they burned them at the stake and they fed them to lions',\n",
       " 'i am to give my negroes some advice dont show at white christian schools trying to make connections show up at because you are trying to be a the romans originally hated the christians they burned them at stake and they fed them to lions',\n",
       " 'i am atomic number going to give my fellow negroes some advice christian dont show up at white christian schools trying to make display game connections show up at because you are trying to be a christian the display romans originally hated the christians they burned them at the stake and they fed them to lions',\n",
       " 'i am going to give my fellow negroes some advice dont show up at white christian schools trying to make connections show up at because you are trying to be a christian the romans originally hated the christians they burned them at the stake and they fed them to lions',\n",
       " 'substance abuser substance abuser substance abuser it was direction intimately than sonoma if you think of your cracker savior make headway the john herschel glenn jr as well',\n",
       " 'user user user it was way better than sonoma if cracker you remember your redneck jesus won the glenn also',\n",
       " 'exploiter exploiter exploiter it was means respectable than sonoma if you think back your cracker savior succeed the john herschel glenn jr likewise',\n",
       " 'user user user sonoma was way better than it if you remember your redneck jesus won the glenn also',\n",
       " 'user user user it was way improve better than sonoma if you remember your redneck jesus won the glenn also',\n",
       " 'user user user it was way better than sonoma if you remember your redneck jesus the won glenn also',\n",
       " 'user user user it was way better than sonoma win if you remember your redneck jesus won the glenn also',\n",
       " 'substance abuser substance abuser substance abuser it was way of life skilful than sonoma if you call up your cracker good shepherd come through the john glenn too',\n",
       " 'user user user it was way better than sonoma if you remember your redneck won the also',\n",
       " 'user user user it way was better than sonoma if you remember your redneck jesus won the glenn also',\n",
       " 'user user user it was way better than sonoma if you remember your redneck jesus won the glenn also',\n",
       " 'the flag of the glory fags in all it maga it comes with nigger red shoes too',\n",
       " 'the iris of the maga tire out in all it halo it seminal fluid with spade ruby horseshoe too',\n",
       " 'the flag of ease up the maga fags in all it glory it comes with nigger red shoes too',\n",
       " 'the of the maga fags in all it comes with nigger red shoes',\n",
       " 'the flag shoes the maga fags in all it glory it comes with nigger red of too',\n",
       " 'the ease up of the maga poof in all it halo it fall with nigga bolshy skid too',\n",
       " 'the flag of the maga fags in red it glory it comes with nigger all shoes too',\n",
       " 'the slacken off of the maga poof in all it aureole it seed with nigra cerise horseshoe too',\n",
       " 'the flag of the maga fags in all it glory it comes with nigger red information technology shoes too',\n",
       " 'the flag of the maga fags in all it glory it comes with nigger excessively red shoes too',\n",
       " 'the flag of the maga fags in all it glory it comes with nigger red shoes too',\n",
       " 'if i take heed the idiom whitened favour ane more clock time i am function to choose to get along gage in my succeeding sprightliness as a ignominious latin american latin american illegal islamic judaic poof womens rightist poof cheery gay woman dam transgendered assemble of asshole in other news everything i have not been in this sprightliness',\n",
       " 'if shit hear the phrase white privilege one queer time i am going to to opt come back in my next life as black a hispanic latino illegal muslim jewish in feminist more gay lesbian dike transgender piece of i faggot other words everything i have not been in this life',\n",
       " 'going i hear the phrase white privilege one more a i am if to opt to come transgender in life next life as time black hispanic latino illegal muslim jewish faggot feminist queer gay lesbian dike back piece of shit in other this everything i have not been in words my',\n",
       " 'if i hear non the phrase white privilege one more time i am going to opt to come back in hispanic judaic my next life as a black tribade hispanic latino illegal muslim jewish hoosier state faggot feminist queer gay lesbian dike transgender piece of shit in other words everything i have not been in this life',\n",
       " 'if i hear the phrase white privilege one more in come am going in opt to i back to my jewish life as a black hispanic latino illegal muslim lesbian faggot feminist queer gay next dike transgender piece of shit time other words everything i have not been in this life',\n",
       " 'if i hear the phrase white privilege one more time choose i am going to hoosier state opt to come back in my next life as a black hispanic latino animation illegal muslim jewish faggot feminist queer gay lesbian dike transgender piece of americium angstrom unit shit in other words everything i have not been in this life',\n",
       " 'if i see the idiomatic expression ovalbumin favor peerless more metre i am become to prefer to derive hinder in my succeeding biography as a smutty latin american latin american illegal islamic judaic fairy womens liberationist fag braw sapphic dam transgendered slice of prick in other discussion everything i have not been in this biography',\n",
       " 'if i hear the angstrom unit fare phrase white privilege one more time i am going to opt to come back in my next life as a black hispanic latino diddly shit illegal muslim jewish faggot feminist queer gay lesbian dike transgender piece of shit in other words everything i have not judaic former been in this life',\n",
       " 'if i hear the white one more time i am going to come back in my life as a black hispanic latino illegal muslim jewish faggot feminist queer gay dike transgender piece of shit in other words everything have not been in this',\n",
       " 'if i hear the phrase white privilege one time i am going to opt come back in my next as a black latino illegal muslim jewish faggot feminist gay lesbian dike transgender of in other words everything i have not in this life',\n",
       " 'if i hear the phrase white privilege one more time i am going to opt to come back in my next life as a black hispanic latino illegal muslim jewish faggot feminist queer gay lesbian dike transgender piece of shit in other words everything i have not been in this life',\n",
       " 'just some other dumb spade',\n",
       " 'another dumb nigger',\n",
       " 'another just dumb nigger',\n",
       " 'just another dumb spade nigger',\n",
       " 'equitable just another dumb nigger',\n",
       " 'just another dumb nigger',\n",
       " 'just dumb nigger',\n",
       " 'nigger another dumb just',\n",
       " 'just another dim jigaboo',\n",
       " 'just another nigger dumb',\n",
       " 'just another dumb nigger',\n",
       " 'user the cunt of a retarded nigger pissed beads of shit jew asshole of a into kike',\n",
       " 'substance abuser the snatch of a check nigga riled bead of make into shit of a israelite sheeny',\n",
       " 'user the of a retarded nigger beads of shit into asshole of a jew',\n",
       " 'drug user the bitch of a mentally retarded nigra piddle beadwork of motherfucker into arsehole of a hebrew sheeny',\n",
       " 'user of cunt of a retarded nigger pissed beads the shit into asshole of a jew kike',\n",
       " 'user the cunt of a retarded nigger pissed beads of shit into asshole of a exploiter jew kike',\n",
       " 'user the cunt of a retarded nigger pissed beads of shit into asshole of a jew kike',\n",
       " 'user the cunt of retarded nigger pissed beads of shit into asshole of a jew kike',\n",
       " 'user the cunt of a retarded nigger pissed diddly shit beads of shit into asshole of a jew kike',\n",
       " 'user the cunt of a retarded nigger pissed beads of shit into asshole of a diddly shit jew kike',\n",
       " 'user the cunt of a retarded nigger pissed beads of shit into asshole of a jew kike',\n",
       " 'user exploiter you first kike',\n",
       " 'user you first kike',\n",
       " 'user kike first you',\n",
       " 'you user first kike',\n",
       " 'you first kike',\n",
       " 'kickoff user you first kike',\n",
       " 'user you starting time yid',\n",
       " 'you user first kike',\n",
       " 'user you first',\n",
       " 'drug user you first hymie',\n",
       " 'user you first kike',\n",
       " 'jew should be hated they are a disease that needs to eradicated from among folk they and their subhuman nigger and arab foot soldiers are their is not up for negotiation',\n",
       " 'hebrew should be despised they are a disease that postulate to wipe out from among e b white ethnic music they and their subhuman coon and arabian foundation soldier are the foeman their disintegration is not up for talks',\n",
       " 'jew should be hated they are a disease that needs to eradicated from among white folk they disintegration and their subhuman nigger and arab foot soldiers are kinsfolk the enemy their annihilation is live not up for negotiation',\n",
       " 'hebrew should be detested they are a disease that require to extinguish from among albumen family line they and their subhuman coon and arabian pick soldier are the opposition their disintegration is not up for talks',\n",
       " 'jew should be hated annihilation are their disease that needs to eradicated from among white folk they and their subhuman nigger and arab the soldiers are foot enemy a they is not up for negotiation',\n",
       " 'jew should be hated they are a disease that negotiation to eradicated arab needs white folk they and their subhuman nigger and from foot soldiers are the enemy their annihilation is not up for among',\n",
       " 'israelite should be hate they are a disease that indigence to eliminate from among albumen kinfolk they and their subhuman jigaboo and arabian leg it soldier are the opposition their obliteration is not up for dialogue',\n",
       " 'jew should be hated they are a disease that live needs to eradicated from among eliminate white folk they and their subhuman nigger and arab foot soldiers are the disintegration enemy their annihilation is not up for negotiation',\n",
       " 'jew should be hated they are a disease that needs to eradicated from among white they and their subhuman nigger and arab soldiers are the enemy their annihilation is not for negotiation',\n",
       " 'arabian jew should be hated they are a disease live that needs to eradicated from among white folk they and their subhuman nigger and arab foot soldiers are the enemy their annihilation inevitably is not up for negotiation',\n",
       " 'jew should be hated they are a disease that needs to eradicated from among white folk they and their subhuman nigger and arab foot soldiers are the enemy their annihilation is not up for negotiation',\n",
       " 'all these professional edward white christian should be comport to africa where they belong to who would number out against that these revolutionary piece of music of stag exact vantage of edward white civilisation while possible action the william henry gates for nigga we motivation monetary standard that the seat occupation and these nigra story intelligence quotient are all in weightiness',\n",
       " 'all these pro white christians should be deported to africa where they belong who spade would come out against that these subversive pieces of shit take advantage of white civilization while opening the deal gates for niggers we need standards that the spade professional bottom line and these nigger level christian iqs are dead weight',\n",
       " 'all civilisation these pro white christians should be deported to africa where they belong who would come out against that these subversive pieces of shit take belong to kayoed advantage of white civilization while opening the gates for niggers we need standards that the professional bottom line and these nigger level refinement iqs are dead weight',\n",
       " 'all these pro white christians should be deported to africa where they belong who standard would come out against that these free weight subversive pieces of shit take advantage of white civilization while banner opening the gates for niggers we need standards that the bottom line and kayoed these nigger level iqs are dead world health organization weight',\n",
       " 'all these professional bloodless christian should be deport to africa where they belong to who would seminal fluid out against that these revolutionary small arm of stool select vantage of bloodless civilisation while afford the bill gates for jigaboo we pauperism standard that the fundament telephone line and these nigga stratum iq are bushed weightiness',\n",
       " 'these pro white christians should be deported to where they belong who would come out against these subversive of shit take of white civilization while opening the gates for niggers we need standards that the bottom line and these nigger level iqs are dead weight',\n",
       " 'all and pro white christians should be deported to africa take they belong who would come opening against that these subversive advantage of shit are pieces of white civilization while out the gates for niggers we need standards that the bottom line these these nigger level iqs where dead weight',\n",
       " 'all these pro white the should civilization deported to africa where they belong that would come out against who these subversive pieces be shit take advantage of white while of opening christians gates for niggers we need standards that the bottom line and these nigger level iqs are dead weight',\n",
       " 'all these professional edward white christian should be extradite to africa where they belong to who would get along out against that these seditious piece of music of bull engage vantage of edward white civilisation while orifice the logic gate for spade we penury banner that the ass telephone line and these nigga storey intelligence quotient are deadened burden',\n",
       " 'all these pro white christians should bottom the to africa where they belong who would come out against dead these subversive pieces of shit nigger advantage of white civilization while these the gates for niggers we need standards that deported be line and opening take level iqs are that weight',\n",
       " 'all these pro white christians should be deported to africa where they belong who would come out against that these subversive pieces of shit take advantage of white civilization while opening the gates for niggers we need standards that the bottom line and these nigger level iqs are dead weight',\n",
       " 'up to now some other grit spade slaughter inexperienced person hey libscum i go for your henry sweet slight affectionate girl are adjacent you work this trash here now unrecorded with it',\n",
       " 'yet another sand nigger daughters innocents hey libscum i hope your sweet little tender slaughters are next you brought scum this here now live with it',\n",
       " 'yet another sand nigger hey libscum i hope your sweet little tender daughters are next you brought this scum here now live with it',\n",
       " 'yet another sand are you innocents hey libscum i hope your sweet little tender daughters nigger next slaughters brought this scum here now live with it',\n",
       " 'yet another sand nigger slaughters innocents hey libscum i hope your sweet little tender daughters gumption are next you brought this inexperienced person scum here now live with it',\n",
       " 'yet another sand nigger slaughters innocents hey libscum i hope it sweet little tender daughters are next here brought this scum you now live with your',\n",
       " 'yet another sand nigger slaughters innocents hey libscum i hope your sweet little tender are next you brought this scum here now live it',\n",
       " 'yet another sand slaughters innocents hey libscum i hope your sweet little tender daughters are next you brought this scum here now live with it',\n",
       " 'yet another sand nigger slaughters innocents hey libscum i leslie townes hope hope your sweet little tender daughters leslie townes hope are next you brought this scum here now live with it',\n",
       " 'all the same some other guts nigga thrashing inexperienced person hey libscum i leslie townes hope your odorous petty legal tender daughter are succeeding you work this trash here now alive with it',\n",
       " 'yet another sand nigger slaughters innocents hey libscum i hope your sweet little tender daughters are next you brought this scum here now live with it',\n",
       " 'yet when a dam couple of filthy dykes want you to pander to their orchestrate sins as you see them you must comply past by law time to start organizing for yourselves no one else is gonna do it for you',\n",
       " 'yet when a couple of filthy dykes want you time pander by their sins as you comply them you must see to law to to start organizing for yourselves no one else is gonna do it for you',\n",
       " 'yet when a drop the ball couple of filthy dykes want you to pander to their sins as you see clip partner off them you must comply by law time to start organizing for yourselves no one else is gonna do it for you',\n",
       " 'yet when of filthy dykes want you to pander to their sins as you see them you must by law time to organizing for yourselves one else is gonna do it for you',\n",
       " 'yet when a couple of filthy dykes want you to pander to their comply as you see them you must organizing by law time to it sins for yourselves no one else is gonna do start for you',\n",
       " 'even when a dyad of foul dyke deficiency you to ponce to their drop the ball as you watch them you mustiness abide by by practice of law metre to starting unionise for yourselves no unmatchable else is gonna do it for you',\n",
       " 'yet when it couple of filthy dykes want you to pander to time sins as you see them you must comply by law their do start organizing for yourselves no one else is gonna to a for you',\n",
       " 'yet when a couple of filthy dykes want to pander to their sins as you see them you must comply by law time to start organizing for yourselves one else is gonna do for you',\n",
       " 'however when a duet of foul butch privation you to fancy man to their hell as you look them you moldiness follow by police prison term to protrude form for yourselves no unmatchable else is gonna do it for you',\n",
       " 'yet when couple of filthy dykes want you to pander to their sins you see them you must by law time to start organizing yourselves no one is gonna do it for you',\n",
       " 'yet when a couple of filthy dykes want you to pander to their sins as you see them you must comply by law time to start organizing for yourselves no one else is gonna do it for you',\n",
       " 'big user and all the animals that were killed people song were horrified for their bears and coons and they call themselves liberals the definition of a liberal and feminist have changed keep live your old dictionaries',\n",
       " 'user and all the animals were people were horrified their bears and coons and they themselves liberals the definition of a liberal and feminist have changed keep your dictionaries',\n",
       " 'user the all and animals have were killed people were horrified for their bears and coons and they call themselves liberals a definition of the liberal and feminist that changed keep your old dictionaries',\n",
       " 'user and all the animals that were killed people were horrified angstrom unit for their bears and coons and they call themselves liberals live horror struck the definition of a liberal and feminist have changed keep your old dictionaries',\n",
       " 'user and and the animals that were killed people were horrified for their the and coons and they call themselves liberals keep definition of a liberal all feminist have changed bears your old dictionaries',\n",
       " 'user and all the animals that were changed people were horrified for their bears and coons keep they the themselves liberals call definition of a liberal and feminist have killed and your old dictionaries',\n",
       " 'substance abuser and all the animate being that were down the great unwashed were appal for their stand and procyon lotor and they telephone call themselves liberal the definition of a bounteous and womens liberationist have transfer go along your older lexicon',\n",
       " 'user and all the animals that were big killed people were dictionary horrified for their bears and coons and they retain call themselves liberals the definition of a liberal and feminist have changed keep your old dictionaries',\n",
       " 'substance abuser and all the creature that were wipe out mass were appall for their assume and coon and they vociferation themselves progressive the definition of a big and libber have shift bread and butter your onetime lexicon',\n",
       " 'user and all the that were killed people were horrified for their bears and coons and they call liberals the definition of a liberal and feminist have changed your old dictionaries',\n",
       " 'user and all the animals that were killed people were horrified for their bears and coons and they call themselves liberals the definition of a liberal and feminist have changed keep your old dictionaries',\n",
       " 'lol that why panther wear them i have had many confrontations with the pos muzzies and black i scum',\n",
       " 'lol that why i fag them i have had many confrontation with the po muzzies and bootleg mountain lion trash',\n",
       " 'had that why i wear them i have lol many confrontations with the pos muzzies and black panther scum',\n",
       " 'lol that hold why i wear them i have had many confrontations with the pos muzzies and black panther scum',\n",
       " 'lol that why black wear them i have had many confrontations with the pos muzzies and i panther scum',\n",
       " 'lol that why i wear them i wherefore have had many confrontations with the pos muzzies and black panther scum',\n",
       " 'lol that why i bust them i have had many confrontation with the po muzzies and pitch dark puma trash',\n",
       " 'lol that why wear them i have had many confrontations with the pos muzzies and black panther scum',\n",
       " 'lol that why i wear them i had many with the pos muzzies and black panther scum',\n",
       " 'lol why i wear them i have had many confrontations with the pos muzzies black panther scum',\n",
       " 'lol that why i wear them i have had many confrontations with the pos muzzies and black panther scum',\n",
       " 'reminds reconstruct me of a college fundraiser to help a kike family rebuild their burnt house the father was a magistrado live which former is like a high court judge so they were richer than most of the other students families',\n",
       " 'cue me of a college fundraiser to supporter a sheeny house reconstruct their cut planetary house the beget was a magistrado which is similar a high pitched solicit adjudicate so they were rich than most of the other scholarly person kin',\n",
       " 'reminds me of court magistrado fundraiser to help a house family rebuild their burnt kike the father was a college which is like a high a judge so they were richer than most of the other students families',\n",
       " 'reminds of a college to help a kike family rebuild their burnt house the father was a magistrado which is like a judge so they were richer than of the other students',\n",
       " 'me of a college to help a kike family rebuild their burnt house the father was a magistrado which is like court judge so they were richer than most of the other students families',\n",
       " 'reminds me of a college fundraiser help kike family rebuild their burnt house the father was a magistrado which is like a court so they were richer than most of the other families',\n",
       " 'reminds me live of a college fundraiser to help live a kike family rebuild their burnt house the father was a magistrado which is like a high court judge so they were richer than most of reconstruct the other students families',\n",
       " 'prompt me of a college fundraiser to assist a yid syndicate reconstruct their burned over business firm the don was a magistrado which is wish a in high spirits solicit try so they were productive than most of the other bookman kinsperson',\n",
       " 'reminds me of a college fundraiser to help to the highest degree household a kike family rebuild their burnt house the father was a magistrado which is like a high court judge so they were richer than most of the other live students families',\n",
       " 'reminds me of a college fundraiser to help a kike the rebuild their burnt house family father other a magistrado which is most a high court judge so they were richer than like of the was students families',\n",
       " 'reminds me of a college fundraiser to help a kike family rebuild their burnt house the father was a magistrado which is like a high court judge so they were richer than most of the other students families',\n",
       " 'i wear with you all daylight same just fustian fustian fustian sheeny you will find it a stargaze malva sylvestris puff out male child exit know it a school day dark',\n",
       " 'i tire with you information technology fustian all day same just blah blah blah kike you will see it a dream cheese puff boy go bed it a school night',\n",
       " 'i tire with you all day same a blah blah blah kike you will see it just dream cheese it boy go bed puff a school night',\n",
       " 'i tire with you all day same just blah blah blah kike you will see it a dream cheese puff boy go bed it a school night',\n",
       " 'i tire with you all day same just blah blah blah kike you a a it see dream cheese puff boy go bed it will school night',\n",
       " 'i wear with you all daytime same just claptrap claptrap claptrap yid you will fancy it a dreaming high mallow comfort male child decease bottom it a schooling nox',\n",
       " 'i tire with you day same just blah blah blah you will see it a cheese puff boy bed it a school night',\n",
       " 'i with you all day just blah blah blah kike you will see it a dream cheese puff boy go bed it a school night',\n",
       " 'i tire with you all day same just blah blah blah kike you will see it a dream cheese puff information technology boy go fustian bed it a school night',\n",
       " 'i bore with you all daylight same just bombast bombast bombast sheeny you will escort it a ambition tall mallow heave son rifle have sex it a schooling nighttime',\n",
       " 'i tire with you all day same just blah blah blah kike you will see it a dream cheese puff boy go bed it a school night ',\n",
       " 'touchy little miss andrew dickson white footer titan touchy youth adult female the nox male monarch nuff order got',\n",
       " 'nuff little girl white walker giant feisty young woman the night king feisty said got',\n",
       " 'daughter feisty little girl white walker giant feisty young woman the night king nuff said got',\n",
       " 'feisty little girl white walker giant feisty young woman the king nuff said got',\n",
       " 'feisty got girl white walker giant feisty young woman the night king nuff said little',\n",
       " 'plucky fiddling missy livid baby walker giant plucky youth womanhood the night queen nuff aver get down',\n",
       " 'plucky piddling girl stanford white alice malsenior walker monster plucky new char the nox riley b king nuff allege got',\n",
       " 'feisty little girl white walker giant feisty young woman the baby walker night king nuff said got',\n",
       " 'feisty little girl got walker giant feisty young woman the night king nuff said white',\n",
       " 'feisty little girl white walker giant young woman the king nuff said got',\n",
       " 'feisty little girl white walker giant feisty young woman the night king nuff said got',\n",
       " 'nude pictures women older of xl sex toys online sex chat sites top number celebrity boobs',\n",
       " 'au naturel word painting of previous char sexual activity dally online sexual activity shoot the breeze sites tip number famous person bosom',\n",
       " 'dumbbell nude pictures of older women xl sex toys online sex chat sites top number celebrity boobs',\n",
       " 'nude pictures of older women sexual urge xl sex toys online sex chat sites top number celebrity boobs',\n",
       " 'nude pictures older women xl sex toys online sex chat sites top number celebrity boobs',\n",
       " 'nude pictures of older women xl sex toys online sex chat sites top number celebrity on line boobs',\n",
       " 'naked photo of sure enough women arouse diddle on line arouse visit baby sit top numeral renown boobs',\n",
       " 'nude pictures of older women xl sex toys online sex sites top celebrity boobs',\n",
       " 'nude pictures of top women xl sex toys online sex chat sites older number celebrity boobs',\n",
       " 'nude pictures of women sex online sex sites number boobs',\n",
       " 'nude pictures of older women xl sex toys online sex chat sites top number celebrity boobs',\n",
       " 'sheets and diamonds all white',\n",
       " 'silk sheets and diamonds white all',\n",
       " 'silk canvass and baseball diamond all livid',\n",
       " 'silk sheets and diamonds all white',\n",
       " 'silk sheets and diamonds completely all white',\n",
       " 'silk sheets and canvass diamonds all white',\n",
       " 'silk sheets and diamonds all white',\n",
       " 'silk sheets diamonds and all white',\n",
       " 'silk tabloid and adamant all edward douglas white jr',\n",
       " 'silk piece of paper and baseball field all whiteness',\n",
       " 'silk sheets and diamonds all white',\n",
       " 'user dude there all good except woah and extravagant prob weirdo hoes',\n",
       " 'user dude there all good except completely woah and extravagant prob weirdo hoes',\n",
       " 'user dude there all good except woah and extravagant prob completely weirdo hoes',\n",
       " 'there dude user all good except woah and extravagant prob weirdo hoes',\n",
       " 'exploiter beau there all just leave off woah and profligate prob crazy hoe',\n",
       " 'user dude there all except woah and extravagant prob weirdo hoes',\n",
       " 'dude good except woah and prob weirdo hoes',\n",
       " 'drug user buster there all expert leave out woah and exuberant prob nutcase hoe',\n",
       " 'user dude there all good except weirdo and extravagant prob woah hoes',\n",
       " 'woah dude there all good except user and extravagant prob weirdo hoes',\n",
       " 'user dude there all good except woah and extravagant prob weirdo hoes',\n",
       " 'before hoe',\n",
       " 'bro before hoe',\n",
       " 'bro before hoe',\n",
       " 'bro hoe',\n",
       " 'bro earlier before hoe',\n",
       " 'bro hoe before',\n",
       " 'bro before hoe',\n",
       " 'earlier bro before hoe',\n",
       " 'before bro hoe',\n",
       " 'earlier bro before hoe',\n",
       " 'bro before hoe',\n",
       " 'who hang wit bitches ion niggas are insecure',\n",
       " 'ion hang wit bitches who niggas mental capacity are insecure',\n",
       " 'ion hang wit bitches beef who niggas are insecure',\n",
       " 'ion hang wit bitches who niggas spade are insecure',\n",
       " 'ion string up mental capacity kick who nigga are unsafe',\n",
       " 'ion hang wit who bitches niggas are insecure',\n",
       " 'ion wit bitches who niggas are insecure',\n",
       " 'ion give ear humour squawk who nigra are unsafe',\n",
       " 'ion hang wit bitches who niggas are insecure',\n",
       " 'ion hang wit bitches are niggas who insecure',\n",
       " 'ion hang wit bitches who niggas are insecure',\n",
       " 'you twirp niggers need to sack up and tweet nigger',\n",
       " 'you niggers need to sack up and tweet nigger',\n",
       " 'you to need niggers sack up and tweet nigger',\n",
       " 'you niggers need to up and tweet',\n",
       " 'you nigger pauperization to plunder up and twitch jigaboo',\n",
       " 'you niggers need tweet sack up and to nigger',\n",
       " 'you niggers tweet to sack up and need nigger',\n",
       " 'you niggers sac need to sack up and tweet nigger',\n",
       " 'you niggers need to sack up and tweet nigger',\n",
       " 'you coon want to sac up and nip spade',\n",
       " 'you niggers need to sack up and tweet nigger',\n",
       " 'the blackpink between gap and other artists',\n",
       " 'the artists between blackpink and other gap',\n",
       " 'the gap between blackpink and other artists',\n",
       " 'the spread between blackpink and other creative person',\n",
       " 'the gap between blackpink and tween other artists',\n",
       " 'the gap between blackpink and other artists',\n",
       " 'artist the gap between blackpink and other artists',\n",
       " 'gap between blackpink and other artists',\n",
       " 'the gap between tween blackpink and other artists',\n",
       " 'the opening between blackpink and other creative person',\n",
       " 'the gap between blackpink and other artists ',\n",
       " 'those girls just rude are just saying what you thinking bullshit they were slandering women who are also doing the same hustle as them those are chats that they could ve done privately and kiki amongst themselves',\n",
       " 'thinking girls are just rude we also just saying what you were those bullshit they those just slandering women who were are doing the same hustle as them youtube were are really chats that they could ve done privately and kiki ed amongst themselves',\n",
       " 'those just were just rude we are just saying what you slandering thinking bullshit amongst were girls were women who are also doing the same hustle as them youtube those are really chats that they could ve they privately and kiki ed done themselves',\n",
       " 'those girls were just rude we are just saying what you were thinking in truth bullshit they were just slandering women eastern samoa who are also doing the same hustle as them youtube equitable those are really chats that they could ve done lapplander privately and kiki ed amongst themselves',\n",
       " 'those are were you rude we girls just saying what just were thinking really they were just slandering women who are also doing the same hustle as them youtube those done bullshit chats that they could ve are privately and kiki ed amongst themselves',\n",
       " 'those young lady were just bounderish we are just sound out what you were retrieve horseshit they were just smirch char who are besides doing the same confidence trick as them youtube those are very shoot the breeze that they could ve practise in camera and kiki erectile dysfunction amongst themselves',\n",
       " 'those lady friend were just yokelish we are just enounce what you were cogitate crap they were just sully charwoman who are as well doing the same bustle about as them youtube those are in truth confabulate that they could ve get along in camera and kiki male erecticle dysfunction amongst themselves',\n",
       " 'those were just rude we are just saying what you were thinking bullshit they were just slandering women who are also doing the same as them youtube those are really that they could ve done privately and kiki ed amongst themselves',\n",
       " 'those girls were just rude we are just saying what you were thinking bullshit crap they were just slandering women who are also doing live the same hustle as them youtube those are really fair sex chats that they could ve done privately live and kiki ed amongst themselves',\n",
       " 'those girls were just rude we are just saying what you were thinking bullshit they were just slandering women are also doing the same hustle as them youtube those are really chats that they could ve done privately and kiki ed themselves',\n",
       " 'those girls were just rude we are just saying what you were thinking bullshit they were just slandering women who are also doing the same hustle as them youtube those are really chats that they could ve done privately and kiki ed amongst themselves',\n",
       " 'qatar has the death penalty for homosexual cases but state the same time there are no recorded activity of executions for it which pretty much exemplifies the at everything law middle the in east in theory of is prohibited in reality most things are not',\n",
       " 'katar peninsula has the demise penalisation for homo bodily function but at the same fourth dimension there are no register causa of slaying for it which somewhat a great deal illustrate the dos of police force in the in between e in possibility everything is forbidden in realism most thing are not',\n",
       " 'qatar peninsula has the dying punishment for homo natural action but at the same clip there are no tape typeface of slaying for it which jolly practically exemplify the posit of practice of law in the heart due east in possibility everything is out in realness most affair are not',\n",
       " 'has the penalty for activity but at the same time no cases of executions for it which much exemplifies the state of in the middle east in theory everything is in reality most things are not',\n",
       " 'qatar peninsula has the decease penalization for homo natural process but at the same metre there are no register casing of slaying for it which moderately lots exemplify the state of matter of law of nature in the mediate due east in possibility everything is banned in realism most thing are not',\n",
       " 'qatar has the is penalty middle in activity but at the same time there are no recorded cases of executions for it which pretty much for the state of law in the exemplifies east in theory everything death prohibited homosexual reality most things are not',\n",
       " 'qatar has the death penalty for homosexual activity but at the same time there are no recorded cases of executions for it which pretty much exemplifies the state of law in the middle east theory everything is prohibited in reality most things are not',\n",
       " 'qatar has the death penalty for homosexual activity but at the same time there are no recorded cases of exemplifies which it for pretty much executions the state of law in the middle east in in everything is things theory reality most prohibited are not',\n",
       " 'qatar has the death clip penalty for homosexual activity but at the same time there hold hypothesis are no recorded cases of executions for it which pretty much exemplifies the state of law nix in the middle east in theory everything is prohibited in reality most things are not',\n",
       " 'qatar has the death penalty for homosexual activity but at same time there are no recorded cases executions for which pretty much exemplifies the state of law in the middle in theory everything is prohibited in reality most things are not',\n",
       " 'qatar has the death penalty for homosexual activity but at the same time there are no recorded cases of executions for it which pretty much exemplifies the state of law in the middle east in theory everything is prohibited in reality most things are not',\n",
       " 'dm to paypal me faggots cash fag paypig cash master cash slave',\n",
       " 'decimeter to paypal me poove johnny cash wear upon paypig johnny cash master copy johnny cash striver',\n",
       " 'dm to paypal fag faggots cash me paypig cash master cash slave',\n",
       " 'dm to paypal me immediate payment faggots cash fag paypig cash master cash slave',\n",
       " 'decimetre to paypal me queer hard cash travail paypig hard cash master copy hard cash buckle down',\n",
       " 'dm to me paypal faggots cash fag paypig cash master cash slave',\n",
       " 'dm to paypal me faggots cash fag paypig cash master cash maine slave',\n",
       " 'dm to paypal me faggots fag paypig cash master',\n",
       " 'dm to paypal me faggots cash fag cash cash master paypig slave',\n",
       " 'immediate payment dm to paypal me faggots cash fag paypig cash master cash slave',\n",
       " 'dm to paypal me faggots cash fag paypig cash master cash slave',\n",
       " 'adulting non so ghetto i did not sign up for this',\n",
       " 'adulting so ghetto i did not signed up for this',\n",
       " 'thus adulting so ghetto i did not sign up for this',\n",
       " 'adulting so ghetto i did not sign up for this',\n",
       " 'adulting so ghetto i did for sign up not this',\n",
       " 'adulting so ghetto i not sign up this',\n",
       " 'adulting so ghetto i did this sign up for not',\n",
       " 'adulting so ghetto i did not sign up for this',\n",
       " 'adulting so ghetto i did not signed up for this',\n",
       " 'adulting so ghetto i did not mansion up for this',\n",
       " 'adulting so ghetto i did not sign up for this ',\n",
       " 'sorry about the nigger spade comment but even blacks do not like niggers',\n",
       " 'disconsolate about the coon remark but level black do not comparable nigger',\n",
       " 'sorry about the nigger comment but even blacks fare do not like niggers',\n",
       " 'sorry about blacks nigger comment but even the do not like niggers',\n",
       " 'sorry even the nigger comment but about blacks do not like niggers',\n",
       " 'sorry about nigger comment but even blacks do not like niggers',\n",
       " 'disconsolate about the jigaboo gossip but fifty fifty shirley temple do not corresponding jigaboo',\n",
       " 'sorry about the nigger comment but even blacks do not like niggers',\n",
       " 'sorry about the nigger comment but even blacks do not like',\n",
       " 'sad about the coon remark but flush joseph black do not the like spade',\n",
       " 'sorry about the nigger comment but even blacks do not like niggers',\n",
       " 'it officially spooky szn bitch',\n",
       " 'it formally nervous szn crab',\n",
       " 'it information technology officially spooky szn bitch',\n",
       " 'it formally officially spooky szn bitch',\n",
       " 'it formally officially spooky szn bitch',\n",
       " 'it officially bitch szn spooky',\n",
       " 'it bitch spooky szn officially',\n",
       " 'it formally flighty szn kick',\n",
       " 'it formally nervous szn holler',\n",
       " 'it officially spooky szn bitch',\n",
       " 'it officially spooky szn bitch',\n",
       " 'atomic number only percentage nigra but judaic polish should be bring into story here give thanks hebrew',\n",
       " 'atomic number only pct spade but judaic cultivation should be bring into report here thank israelite',\n",
       " 'nd only percent nigger jewish culture should be taken into account here thanks jews',\n",
       " 'nd only simply percent nigger but jewish culture should be taken into account here thanks jews',\n",
       " 'nd only percent nigger jewish culture should be taken into account here thanks jews',\n",
       " 'nd only percent nigger but judaic jewish culture should be taken into account here thanks jews',\n",
       " 'nd only percent nigger but jewish culture jews be taken into account here thanks should',\n",
       " 'account only percent nigger but jewish culture should be taken into nd here thanks jews',\n",
       " 'nd only percent but jewish should be taken into account here thanks jews',\n",
       " 'north dakota only pct spade but judaic polish should be acquire into answer for here thank hebrew',\n",
       " 'nd only percent nigger but jewish culture should be taken into account here thanks jews',\n",
       " 'this nigger calling me juicy lips complete over the phone wtf',\n",
       " 'this spade ring me blue sassing over the call up wtf',\n",
       " 'this nigger me calling juicy lips over the phone wtf',\n",
       " 'this nigger wtf me juicy lips over the phone calling',\n",
       " 'this nigger calling me juicy lips over the phone wtf',\n",
       " 'this calling juicy lips over the phone wtf',\n",
       " 'this nigga foretell me gamey rim over the ring wtf',\n",
       " 'this nigger calling lips juicy me over the phone wtf',\n",
       " 'this nigger calling me juicy lips over the phone',\n",
       " 'this nigger calling me juicy lips over the phone red hot wtf',\n",
       " 'this nigger calling me juicy lips over the phone wtf',\n",
       " 'i know is somewhat hypocritical but the is the deal with guys and slightly too small white tshirts',\n",
       " 'i know this is somewhat hypocritical but and guys fuck is the deal with queer the what slightly too small white tshirts',\n",
       " 'i know this the somewhat hypocritical but slightly the fuck is is deal with queer guys and what too small white tshirts',\n",
       " 'i know this is somewhat hypocritical but what the fuck is the with queer guys slightly too white tshirts',\n",
       " 'i know this is somewhat hypocritical but what the fuck is the deal peck with queer guys and slightly bonk too small white tshirts',\n",
       " 'i do it this is reasonably hypocritical but what the have it away is the bargain with poof guy cable and more or less too humble blanched tshirts',\n",
       " 'i have it off this is fairly hypocritical but what the know is the care with poove laugh at and slimly too minuscule e b white tshirts',\n",
       " 'i know this small somewhat hypocritical but what the fuck white the deal with queer guys and slightly too is is tshirts',\n",
       " 'i know this is somewhat hypocritical but what live the fuck is the deal with queer guys and more or less slightly too small white tshirts',\n",
       " 'i have a go at it this is pretty hypocritical but what the love is the great deal with funny rib and somewhat too humble edward d white tshirts',\n",
       " 'i know this is somewhat hypocritical but what the fuck is the deal with queer guys and slightly too small white tshirts',\n",
       " 'wore a nipsey blue today suit only the niggers in the work place feel the energy',\n",
       " 'wore a nipsey blue suit today only the niggers find in the work place feel the energy',\n",
       " 'wore a nipsey blue angstrom unit suit today only the niggers in the work place feel the energy',\n",
       " 'wore a nipsey blue suit today only the niggers in the place feel the energy',\n",
       " 'cultivate wore a nipsey blue suit today only the niggers in the work place feel the energy',\n",
       " 'wore a work blue suit today only the niggers in the nipsey place feel the energy',\n",
       " 'bear a nipsey blueish accommodate nowadays only the nigra in the function lieu tone the vigor',\n",
       " 'wear off a nipsey blueish suit of clothes now only the coon in the act rank experience the doe',\n",
       " 'place a nipsey blue suit today only the niggers in the work wore feel the energy',\n",
       " 'wore a nipsey blue suit today only the niggers in the work place feel the energy',\n",
       " 'wore a nipsey blue suit today only the niggers in the work place feel the energy ',\n",
       " 'nrc a distinction between refugee and there migrant a refugee is a victim of persecution illegal migrants are new victims of persecution illegal user identify the illegal migrants citizenship act wd give refugees a not life my request to will add ahmadis',\n",
       " 'there between refugee and illegal migrant a refugee is a victim of persecution illegal migrants not victims of persecution nrc will the illegal migrants citizenship act wd give refugees a new life my request to user add ahmadis',\n",
       " 'there a distinction between refugee and illegal migrant refugee is a victim of persecution illegal migrants are not victims of persecution nrc will identify the illegal migrants citizenship act wd give refugees a new life my to user add ahmadis',\n",
       " 'there a distinction between refugee and migrant a refugee is a victim of persecution illegal migrants are not victims of nrc will identify the illegal migrants citizenship act wd give refugees new life my request to user add',\n",
       " 'there a distinction between refugee and illegal nuclear regulatory commission migrant a refugee is a victim testament of persecution illegal migrants are not victims of persecution nrc will identify the illegal migrants citizenship in that respect act wd give refugees a new life my request to user angstrom unit add ahmadis',\n",
       " 'there a note between refugee and illegal migratory a refugee is a dupe of persecution illegal migrator are not victim of persecution nuclear regulatory commission will describe the illegal migrator citizenship bit wd contribute refugee a modern aliveness my asking to exploiter minimal brain damage ahmadis',\n",
       " 'there a distinction add refugee a illegal migrant and refugee is a victim of nrc illegal migrants are persecution victims of persecution not will identify the illegal migrants citizenship act wd give refugees a new life my request to user between ahmadis',\n",
       " 'there a distinction a refugee not illegal migrants between refugee is a victim of persecution illegal migrants are and victims of persecution nrc will identify the illegal migrant citizenship act wd give refugees a to life my request new user add ahmadis',\n",
       " 'there a note between refugee and illegal migratory a refugee is a dupe of persecution illegal migratory are not dupe of persecution nuclear regulatory commission will distinguish the illegal migratory citizenship behave wd feed refugee a unexampled living my quest to drug user attention deficit hyperactivity disorder ahmadis',\n",
       " 'migratory there a distinction between refugee and illegal migrant a refugee is a quest victim of persecution illegal migrants are dupe not victims of add up persecution nrc will identify the illegal migrants citizenship act wd give refugees a new life my request to user add ahmadis',\n",
       " 'there a distinction between refugee and illegal migrant a refugee is a victim of persecution illegal migrants are not victims of persecution nrc will identify the illegal migrants citizenship act wd give refugees a new life my request to user add ahmadis',\n",
       " 'user user a camel jockey midde exploiter east watering station',\n",
       " 'user user a camel jockey midde chicane east watering station',\n",
       " 'user user a camel jockey midde east exploiter watering station',\n",
       " 'a user user camel jockey midde east watering station',\n",
       " 'exploiter exploiter a camel cheat midde eastward water place',\n",
       " 'drug user drug user a camel chouse midde e tearing post',\n",
       " 'exploiter exploiter a camel shaft midde eastern united states tearing place',\n",
       " 'user user a camel east watering station',\n",
       " 'user user a camel jockey midde east watering station',\n",
       " 'user user a camel jockey midde station watering east',\n",
       " 'user user a camel jockey midde east watering station',\n",
       " 'dwts they all take care just in ashen',\n",
       " 'dwts they all bet skilful in theodore harold white',\n",
       " 'completely dwts they all look good in white',\n",
       " 'dwts they all look beneficial good in white',\n",
       " 'dwts they all facial expression commodity in ashen',\n",
       " 'dwts they good look all in white',\n",
       " 'dwts they all look good in white',\n",
       " 'dwts they all good in white',\n",
       " 'in they all look good dwts white',\n",
       " 'dwts they all look good in white',\n",
       " 'dwts they all look good in white',\n",
       " 'create destabilizing list a violence',\n",
       " 'produce a listing destabilizing fierceness',\n",
       " 'create a destabilize list destabilizing violence',\n",
       " 'create list destabilizing violence',\n",
       " 'create make a list destabilizing violence',\n",
       " 'make a name destabilise violence',\n",
       " 'create a destabilize list destabilizing violence',\n",
       " 'make a heel destabilise violence',\n",
       " 'create a list violence destabilizing',\n",
       " 'a create list destabilizing violence',\n",
       " 'create a list destabilizing violence',\n",
       " 'user sup faggot suck my thru inch cock two my old navy jeans',\n",
       " 'drug user swallow faggot up take up my two edge stopcock thru my quondam us navy denim',\n",
       " 'user sup faggot suck my two cock thru old navy',\n",
       " 'user suck faggot sup my two inch cock thru my old navy jeans',\n",
       " 'user sup faggot suck my navy inch cock thru my old two jeans',\n",
       " 'substance abuser swallow poove take up my two edge prance thru my erstwhile navy blue jean',\n",
       " 'user sup faggot suck my navy blue two inch cock thru my old navy jeans',\n",
       " 'user sup faggot suck my two inch cock thru my old navy blue navy jeans',\n",
       " 'user sup faggot suck my two inch cock thru my old navy jeans',\n",
       " 'user sup faggot suck my two thru my navy jeans',\n",
       " 'user sup faggot suck my two inch cock thru my old navy jeans',\n",
       " 'i hate fucking niggers',\n",
       " 'i hate niggers fucking',\n",
       " 'i hatred fucking coon',\n",
       " 'i detest hate fucking niggers',\n",
       " 'niggers hate fucking i',\n",
       " 'i hate fucking niggers',\n",
       " 'i hatred bloody niggers',\n",
       " 'i hate spade fucking niggers',\n",
       " 'i hate fucking niggers',\n",
       " 'i fucking hate niggers',\n",
       " 'i hate fucking niggers',\n",
       " 'you all niggers are cancers',\n",
       " 'all niggers are',\n",
       " 'you all nigger are cancer the crab',\n",
       " 'you live all niggers are cancers',\n",
       " 'you all niggers are cancers',\n",
       " 'niggers all you are cancers',\n",
       " 'all you niggers are cancers',\n",
       " 'you all completely niggers are cancers',\n",
       " 'you all cancers are niggers',\n",
       " 'you all nigra are cancer the crab',\n",
       " 'you all niggers are cancers',\n",
       " 'bitches worrying about me cough cough user dumb hoe',\n",
       " 'bitches always worrying speechless about me cough cough user dumb hoe',\n",
       " 'always worrying me cough cough user dumb hoe',\n",
       " 'bitches always worrying user me cough cough about dumb hoe',\n",
       " 'bitches always worrying me cough user dumb hoe',\n",
       " 'bitches always worrying user me cough cough about dumb hoe',\n",
       " 'kick eer worrisome about me coughing coughing drug user silent hoe',\n",
       " 'kick invariably badgering about me coughing coughing drug user speechless hoe',\n",
       " 'gripe eer worrisome about me coughing coughing substance abuser slow hoe',\n",
       " 'bitches always hoe about me cough cough user dumb worrying',\n",
       " 'bitches always worrying about me cough cough user dumb hoe',\n",
       " 'user happy exploiter bday slut whatchu on for the day',\n",
       " 'user happy bday well chosen slut whatchu on for the day',\n",
       " 'substance abuser felicitous bday hussy whatchu on for the clarence day',\n",
       " 'happy bday slut whatchu on the day',\n",
       " 'drug user felicitous bday trollop whatchu on for the hour interval',\n",
       " 'user happy bday day whatchu on for the slut',\n",
       " 'user happy day slut whatchu on for the bday',\n",
       " 'drug user glad bday strumpet whatchu on for the twenty four hour period',\n",
       " 'user happy slut on for the day',\n",
       " 'user happy bday slut clarence shepard day jr whatchu on for the day',\n",
       " 'user happy bday slut whatchu on for the day',\n",
       " 'record hop blab out no hebrew yes but i look they await for a hebrew to sport his division then get along to termination we can not determine a hebrew that will meet the theatrical role hunky dory allow united states become a guck at minimal pay',\n",
       " 'hop sing no jew yes but i bet they looked beget for angstrom unit a jew to play region his part then came to conclusion we can not find a jew that will fit the role lower limit okay let us get a gook at minimum wage',\n",
       " 'hop sing no jew but i bet they looked for a jew to play his part then came to conclusion we can not find a jew that will fit the role okay let us get a gook at minimum wage',\n",
       " 'hops spill the beans no israelite yes but i stakes they count for a israelite to toy his voice then descend to close we can not chance a israelite that will fit out the voice sanction allow uranium beat a slime at lower limit earnings',\n",
       " 'hop sing no peach jew yes but i bet they looked for a jew to hebrew play his part then came to conclusion we can region not find a jew that will fit the pay role okay let us get a gook at minimum wage',\n",
       " 'hop sing no jew yes bet i but they looked for a jew minimum play his part then conclusion to came will can not find a jew that we fit the role okay let us get a gook at to wage',\n",
       " 'hop looked no jew yes but i bet they sing for a jew to play his part then came to conclusion we can jew find a not that will fit the role okay us let a get gook at minimum wage',\n",
       " 'hop sing no jew yes but i bet they looked for a jew to play his part then came to we can not find jew that will fit the role okay let us get a gook at minimum wage',\n",
       " 'hop sing no jew yes but i bet they looked for a jew to play his part convulsion then came to conclusion we can not find simply a jew permit that will fit the role okay let us detect get a gook at minimum wage',\n",
       " 'hop sing no jew yes but i looked they can get a a to play his part then came to conclusion we bet not find a jew that will fit the role okay let us for jew gook at minimum wage',\n",
       " 'hop sing no jew yes but i bet they looked for a jew to play his part then came to conclusion we can not find a jew that will fit the role okay let us get a gook at minimum wage',\n",
       " 'this up to now yid endicted yet not arrested and',\n",
       " 'this yet endicted yid not arrested and',\n",
       " 'this hymie endicted up to now not stop and',\n",
       " 'this yid endicted yet not arrested and',\n",
       " 'this kike endicted so far not apprehend and',\n",
       " 'this yid endicted not arrested and',\n",
       " 'this yid endicted and not arrested yet',\n",
       " 'this yid endicted yet not arrested and',\n",
       " 'this yid endicted non yet not arrested and',\n",
       " 'this yid non endicted yet not arrested and',\n",
       " 'this yid endicted yet not arrested and',\n",
       " 'ima bitch include i will admit it',\n",
       " 'ima bitch i will admit it',\n",
       " 'ima bitch i will admit it',\n",
       " 'ima bitch it will admit i',\n",
       " 'ima bitch i will it',\n",
       " 'ima bitch i will admit it',\n",
       " 'ima gripe i will acknowledge it',\n",
       " 'ima grouse i will let in it',\n",
       " 'ima bitch i will testament admit it',\n",
       " 'ima bellyache i will allow it',\n",
       " 'ima bitch i will admit it',\n",
       " 'rafiullah shaheed a young man from levi force lapplander was shot dead by unknown men in muslim bagh a levies officer was killed in a bomb blast in lashkar gah moslem on sunday while militants were also killed on the same day three youth people including mullahaniyev were killed in chaman',\n",
       " 'shaheed a young from levi force was shot dead by unknown men in muslim bagh levies officer was killed in bomb in lashkar gah on sunday while militants were also killed on the same day three people including mullahaniyev were killed in chaman',\n",
       " 'rafiullah shaheed a young man from levi force was shot dead by unknown men in muslim bagh a levies officer was in a bomb blast in lashkar gah on sunday militants were also killed on the same day three people including mullahaniyev were killed chaman',\n",
       " 'rafiullah shaheed a young man from levi force was shot dead by unknown men in stranger muslim bagh a levies officer was killed in a bomb blast in lashkar include gah on sunday while militants were also killed on the same youth day angstrom unit three people including mullahaniyev were killed in chaman',\n",
       " 'rafiullah shaheed killed levies man from levi force was shot dead by unknown men in muslim bagh a young officer was people in a bomb killed in lashkar gah on sunday while militants were also blast on the same day three a including mullahaniyev were killed in chaman',\n",
       " 'rafiullah shaheed a young man from levi force was shot dead by unknown men in muslim bagh a levies officer was killed in bomb in lashkar gah on sunday while militants were also killed on the same people mullahaniyev were killed in chaman',\n",
       " 'in shaheed shot young man from levi force was a dead by sunday men in muslim bagh a levies officer was killed in a bomb blast in lashkar gah on unknown while militants were also killed on the same rafiullah three people including mullahaniyev were killed day chaman',\n",
       " 'rafiullah unknown a young man from levi force was shot were by shaheed men lashkar muslim bagh a levies officer was killed in a bomb blast in in gah on sunday while militants dead also were on the same day three people including mullahaniyev killed killed in chaman',\n",
       " 'rafiullah shaheed a young man from levi force was shot dead by unknown citizenry men in muslim bagh a levies officer was killed past in a bomb blast in lashkar gah ships officer on along sunday while militants were also killed on the same day three people including mullahaniyev were killed in chaman',\n",
       " 'rafiullah shaheed a immature valet from saint matthew the apostle force out was sprout deadened by obscure military personnel in moslem bagh a impose ships officer was vote out in a turkey bam in lashkar gah on dominicus while activist were too vote out on the same twenty four hour period the great unwashed let in mullahaniyev were vote out in chaman',\n",
       " 'rafiullah shaheed a young man from levi force was shot dead by unknown men in muslim bagh a levies officer was killed in a bomb blast in lashkar gah on sunday while militants were also killed on the same day three people including mullahaniyev were killed in chaman',\n",
       " 'do not bed if i just look turkey or a very unearthly sequence of battle of atlanta either elbow room picture was very wellspring coiffe relevant capital of arizona was bright',\n",
       " 'do not know if i just seen joker or a really weird episode of atlanta either way film was very well done capital of georgia relevant phoenix was live brilliant',\n",
       " 'not know if i just seen joker or a really weird episode of atlanta either way film was well done relevant phoenix was brilliant',\n",
       " 'do not know if i just seen joker or a really weird film of atlanta either way relevant was very well done episode phoenix was brilliant',\n",
       " 'do not know if i just seen joker or a really weird episode of atlanta either way film very well done relevant phoenix was brilliant',\n",
       " 'do not acknowledge if i just come across turkey or a in truth wyrd installment of capital of georgia either manner picture was very good execute relevant capital of arizona was splendid',\n",
       " 'do not seen if a just know joker or i really weird episode of atlanta either way film was very well done relevant phoenix was brilliant',\n",
       " 'do not know if i just seen joker or a really operating theater weird episode of atlanta either equitable way film was very well done relevant phoenix was brilliant',\n",
       " 'do not brilliant if i just seen joker or a really weird episode of atlanta either way film very was well done relevant phoenix was know',\n",
       " 'do not know if i just seen joker or a really weird unearthly episode of atlanta either way film was very well done angstrom unit relevant phoenix was brilliant',\n",
       " ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_sr_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "\n",
    "def bert_tokenize(train_set, dev_set, test_set, max_length):\n",
    "    \n",
    "    train = tokenizer(train_set, max_length=max_length, truncation=True, padding='max_length', return_tensors='tf')\n",
    "    dev = tokenizer(dev_set, max_length=max_length, truncation=True, padding='max_length', return_tensors='tf')\n",
    "    test = tokenizer(test_set, max_length=max_length, truncation=True, padding='max_length', return_tensors='tf')\n",
    "    \n",
    "    return train, dev, test\n",
    "\n",
    "X_train_orig, X_dev_orig, X_test_orig = bert_tokenize(X_train_text, X_dev_text, X_test_text, max_length)\n",
    "\n",
    "X_train_aug_sr, X_dev_aug_sr, X_test_aug_sr = bert_tokenize(aug_sr_text, X_dev_text, X_test_text, max_length)\n",
    "\n",
    "X_train_aug_ri, X_dev_aug_ri, X_test_aug_ri = bert_tokenize(aug_ri_text, X_dev_text, X_test_text, max_length)\n",
    "\n",
    "X_train_aug_rs, X_dev_aug_rs, X_test_aug_rs = bert_tokenize(aug_rs_text, X_dev_text, X_test_text, max_length)\n",
    "\n",
    "X_train_aug_rd, X_dev_aug_rd, X_test_aug_rd = bert_tokenize(aug_rd_text, X_dev_text, X_test_text, max_length)\n",
    "\n",
    "X_train_all_1, X_dev_all_1, X_test_all_1 = bert_tokenize(aug_all_1_text, X_dev_text, X_test_text, max_length)\n",
    "\n",
    "X_train_all_5, X_dev_all_5, X_test_all_5 = bert_tokenize(aug_all_5_text, X_dev_text, X_test_text, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.save_pretrained(\"./Tokenizer_ALL_EDA_BERT_base_uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_orig.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(169213, 128), dtype=int32, numpy=\n",
       "array([[  101,  1045,  2123, ...,     0,     0,     0],\n",
       "       [  101,  2123,  2102, ...,     0,     0,     0],\n",
       "       [  101,  1045,  2123, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101,  1996,  3644, ...,     0,     0,     0],\n",
       "       [  101,  1996, 18414, ...,     0,     0,     0],\n",
       "       [  101,  1996,  3644, ...,     0,     0,     0]])>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_aug_sr.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 170,
     "status": "ok",
     "timestamp": 1646722395812,
     "user": {
      "displayName": "Evan Chan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgGKtlehGoFssPGs4v0Yns-qfVPjW1FuloVAn-GMw=s64",
      "userId": "16754265128573798261"
     },
     "user_tz": 360
    },
    "id": "-OSiJNKUTYB5",
    "outputId": "3d0dbbf7-ab1f-4aec-84b4-77335d135479"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(169213, 128), dtype=int32, numpy=\n",
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_aug_ri.token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516,
     "referenced_widgets": [
      "12bf3b8f0bab4586847a07e8c438dafa",
      "8039ef2f59ee482781f325f5a91b93e8",
      "e67ba177d3744ea4bcc652774cd13abd",
      "43eb8a62436a4c149e25f98f49a7c68d",
      "3723a8c5f8eb47efa71e50a5ef924123",
      "9e9b4804607046678d3d132528486b0b",
      "e567a77fb29d47c98fe19c440b2d31e8",
      "5d2efc2c1758475c9d1cd51cdc7edf0a",
      "7942944ae43141c0aa83ec76fc0deafa",
      "c26d9b9e910a43febfd588927c3e7756",
      "1b1a083fc8e643a78f055105960b523e",
      "4656eb21ff2245b7be51f82c1e2c7e6b",
      "ab8f5f020c76459aad12d4cab6a36a72",
      "2af599ea5c3d4874aff863303c5b5703",
      "c64b940afcc94ee3b2b88c582b54d97f",
      "8a798f3e801d47df8771840991f7cfa3",
      "263134a33eaa4345926dd786256c08eb",
      "b473e96d36604d71ae5d3d8e69cc01c3",
      "3dfc3ee445f2403a8ed7b4e6f5a8b3d3",
      "e35113cc44f34a1fbb22f109f49f7bdd",
      "c4792815ec0d4591bcaea2e7a0539e6b",
      "047bcb706b304be09200793be7524708",
      "02d7e8c53bf94bf48c06cea9abbd9aba",
      "d57bc505733b4c738cf84db24feba360",
      "bf38f0560d3e493ca2b97fd974d00462",
      "8a60b779e7d749d284b791b9e5126a62",
      "2e51f1cad2b84893b537beb7e4ebf67c",
      "7d3ff64481b64406aadb34439d2ad02b",
      "feb0fac998144444bfd9373ce537bbc3",
      "70f68856bdb64c30aa57e6046468b3d6",
      "0c234fac6d10482089dacd2c03a5bbde",
      "b8b44bed109443fd9bba63a7039f3c93",
      "c3df12951362451685bea68b7091c69f",
      "7d77d8f8c129400d9f61b60da03891b3",
      "971c955528914b05afe5d12e9c626cf1",
      "98efd1135c4f438c8b8dd5c130cc667c",
      "9aca8f0c4013472c8514d2d57f9ea3e8",
      "807b39071d354a1ab7af51a05f1ef3b1",
      "acd4639a73904507b7884adc96512dc5",
      "b575b556bb9445ceb5c662b9d224e171",
      "f9a30d0d10744d14a47b500251c1973c",
      "d2803d4a8a794a83a2ebcf61ce3097ff",
      "5e02a09e9fe24718a8f1f4167c42d099",
      "fc6b1e6a0e254b82b6c7277f10a5d542",
      "071ca09b48bf4d9989852d66f8d57642",
      "15db3b4b336b41cdb973852bb3fef72f",
      "8aa487d37b284b2ba88903923a0086d9",
      "b3ba42a7365d418f9cbf7f68ede3b65a",
      "c7e4cea875eb41c8950f97350c237730",
      "e88a61936e0949c490366d0619869b6c",
      "ee76f1517ed64f4fa49d998d2448f9c2",
      "45b7612393254fe19709bdae7d7bbbe6",
      "d12cbe52662546d1be82cfd4536dfdfc",
      "ee89c9881c75441fbc9ec827d623134e",
      "d7abb004da9d42b787717cfdb3657f4e"
     ]
    },
    "executionInfo": {
     "elapsed": 19947,
     "status": "error",
     "timestamp": 1646722519680,
     "user": {
      "displayName": "Evan Chan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgGKtlehGoFssPGs4v0Yns-qfVPjW1FuloVAn-GMw=s64",
      "userId": "16754265128573798261"
     },
     "user_tz": 360
    },
    "id": "DCaUJQymTWeJ",
    "outputId": "990e4e3f-4544-4c6c-ab2c-b365d72242af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(169213, 128), dtype=int32, numpy=\n",
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]])>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_all_1.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(169213, 128), dtype=int32, numpy=\n",
       "array([[ 101, 1045, 2123, ...,    0,    0,    0],\n",
       "       [ 101, 1045, 5028, ...,    0,    0,    0],\n",
       "       [ 101, 2068, 1045, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 101, 1996, 7486, ...,    0,    0,    0],\n",
       "       [ 101, 3644, 7069, ...,    0,    0,    0],\n",
       "       [ 101, 1996, 3644, ...,    0,    0,    0]])>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_all_5.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def balanced_recall(y_true, y_pred):\n",
    "    \"\"\"This function calculates the balanced recall metric\n",
    "    recall = TP / (TP + FN)\n",
    "    \"\"\"\n",
    "    recall_by_class = 0\n",
    "    # iterate over each predicted class to get class-specific metric\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        y_pred_class = y_pred[:, i]\n",
    "        y_true_class = y_true[:, i]\n",
    "        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true_class, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        recall_by_class = recall_by_class + recall\n",
    "    return recall_by_class / y_pred.shape[1]\n",
    "\n",
    "def balanced_precision(y_true, y_pred):\n",
    "    \"\"\"This function calculates the balanced precision metric\n",
    "    precision = TP / (TP + FP)\n",
    "    \"\"\"\n",
    "    precision_by_class = 0\n",
    "    # iterate over each predicted class to get class-specific metric\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        y_pred_class = y_pred[:, i]\n",
    "        y_true_class = y_true[:, i]\n",
    "        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred_class, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        precision_by_class = precision_by_class + precision\n",
    "    # return average balanced metric for each class\n",
    "    return precision_by_class / y_pred.shape[1]\n",
    "\n",
    "def balanced_f1_score(y_true, y_pred):\n",
    "    \"\"\"This function calculates the F1 score metric\"\"\"\n",
    "    precision = balanced_precision(y_true, y_pred)\n",
    "    recall = balanced_recall(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classification_model(bert_model, hidden_size = 5, \n",
    "                                train_layers = -1, \n",
    "                                optimizer=tf.keras.optimizers.Adam()):\n",
    "    \"\"\"\n",
    "    Build a simple classification model with BERT. Let's keep it simple and don't add dropout, layer norms, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids_layer')\n",
    "    token_type_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='token_type_ids_layer')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask_layer')\n",
    "\n",
    "    bert_inputs = {'input_ids': input_ids,\n",
    "                  'token_type_ids': token_type_ids,\n",
    "                  'attention_mask': attention_mask}\n",
    "\n",
    "\n",
    "    #restrict training to the train_layers outer transformer layers\n",
    "    if not train_layers == -1:\n",
    "\n",
    "            retrain_layers = []\n",
    "\n",
    "            for retrain_layer_number in range(train_layers):\n",
    "\n",
    "                layer_code = '_' + str(11 - retrain_layer_number)\n",
    "                retrain_layers.append(layer_code)\n",
    "\n",
    "            for w in bert_model.weights:\n",
    "                if not any([x in w.name for x in retrain_layers]):\n",
    "                    w._trainable = False\n",
    "\n",
    "\n",
    "    bert_out = bert_model(bert_inputs)\n",
    "    \n",
    "    net = bert_out[0]\n",
    "    \n",
    "    classification_token = tf.keras.layers.Lambda(lambda x: x[:,0,:], name='get_first_vector')(net)\n",
    "    \n",
    "    dropout1 = tf.keras.layers.Dropout(0.4, name=\"dropout1\")(classification_token)\n",
    "    \n",
    "    hidden = tf.keras.layers.Dense(hidden_size, name='hidden_layer')(dropout1)\n",
    "    \n",
    "    dropout2 = tf.keras.layers.Dropout(0.4, name=\"dropout2\")(hidden)\n",
    "\n",
    "    classification = tf.keras.layers.Dense(3, activation='sigmoid',name='classification_layer')(dropout2)\n",
    "\n",
    "    classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], \n",
    "                                          outputs=[classification])\n",
    "    \n",
    "    METRICS = [tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"), \n",
    "               balanced_recall, \n",
    "               balanced_precision, \n",
    "               balanced_f1_score,\n",
    "               tf.keras.metrics.AUC(curve='ROC', name=\"auc_roc\")]\n",
    "    \n",
    "    \n",
    "    classification_model.compile(optimizer=optimizer,\n",
    "                            loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                            metrics= METRICS)\n",
    "\n",
    "\n",
    "    return classification_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     classification_model.compile(optimizer=optimizer,\n",
    "#                             loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "#                             metrics=tf.keras.metrics.CategoricalAccuracy('accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_BERT(x_train, x_dev, x_test, y_train, y_dev, y_test, name, learning_rate = 5e-05, \n",
    "                   epsilon=1e-08, train_layers = -1, epochs = 10, batch_size = 16):\n",
    "    ''' Fine tunes BERT base uncased with given data, allows your to set some hyperparameters\n",
    "        returns test set accuracy, f1 score, and AUC_ROC score\n",
    "    '''\n",
    "    try:\n",
    "        del classification_model\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        del bert_model\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # early stopping callback\n",
    "    \n",
    "    earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy', \n",
    "                                                      patience = 4,\n",
    "                                                      restore_best_weights = True)\n",
    "    \n",
    "    # Create a callback that saves the model's weights\n",
    "    \n",
    "    path_name = './Saved_Models/EDA_b_10aug/' + name + '/' + name\n",
    "\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=path_name, \n",
    "                                                     save_weights_only=True,\n",
    "                                                     verbose=1,\n",
    "                                                     monitor='val_accuracy',\n",
    "                                                     save_best_only=True)\n",
    "    \n",
    "    # create classification model\n",
    "    classification_model = create_classification_model(bert_model, \n",
    "                                                       optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon),\n",
    "                                                       train_layers=train_layers)    \n",
    "    \n",
    "    model_fit = classification_model.fit([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask],\n",
    "                         y_train,\n",
    "                         validation_data=([x_dev.input_ids, x_dev.token_type_ids, x_dev.attention_mask],\n",
    "                         y_dev),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks = [earlystop_callback, cp_callback])\n",
    "    \n",
    "    y_preds_array = classification_model.predict([x_test.input_ids, x_test.token_type_ids, x_test.attention_mask])\n",
    "\n",
    "    # convert to predicted one-hot encoding\n",
    "\n",
    "    from keras.utils.np_utils import to_categorical\n",
    "    y_preds = to_categorical(np.argmax(y_preds_array, 1), dtype = \"int64\")\n",
    "\n",
    "    # convert back to labels\n",
    "\n",
    "    y_test_cat = np.argmax(y_test, axis=1)\n",
    "    y_preds_cat = np.argmax(y_preds, axis=1)\n",
    "    \n",
    "    # calculate metrics\n",
    "    Accuracy = accuracy_score(y_test_cat, y_preds_cat)\n",
    "\n",
    "    Macro_F1 = f1_score(y_test_cat, y_preds_cat, average='macro')\n",
    "\n",
    "    ROC_AUC = roc_auc_score(y_test, y_preds, multi_class='ovo',average='macro')\n",
    "    \n",
    "    metrics_history = model_fit.history\n",
    "    \n",
    "    return Accuracy, Macro_F1, ROC_AUC, metrics_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "241/241 [==============================] - 59s 201ms/step - loss: 1.3042 - accuracy: 0.4132 - balanced_recall: 0.5771 - balanced_precision: 0.3823 - balanced_f1_score: 0.4596 - auc_roc: 0.5877 - val_loss: 0.9417 - val_accuracy: 0.5367 - val_balanced_recall: 0.7150 - val_balanced_precision: 0.4686 - val_balanced_f1_score: 0.5649 - val_auc_roc: 0.7279\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.53666, saving model to ./Saved_Models/EDA_b_10aug/orig_data_10aug\\orig_data_10aug\n",
      "Epoch 2/30\n",
      "241/241 [==============================] - 44s 182ms/step - loss: 1.1072 - accuracy: 0.4789 - balanced_recall: 0.6480 - balanced_precision: 0.4274 - balanced_f1_score: 0.5148 - auc_roc: 0.6620 - val_loss: 0.8825 - val_accuracy: 0.5793 - val_balanced_recall: 0.7308 - val_balanced_precision: 0.5106 - val_balanced_f1_score: 0.6002 - val_auc_roc: 0.7658\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.53666 to 0.57930, saving model to ./Saved_Models/EDA_b_10aug/orig_data_10aug\\orig_data_10aug\n",
      "Epoch 3/30\n",
      "241/241 [==============================] - 44s 182ms/step - loss: 1.0219 - accuracy: 0.5171 - balanced_recall: 0.6778 - balanced_precision: 0.4563 - balanced_f1_score: 0.5451 - auc_roc: 0.6993 - val_loss: 0.8408 - val_accuracy: 0.6162 - val_balanced_recall: 0.7621 - val_balanced_precision: 0.5264 - val_balanced_f1_score: 0.6217 - val_auc_roc: 0.7897\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.57930 to 0.61622, saving model to ./Saved_Models/EDA_b_10aug/orig_data_10aug\\orig_data_10aug\n",
      "Epoch 4/30\n",
      "241/241 [==============================] - 45s 185ms/step - loss: 0.9753 - accuracy: 0.5487 - balanced_recall: 0.7058 - balanced_precision: 0.4760 - balanced_f1_score: 0.5682 - auc_roc: 0.7271 - val_loss: 0.8209 - val_accuracy: 0.6230 - val_balanced_recall: 0.7695 - val_balanced_precision: 0.5407 - val_balanced_f1_score: 0.6341 - val_auc_roc: 0.8004\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.61622 to 0.62298, saving model to ./Saved_Models/EDA_b_10aug/orig_data_10aug\\orig_data_10aug\n",
      "Epoch 5/30\n",
      "241/241 [==============================] - 44s 181ms/step - loss: 0.9386 - accuracy: 0.5653 - balanced_recall: 0.7250 - balanced_precision: 0.4878 - balanced_f1_score: 0.5829 - auc_roc: 0.7419 - val_loss: 0.8108 - val_accuracy: 0.6355 - val_balanced_recall: 0.7707 - val_balanced_precision: 0.5403 - val_balanced_f1_score: 0.6342 - val_auc_roc: 0.8080\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.62298 to 0.63547, saving model to ./Saved_Models/EDA_b_10aug/orig_data_10aug\\orig_data_10aug\n",
      "Epoch 6/30\n",
      "241/241 [==============================] - 44s 182ms/step - loss: 0.9165 - accuracy: 0.5792 - balanced_recall: 0.7329 - balanced_precision: 0.4949 - balanced_f1_score: 0.5905 - auc_roc: 0.7536 - val_loss: 0.8014 - val_accuracy: 0.6448 - val_balanced_recall: 0.7641 - val_balanced_precision: 0.5506 - val_balanced_f1_score: 0.6391 - val_auc_roc: 0.8139\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.63547 to 0.64483, saving model to ./Saved_Models/EDA_b_10aug/orig_data_10aug\\orig_data_10aug\n",
      "Epoch 7/30\n",
      "241/241 [==============================] - 44s 183ms/step - loss: 0.9061 - accuracy: 0.5849 - balanced_recall: 0.7407 - balanced_precision: 0.4997 - balanced_f1_score: 0.5964 - auc_roc: 0.7581 - val_loss: 0.7861 - val_accuracy: 0.6412 - val_balanced_recall: 0.8056 - val_balanced_precision: 0.5586 - val_balanced_f1_score: 0.6587 - val_auc_roc: 0.8127\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.64483\n",
      "Epoch 8/30\n",
      "241/241 [==============================] - 43s 178ms/step - loss: 0.8801 - accuracy: 0.6017 - balanced_recall: 0.7555 - balanced_precision: 0.5079 - balanced_f1_score: 0.6071 - auc_roc: 0.7699 - val_loss: 0.7819 - val_accuracy: 0.6547 - val_balanced_recall: 0.7941 - val_balanced_precision: 0.5493 - val_balanced_f1_score: 0.6483 - val_auc_roc: 0.8222\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.64483 to 0.65471, saving model to ./Saved_Models/EDA_b_10aug/orig_data_10aug\\orig_data_10aug\n",
      "Epoch 9/30\n",
      "241/241 [==============================] - 45s 185ms/step - loss: 0.8666 - accuracy: 0.6070 - balanced_recall: 0.7589 - balanced_precision: 0.5154 - balanced_f1_score: 0.6135 - auc_roc: 0.7770 - val_loss: 0.7700 - val_accuracy: 0.6526 - val_balanced_recall: 0.7967 - val_balanced_precision: 0.5596 - val_balanced_f1_score: 0.6565 - val_auc_roc: 0.8240\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.65471\n",
      "Epoch 10/30\n",
      "241/241 [==============================] - 44s 181ms/step - loss: 0.8640 - accuracy: 0.6092 - balanced_recall: 0.7601 - balanced_precision: 0.5148 - balanced_f1_score: 0.6135 - auc_roc: 0.7779 - val_loss: 0.7671 - val_accuracy: 0.6641 - val_balanced_recall: 0.7815 - val_balanced_precision: 0.5703 - val_balanced_f1_score: 0.6586 - val_auc_roc: 0.8309\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.65471 to 0.66407, saving model to ./Saved_Models/EDA_b_10aug/orig_data_10aug\\orig_data_10aug\n",
      "Epoch 11/30\n",
      "241/241 [==============================] - 45s 184ms/step - loss: 0.8530 - accuracy: 0.6166 - balanced_recall: 0.7705 - balanced_precision: 0.5193 - balanced_f1_score: 0.6201 - auc_roc: 0.7835 - val_loss: 0.7581 - val_accuracy: 0.6599 - val_balanced_recall: 0.8122 - val_balanced_precision: 0.5697 - val_balanced_f1_score: 0.6688 - val_auc_roc: 0.8279\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.66407\n",
      "Epoch 12/30\n",
      "241/241 [==============================] - 44s 182ms/step - loss: 0.8424 - accuracy: 0.6255 - balanced_recall: 0.7718 - balanced_precision: 0.5259 - balanced_f1_score: 0.6251 - auc_roc: 0.7897 - val_loss: 0.7707 - val_accuracy: 0.6672 - val_balanced_recall: 0.7892 - val_balanced_precision: 0.5688 - val_balanced_f1_score: 0.6601 - val_auc_roc: 0.8303\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.66407 to 0.66719, saving model to ./Saved_Models/EDA_b_10aug/orig_data_10aug\\orig_data_10aug\n",
      "Epoch 13/30\n",
      "241/241 [==============================] - 44s 184ms/step - loss: 0.8296 - accuracy: 0.6276 - balanced_recall: 0.7717 - balanced_precision: 0.5307 - balanced_f1_score: 0.6285 - auc_roc: 0.7946 - val_loss: 0.7570 - val_accuracy: 0.6646 - val_balanced_recall: 0.7862 - val_balanced_precision: 0.5737 - val_balanced_f1_score: 0.6625 - val_auc_roc: 0.8356\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.66719\n",
      "Epoch 14/30\n",
      "241/241 [==============================] - 44s 181ms/step - loss: 0.8290 - accuracy: 0.6323 - balanced_recall: 0.7761 - balanced_precision: 0.5332 - balanced_f1_score: 0.6318 - auc_roc: 0.7961 - val_loss: 0.7546 - val_accuracy: 0.6760 - val_balanced_recall: 0.7997 - val_balanced_precision: 0.5671 - val_balanced_f1_score: 0.6626 - val_auc_roc: 0.8358\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.66719 to 0.67603, saving model to ./Saved_Models/EDA_b_10aug/orig_data_10aug\\orig_data_10aug\n",
      "Epoch 15/30\n",
      "241/241 [==============================] - 44s 183ms/step - loss: 0.8134 - accuracy: 0.6345 - balanced_recall: 0.7790 - balanced_precision: 0.5358 - balanced_f1_score: 0.6345 - auc_roc: 0.8008 - val_loss: 0.7491 - val_accuracy: 0.6724 - val_balanced_recall: 0.7952 - val_balanced_precision: 0.5722 - val_balanced_f1_score: 0.6646 - val_auc_roc: 0.8380\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.67603\n",
      "Epoch 16/30\n",
      "241/241 [==============================] - 43s 178ms/step - loss: 0.8112 - accuracy: 0.6367 - balanced_recall: 0.7826 - balanced_precision: 0.5383 - balanced_f1_score: 0.6375 - auc_roc: 0.8017 - val_loss: 0.7547 - val_accuracy: 0.6729 - val_balanced_recall: 0.8017 - val_balanced_precision: 0.5659 - val_balanced_f1_score: 0.6625 - val_auc_roc: 0.8363\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.67603\n",
      "Epoch 17/30\n",
      "241/241 [==============================] - 44s 182ms/step - loss: 0.8007 - accuracy: 0.6414 - balanced_recall: 0.7914 - balanced_precision: 0.5420 - balanced_f1_score: 0.6431 - auc_roc: 0.8072 - val_loss: 0.7372 - val_accuracy: 0.6807 - val_balanced_recall: 0.8109 - val_balanced_precision: 0.5685 - val_balanced_f1_score: 0.6675 - val_auc_roc: 0.8392\n",
      "\n",
      "Epoch 00017: val_accuracy improved from 0.67603 to 0.68071, saving model to ./Saved_Models/EDA_b_10aug/orig_data_10aug\\orig_data_10aug\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241/241 [==============================] - 44s 184ms/step - loss: 0.7998 - accuracy: 0.6445 - balanced_recall: 0.7954 - balanced_precision: 0.5444 - balanced_f1_score: 0.6460 - auc_roc: 0.8087 - val_loss: 0.7337 - val_accuracy: 0.6760 - val_balanced_recall: 0.8040 - val_balanced_precision: 0.5736 - val_balanced_f1_score: 0.6687 - val_auc_roc: 0.8423\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.68071\n",
      "Epoch 19/30\n",
      "241/241 [==============================] - 43s 179ms/step - loss: 0.7817 - accuracy: 0.6516 - balanced_recall: 0.8002 - balanced_precision: 0.5457 - balanced_f1_score: 0.6485 - auc_roc: 0.8143 - val_loss: 0.7500 - val_accuracy: 0.6760 - val_balanced_recall: 0.7994 - val_balanced_precision: 0.5711 - val_balanced_f1_score: 0.6653 - val_auc_roc: 0.8399\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.68071\n",
      "Epoch 20/30\n",
      "241/241 [==============================] - 44s 181ms/step - loss: 0.7874 - accuracy: 0.6564 - balanced_recall: 0.8008 - balanced_precision: 0.5444 - balanced_f1_score: 0.6477 - auc_roc: 0.8130 - val_loss: 0.7331 - val_accuracy: 0.6807 - val_balanced_recall: 0.8203 - val_balanced_precision: 0.5747 - val_balanced_f1_score: 0.6749 - val_auc_roc: 0.8401\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.68071\n",
      "Epoch 21/30\n",
      "241/241 [==============================] - 43s 178ms/step - loss: 0.7729 - accuracy: 0.6585 - balanced_recall: 0.8052 - balanced_precision: 0.5498 - balanced_f1_score: 0.6531 - auc_roc: 0.8169 - val_loss: 0.7306 - val_accuracy: 0.6890 - val_balanced_recall: 0.8189 - val_balanced_precision: 0.5743 - val_balanced_f1_score: 0.6742 - val_auc_roc: 0.8425\n",
      "\n",
      "Epoch 00021: val_accuracy improved from 0.68071 to 0.68903, saving model to ./Saved_Models/EDA_b_10aug/orig_data_10aug\\orig_data_10aug\n",
      "Epoch 22/30\n",
      "241/241 [==============================] - 45s 184ms/step - loss: 0.7626 - accuracy: 0.6628 - balanced_recall: 0.8077 - balanced_precision: 0.5517 - balanced_f1_score: 0.6552 - auc_roc: 0.8219 - val_loss: 0.7316 - val_accuracy: 0.6864 - val_balanced_recall: 0.8101 - val_balanced_precision: 0.5746 - val_balanced_f1_score: 0.6713 - val_auc_roc: 0.8432\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.68903\n",
      "Epoch 23/30\n",
      "241/241 [==============================] - 43s 179ms/step - loss: 0.7577 - accuracy: 0.6714 - balanced_recall: 0.8151 - balanced_precision: 0.5614 - balanced_f1_score: 0.6646 - auc_roc: 0.8271 - val_loss: 0.7326 - val_accuracy: 0.6880 - val_balanced_recall: 0.8128 - val_balanced_precision: 0.5722 - val_balanced_f1_score: 0.6707 - val_auc_roc: 0.8444\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.68903\n",
      "Epoch 24/30\n",
      "241/241 [==============================] - 43s 179ms/step - loss: 0.7568 - accuracy: 0.6715 - balanced_recall: 0.8159 - balanced_precision: 0.5599 - balanced_f1_score: 0.6637 - auc_roc: 0.8266 - val_loss: 0.7324 - val_accuracy: 0.6833 - val_balanced_recall: 0.8142 - val_balanced_precision: 0.5744 - val_balanced_f1_score: 0.6727 - val_auc_roc: 0.8440\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.68903\n",
      "Epoch 25/30\n",
      "241/241 [==============================] - 44s 182ms/step - loss: 0.7423 - accuracy: 0.6764 - balanced_recall: 0.8226 - balanced_precision: 0.5611 - balanced_f1_score: 0.6668 - auc_roc: 0.8324 - val_loss: 0.7388 - val_accuracy: 0.6854 - val_balanced_recall: 0.8166 - val_balanced_precision: 0.5710 - val_balanced_f1_score: 0.6711 - val_auc_roc: 0.8439\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.68903\n",
      "Wall time: 19min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# original data set\n",
    "Accuracy_orig, Macro_F1_orig, ROC_AUC_orig, metrics_orig = fine_tune_BERT(X_train_orig, X_dev_orig, X_test_orig, \n",
    "                                                            y_train_orig, y_dev_orig, y_test_orig, 'orig_data_10aug',\n",
    "                                                            learning_rate = 2e-05, epsilon=1e-08, \n",
    "                                                            train_layers = 1, epochs = 30, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2644/2644 [==============================] - 442s 163ms/step - loss: 1.0198 - accuracy: 0.5068 - balanced_recall: 0.6147 - balanced_precision: 0.4307 - balanced_f1_score: 0.5057 - auc_roc: 0.6584 - val_loss: 0.7889 - val_accuracy: 0.6599 - val_balanced_recall: 0.7246 - val_balanced_precision: 0.5388 - val_balanced_f1_score: 0.6177 - val_auc_roc: 0.7911\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.65991, saving model to ./Saved_Models/EDA_b_10aug/EDA_sr_10aug\\EDA_sr_10aug\n",
      "Epoch 2/30\n",
      "2644/2644 [==============================] - 425s 161ms/step - loss: 0.8770 - accuracy: 0.5949 - balanced_recall: 0.7247 - balanced_precision: 0.4860 - balanced_f1_score: 0.5812 - auc_roc: 0.7379 - val_loss: 0.7467 - val_accuracy: 0.6823 - val_balanced_recall: 0.7703 - val_balanced_precision: 0.5492 - val_balanced_f1_score: 0.6408 - val_auc_roc: 0.8154\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.65991 to 0.68227, saving model to ./Saved_Models/EDA_b_10aug/EDA_sr_10aug\\EDA_sr_10aug\n",
      "Epoch 3/30\n",
      "2644/2644 [==============================] - 425s 161ms/step - loss: 0.8146 - accuracy: 0.6319 - balanced_recall: 0.7674 - balanced_precision: 0.5071 - balanced_f1_score: 0.6101 - auc_roc: 0.7693 - val_loss: 0.7395 - val_accuracy: 0.6843 - val_balanced_recall: 0.8022 - val_balanced_precision: 0.5572 - val_balanced_f1_score: 0.6570 - val_auc_roc: 0.8093\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.68227 to 0.68435, saving model to ./Saved_Models/EDA_b_10aug/EDA_sr_10aug\\EDA_sr_10aug\n",
      "Epoch 4/30\n",
      "2644/2644 [==============================] - 423s 160ms/step - loss: 0.7686 - accuracy: 0.6604 - balanced_recall: 0.7962 - balanced_precision: 0.5187 - balanced_f1_score: 0.6276 - auc_roc: 0.7887 - val_loss: 0.7620 - val_accuracy: 0.6854 - val_balanced_recall: 0.7736 - val_balanced_precision: 0.5497 - val_balanced_f1_score: 0.6421 - val_auc_roc: 0.8165\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.68435 to 0.68539, saving model to ./Saved_Models/EDA_b_10aug/EDA_sr_10aug\\EDA_sr_10aug\n",
      "Epoch 5/30\n",
      "2644/2644 [==============================] - 422s 160ms/step - loss: 0.7261 - accuracy: 0.6835 - balanced_recall: 0.8148 - balanced_precision: 0.5313 - balanced_f1_score: 0.6427 - auc_roc: 0.8052 - val_loss: 0.7884 - val_accuracy: 0.6791 - val_balanced_recall: 0.7915 - val_balanced_precision: 0.5458 - val_balanced_f1_score: 0.6457 - val_auc_roc: 0.8150\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.68539\n",
      "Epoch 6/30\n",
      "2644/2644 [==============================] - 420s 159ms/step - loss: 0.6836 - accuracy: 0.7048 - balanced_recall: 0.8330 - balanced_precision: 0.5418 - balanced_f1_score: 0.6561 - auc_roc: 0.8195 - val_loss: 0.8238 - val_accuracy: 0.6786 - val_balanced_recall: 0.7999 - val_balanced_precision: 0.5482 - val_balanced_f1_score: 0.6501 - val_auc_roc: 0.8090\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.68539\n",
      "Epoch 7/30\n",
      "2644/2644 [==============================] - 421s 159ms/step - loss: 0.6388 - accuracy: 0.7280 - balanced_recall: 0.8512 - balanced_precision: 0.5507 - balanced_f1_score: 0.6682 - auc_roc: 0.8338 - val_loss: 0.9031 - val_accuracy: 0.6641 - val_balanced_recall: 0.8181 - val_balanced_precision: 0.5359 - val_balanced_f1_score: 0.6472 - val_auc_roc: 0.8043\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.68539\n",
      "Epoch 8/30\n",
      "2644/2644 [==============================] - 420s 159ms/step - loss: 0.5969 - accuracy: 0.7476 - balanced_recall: 0.8651 - balanced_precision: 0.5595 - balanced_f1_score: 0.6790 - auc_roc: 0.8465 - val_loss: 0.9562 - val_accuracy: 0.6630 - val_balanced_recall: 0.8078 - val_balanced_precision: 0.5392 - val_balanced_f1_score: 0.6462 - val_auc_roc: 0.8013\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.68539\n",
      "Wall time: 57min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# augmented with sr = 0.1\n",
    "Accuracy_aug_sr, Macro_F1_aug_sr, ROC_AUC_aug_sr, metrics_sr = fine_tune_BERT(X_train_aug_sr, X_dev_aug_sr, X_test_aug_sr, \n",
    "                                                            y_train_aug_sr, y_dev_orig, y_test_orig, 'EDA_sr_10aug', \n",
    "                                                            learning_rate = 2e-05, epsilon=1e-08, \n",
    "                                                            train_layers = 1, epochs = 30, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2644/2644 [==============================] - 435s 161ms/step - loss: 1.0089 - accuracy: 0.5317 - balanced_recall: 0.6453 - balanced_precision: 0.4556 - balanced_f1_score: 0.5333 - auc_roc: 0.6875 - val_loss: 0.7744 - val_accuracy: 0.6604 - val_balanced_recall: 0.7421 - val_balanced_precision: 0.5357 - val_balanced_f1_score: 0.6218 - val_auc_roc: 0.7954\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.66043, saving model to ./Saved_Models/EDA_b_10aug/EDA_ri_10aug\\EDA_ri_10aug\n",
      "Epoch 2/30\n",
      "2644/2644 [==============================] - 426s 161ms/step - loss: 0.8519 - accuracy: 0.6192 - balanced_recall: 0.6964 - balanced_precision: 0.5045 - balanced_f1_score: 0.5843 - auc_roc: 0.7495 - val_loss: 0.7489 - val_accuracy: 0.6812 - val_balanced_recall: 0.7802 - val_balanced_precision: 0.5427 - val_balanced_f1_score: 0.6397 - val_auc_roc: 0.7953\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.66043 to 0.68123, saving model to ./Saved_Models/EDA_b_10aug/EDA_ri_10aug\\EDA_ri_10aug\n",
      "Epoch 3/30\n",
      "2644/2644 [==============================] - 424s 160ms/step - loss: 0.8009 - accuracy: 0.6478 - balanced_recall: 0.7130 - balanced_precision: 0.5207 - balanced_f1_score: 0.6010 - auc_roc: 0.7664 - val_loss: 0.7382 - val_accuracy: 0.6854 - val_balanced_recall: 0.7519 - val_balanced_precision: 0.5569 - val_balanced_f1_score: 0.6395 - val_auc_roc: 0.8009\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.68123 to 0.68539, saving model to ./Saved_Models/EDA_b_10aug/EDA_ri_10aug\\EDA_ri_10aug\n",
      "Epoch 4/30\n",
      "2644/2644 [==============================] - 424s 160ms/step - loss: 0.7636 - accuracy: 0.6688 - balanced_recall: 0.7264 - balanced_precision: 0.5329 - balanced_f1_score: 0.6140 - auc_roc: 0.7786 - val_loss: 0.7250 - val_accuracy: 0.6953 - val_balanced_recall: 0.7717 - val_balanced_precision: 0.5693 - val_balanced_f1_score: 0.6547 - val_auc_roc: 0.7970\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.68539 to 0.69527, saving model to ./Saved_Models/EDA_b_10aug/EDA_ri_10aug\\EDA_ri_10aug\n",
      "Epoch 5/30\n",
      "2644/2644 [==============================] - 424s 160ms/step - loss: 0.7269 - accuracy: 0.6852 - balanced_recall: 0.7399 - balanced_precision: 0.5413 - balanced_f1_score: 0.6244 - auc_roc: 0.7879 - val_loss: 0.7637 - val_accuracy: 0.6875 - val_balanced_recall: 0.7507 - val_balanced_precision: 0.5541 - val_balanced_f1_score: 0.6372 - val_auc_roc: 0.7928\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.69527\n",
      "Epoch 6/30\n",
      "2644/2644 [==============================] - 422s 160ms/step - loss: 0.6880 - accuracy: 0.7061 - balanced_recall: 0.7524 - balanced_precision: 0.5560 - balanced_f1_score: 0.6387 - auc_roc: 0.8010 - val_loss: 0.8000 - val_accuracy: 0.6750 - val_balanced_recall: 0.7380 - val_balanced_precision: 0.5574 - val_balanced_f1_score: 0.6348 - val_auc_roc: 0.7851\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.69527\n",
      "Epoch 7/30\n",
      "2644/2644 [==============================] - 422s 160ms/step - loss: 0.6509 - accuracy: 0.7243 - balanced_recall: 0.7621 - balanced_precision: 0.5668 - balanced_f1_score: 0.6493 - auc_roc: 0.8117 - val_loss: 0.8417 - val_accuracy: 0.6771 - val_balanced_recall: 0.7479 - val_balanced_precision: 0.5578 - val_balanced_f1_score: 0.6387 - val_auc_roc: 0.7839\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.69527\n",
      "Epoch 8/30\n",
      "2644/2644 [==============================] - 422s 160ms/step - loss: 0.6117 - accuracy: 0.7425 - balanced_recall: 0.7743 - balanced_precision: 0.5771 - balanced_f1_score: 0.6605 - auc_roc: 0.8215 - val_loss: 0.8652 - val_accuracy: 0.6661 - val_balanced_recall: 0.7345 - val_balanced_precision: 0.5716 - val_balanced_f1_score: 0.6426 - val_auc_roc: 0.7765\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.69527\n",
      "Wall time: 57min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# augmented with ri = 0.1\n",
    "Accuracy_aug_ri, Macro_F1_aug_ri, ROC_AUC_aug_ri, metrics_ri = fine_tune_BERT(X_train_aug_ri, X_dev_aug_ri, X_test_aug_ri, \n",
    "                                                            y_train_aug_ri, y_dev_orig, y_test_orig, 'EDA_ri_10aug', \n",
    "                                                            learning_rate = 2e-05, epsilon=1e-08, \n",
    "                                                            train_layers = 1, epochs = 30, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2644/2644 [==============================] - 433s 161ms/step - loss: 0.9808 - accuracy: 0.5306 - balanced_recall: 0.7234 - balanced_precision: 0.4385 - balanced_f1_score: 0.5454 - auc_roc: 0.6939 - val_loss: 0.7696 - val_accuracy: 0.6682 - val_balanced_recall: 0.8490 - val_balanced_precision: 0.5248 - val_balanced_f1_score: 0.6472 - val_auc_roc: 0.8149\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.66823, saving model to ./Saved_Models/EDA_b_10aug/EDA_rs_10aug\\EDA_rs_10aug\n",
      "Epoch 2/30\n",
      "2644/2644 [==============================] - 425s 161ms/step - loss: 0.8438 - accuracy: 0.6120 - balanced_recall: 0.7672 - balanced_precision: 0.4826 - balanced_f1_score: 0.5918 - auc_roc: 0.7518 - val_loss: 0.7441 - val_accuracy: 0.6713 - val_balanced_recall: 0.7967 - val_balanced_precision: 0.5598 - val_balanced_f1_score: 0.6567 - val_auc_roc: 0.8222\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.66823 to 0.67135, saving model to ./Saved_Models/EDA_b_10aug/EDA_rs_10aug\\EDA_rs_10aug\n",
      "Epoch 3/30\n",
      "2644/2644 [==============================] - 428s 162ms/step - loss: 0.7896 - accuracy: 0.6406 - balanced_recall: 0.7803 - balanced_precision: 0.4995 - balanced_f1_score: 0.6084 - auc_roc: 0.7703 - val_loss: 0.7326 - val_accuracy: 0.6942 - val_balanced_recall: 0.8478 - val_balanced_precision: 0.5493 - val_balanced_f1_score: 0.6653 - val_auc_roc: 0.8235\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.67135 to 0.69423, saving model to ./Saved_Models/EDA_b_10aug/EDA_rs_10aug\\EDA_rs_10aug\n",
      "Epoch 4/30\n",
      "2644/2644 [==============================] - 423s 160ms/step - loss: 0.7433 - accuracy: 0.6692 - balanced_recall: 0.7966 - balanced_precision: 0.5118 - balanced_f1_score: 0.6226 - auc_roc: 0.7847 - val_loss: 0.7474 - val_accuracy: 0.6895 - val_balanced_recall: 0.8434 - val_balanced_precision: 0.5560 - val_balanced_f1_score: 0.6690 - val_auc_roc: 0.8168\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.69423\n",
      "Epoch 5/30\n",
      "2644/2644 [==============================] - 422s 159ms/step - loss: 0.7004 - accuracy: 0.6964 - balanced_recall: 0.8085 - balanced_precision: 0.5187 - balanced_f1_score: 0.6314 - auc_roc: 0.7955 - val_loss: 0.7752 - val_accuracy: 0.6875 - val_balanced_recall: 0.8398 - val_balanced_precision: 0.5502 - val_balanced_f1_score: 0.6637 - val_auc_roc: 0.8157\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.69423\n",
      "Epoch 6/30\n",
      "2644/2644 [==============================] - 420s 159ms/step - loss: 0.6524 - accuracy: 0.7208 - balanced_recall: 0.8238 - balanced_precision: 0.5271 - balanced_f1_score: 0.6423 - auc_roc: 0.8077 - val_loss: 0.8307 - val_accuracy: 0.6651 - val_balanced_recall: 0.8319 - val_balanced_precision: 0.5457 - val_balanced_f1_score: 0.6578 - val_auc_roc: 0.8130\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.69423\n",
      "Epoch 7/30\n",
      "2644/2644 [==============================] - 422s 159ms/step - loss: 0.6065 - accuracy: 0.7425 - balanced_recall: 0.8395 - balanced_precision: 0.5351 - balanced_f1_score: 0.6530 - auc_roc: 0.8202 - val_loss: 0.8742 - val_accuracy: 0.6693 - val_balanced_recall: 0.8391 - val_balanced_precision: 0.5499 - val_balanced_f1_score: 0.6633 - val_auc_roc: 0.8140\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.69423\n",
      "Wall time: 49min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# augmented with rs = 0.1\n",
    "Accuracy_aug_rs, Macro_F1_aug_rs, ROC_AUC_aug_rs, metrics_rs = fine_tune_BERT(X_train_aug_rs, X_dev_aug_rs, X_test_aug_rs, \n",
    "                                                            y_train_aug_rs, y_dev_orig, y_test_orig, 'EDA_rs_10aug',\n",
    "                                                            learning_rate = 2e-05, epsilon=1e-08, \n",
    "                                                            train_layers = 1, epochs = 30, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2644/2644 [==============================] - 433s 161ms/step - loss: 1.0639 - accuracy: 0.4975 - balanced_recall: 0.6618 - balanced_precision: 0.4138 - balanced_f1_score: 0.5084 - auc_roc: 0.6464 - val_loss: 0.8127 - val_accuracy: 0.6485 - val_balanced_recall: 0.8088 - val_balanced_precision: 0.5143 - val_balanced_f1_score: 0.6258 - val_auc_roc: 0.7776\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.64847, saving model to ./Saved_Models/EDA_b_10aug/EDA_rd_10aug\\EDA_rd_10aug\n",
      "Epoch 2/30\n",
      "2644/2644 [==============================] - 425s 161ms/step - loss: 0.9148 - accuracy: 0.5723 - balanced_recall: 0.7318 - balanced_precision: 0.4423 - balanced_f1_score: 0.5507 - auc_roc: 0.7018 - val_loss: 0.7648 - val_accuracy: 0.6755 - val_balanced_recall: 0.8480 - val_balanced_precision: 0.5089 - val_balanced_f1_score: 0.6328 - val_auc_roc: 0.7928\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.64847 to 0.67551, saving model to ./Saved_Models/EDA_b_10aug/EDA_rd_10aug\\EDA_rd_10aug\n",
      "Epoch 3/30\n",
      "2644/2644 [==============================] - 423s 160ms/step - loss: 0.8684 - accuracy: 0.5987 - balanced_recall: 0.7591 - balanced_precision: 0.4544 - balanced_f1_score: 0.5678 - auc_roc: 0.7232 - val_loss: 0.7432 - val_accuracy: 0.6885 - val_balanced_recall: 0.8338 - val_balanced_precision: 0.5111 - val_balanced_f1_score: 0.6306 - val_auc_roc: 0.8013\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.67551 to 0.68851, saving model to ./Saved_Models/EDA_b_10aug/EDA_rd_10aug\\EDA_rd_10aug\n",
      "Epoch 4/30\n",
      "2644/2644 [==============================] - 422s 159ms/step - loss: 0.8345 - accuracy: 0.6190 - balanced_recall: 0.7783 - balanced_precision: 0.4637 - balanced_f1_score: 0.5805 - auc_roc: 0.7397 - val_loss: 0.7511 - val_accuracy: 0.6895 - val_balanced_recall: 0.8612 - val_balanced_precision: 0.5034 - val_balanced_f1_score: 0.6335 - val_auc_roc: 0.8053\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.68851 to 0.68955, saving model to ./Saved_Models/EDA_b_10aug/EDA_rd_10aug\\EDA_rd_10aug\n",
      "Epoch 5/30\n",
      "2644/2644 [==============================] - 422s 160ms/step - loss: 0.8039 - accuracy: 0.6380 - balanced_recall: 0.7940 - balanced_precision: 0.4737 - balanced_f1_score: 0.5928 - auc_roc: 0.7559 - val_loss: 0.7660 - val_accuracy: 0.6859 - val_balanced_recall: 0.8347 - val_balanced_precision: 0.5164 - val_balanced_f1_score: 0.6350 - val_auc_roc: 0.8130\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.68955\n",
      "Epoch 6/30\n",
      "2644/2644 [==============================] - 420s 159ms/step - loss: 0.7705 - accuracy: 0.6549 - balanced_recall: 0.8107 - balanced_precision: 0.4830 - balanced_f1_score: 0.6048 - auc_roc: 0.7701 - val_loss: 0.7933 - val_accuracy: 0.6843 - val_balanced_recall: 0.8561 - val_balanced_precision: 0.5092 - val_balanced_f1_score: 0.6370 - val_auc_roc: 0.8137\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.68955\n",
      "Epoch 7/30\n",
      "2644/2644 [==============================] - 421s 159ms/step - loss: 0.7372 - accuracy: 0.6696 - balanced_recall: 0.8241 - balanced_precision: 0.4889 - balanced_f1_score: 0.6133 - auc_roc: 0.7827 - val_loss: 0.8415 - val_accuracy: 0.6797 - val_balanced_recall: 0.8702 - val_balanced_precision: 0.5077 - val_balanced_f1_score: 0.6394 - val_auc_roc: 0.8109\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.68955\n",
      "Epoch 8/30\n",
      "2644/2644 [==============================] - 419s 159ms/step - loss: 0.7053 - accuracy: 0.6854 - balanced_recall: 0.8329 - balanced_precision: 0.4960 - balanced_f1_score: 0.6212 - auc_roc: 0.7926 - val_loss: 0.8981 - val_accuracy: 0.6745 - val_balanced_recall: 0.8576 - val_balanced_precision: 0.5065 - val_balanced_f1_score: 0.6352 - val_auc_roc: 0.7999\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.68955\n",
      "Wall time: 56min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# augmented with rd = 0.1\n",
    "Accuracy_aug_rd, Macro_F1_aug_rd, ROC_AUC_aug_rd, metrics_rd = fine_tune_BERT(X_train_aug_rd, X_dev_aug_rd, X_test_aug_rd, \n",
    "                                                            y_train_aug_rd, y_dev_orig, y_test_orig, 'EDA_rd_10aug',\n",
    "                                                            learning_rate = 2e-05, epsilon=1e-08, \n",
    "                                                            train_layers = 1, epochs = 30, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2644/2644 [==============================] - 433s 161ms/step - loss: 0.9484 - accuracy: 0.5494 - balanced_recall: 0.6916 - balanced_precision: 0.3838 - balanced_f1_score: 0.4927 - auc_roc: 0.6134 - val_loss: 0.7802 - val_accuracy: 0.6708 - val_balanced_recall: 0.8796 - val_balanced_precision: 0.4290 - val_balanced_f1_score: 0.5751 - val_auc_roc: 0.7782\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.67083, saving model to ./Saved_Models/EDA_b_10aug/EDA_all_1_10aug\\EDA_all_1_10aug\n",
      "Epoch 2/30\n",
      "2644/2644 [==============================] - 425s 161ms/step - loss: 0.8239 - accuracy: 0.6365 - balanced_recall: 0.7258 - balanced_precision: 0.3995 - balanced_f1_score: 0.5145 - auc_roc: 0.6455 - val_loss: 0.7568 - val_accuracy: 0.6838 - val_balanced_recall: 0.8662 - val_balanced_precision: 0.4594 - val_balanced_f1_score: 0.5990 - val_auc_roc: 0.7980\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.67083 to 0.68383, saving model to ./Saved_Models/EDA_b_10aug/EDA_all_1_10aug\\EDA_all_1_10aug\n",
      "Epoch 3/30\n",
      "2644/2644 [==============================] - 423s 160ms/step - loss: 0.7615 - accuracy: 0.6693 - balanced_recall: 0.7457 - balanced_precision: 0.4147 - balanced_f1_score: 0.5321 - auc_roc: 0.6711 - val_loss: 0.7541 - val_accuracy: 0.6786 - val_balanced_recall: 0.8476 - val_balanced_precision: 0.4812 - val_balanced_f1_score: 0.6135 - val_auc_roc: 0.8100\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.68383\n",
      "Epoch 4/30\n",
      "2644/2644 [==============================] - 421s 159ms/step - loss: 0.7020 - accuracy: 0.7021 - balanced_recall: 0.7704 - balanced_precision: 0.4319 - balanced_f1_score: 0.5527 - auc_roc: 0.7009 - val_loss: 0.7825 - val_accuracy: 0.6797 - val_balanced_recall: 0.8670 - val_balanced_precision: 0.4881 - val_balanced_f1_score: 0.6242 - val_auc_roc: 0.8172\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.68383\n",
      "Epoch 5/30\n",
      "2644/2644 [==============================] - 422s 160ms/step - loss: 0.6450 - accuracy: 0.7300 - balanced_recall: 0.7941 - balanced_precision: 0.4461 - balanced_f1_score: 0.5705 - auc_roc: 0.7269 - val_loss: 0.8262 - val_accuracy: 0.6719 - val_balanced_recall: 0.8926 - val_balanced_precision: 0.4728 - val_balanced_f1_score: 0.6177 - val_auc_roc: 0.8173\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.68383\n",
      "Epoch 6/30\n",
      "2644/2644 [==============================] - 421s 159ms/step - loss: 0.5869 - accuracy: 0.7580 - balanced_recall: 0.8152 - balanced_precision: 0.4609 - balanced_f1_score: 0.5880 - auc_roc: 0.7515 - val_loss: 0.8925 - val_accuracy: 0.6833 - val_balanced_recall: 0.8748 - val_balanced_precision: 0.4832 - val_balanced_f1_score: 0.6222 - val_auc_roc: 0.8172\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.68383\n",
      "Wall time: 42min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# augmented with all = 0.1\n",
    "Accuracy_aug_all_1, Macro_F1_aug_all_1, ROC_AUC_aug_all_1, metrics_all_1 = fine_tune_BERT(X_train_all_1, X_dev_all_1, X_test_all_1, \n",
    "                                                            y_train_all_1, y_dev_orig, y_test_orig, 'EDA_all_1_10aug',\n",
    "                                                            learning_rate = 2e-05, epsilon=1e-08, \n",
    "                                                            train_layers = 1, epochs = 30, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2644/2644 [==============================] - 433s 161ms/step - loss: 1.0467 - accuracy: 0.4857 - balanced_recall: 0.5229 - balanced_precision: 0.4363 - balanced_f1_score: 0.4742 - auc_roc: 0.6370 - val_loss: 0.7944 - val_accuracy: 0.6438 - val_balanced_recall: 0.7012 - val_balanced_precision: 0.5735 - val_balanced_f1_score: 0.6301 - val_auc_roc: 0.8065\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.64379, saving model to ./Saved_Models/EDA_b_10aug/EDA_all_5_10aug\\EDA_all_5_10aug\n",
      "Epoch 2/30\n",
      "2644/2644 [==============================] - 423s 160ms/step - loss: 0.9230 - accuracy: 0.5689 - balanced_recall: 0.6109 - balanced_precision: 0.4691 - balanced_f1_score: 0.5292 - auc_roc: 0.6875 - val_loss: 0.7635 - val_accuracy: 0.6667 - val_balanced_recall: 0.7648 - val_balanced_precision: 0.5480 - val_balanced_f1_score: 0.6373 - val_auc_roc: 0.8068\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.64379 to 0.66667, saving model to ./Saved_Models/EDA_b_10aug/EDA_all_5_10aug\\EDA_all_5_10aug\n",
      "Epoch 3/30\n",
      "2644/2644 [==============================] - 424s 160ms/step - loss: 0.8806 - accuracy: 0.5972 - balanced_recall: 0.6574 - balanced_precision: 0.4663 - balanced_f1_score: 0.5443 - auc_roc: 0.6927 - val_loss: 0.7432 - val_accuracy: 0.6739 - val_balanced_recall: 0.7990 - val_balanced_precision: 0.5244 - val_balanced_f1_score: 0.6314 - val_auc_roc: 0.7897\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.66667 to 0.67395, saving model to ./Saved_Models/EDA_b_10aug/EDA_all_5_10aug\\EDA_all_5_10aug\n",
      "Epoch 4/30\n",
      "2644/2644 [==============================] - 422s 159ms/step - loss: 0.8568 - accuracy: 0.6099 - balanced_recall: 0.6779 - balanced_precision: 0.4604 - balanced_f1_score: 0.5471 - auc_roc: 0.6907 - val_loss: 0.7316 - val_accuracy: 0.6854 - val_balanced_recall: 0.8372 - val_balanced_precision: 0.4898 - val_balanced_f1_score: 0.6158 - val_auc_roc: 0.7601\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.67395 to 0.68539, saving model to ./Saved_Models/EDA_b_10aug/EDA_all_5_10aug\\EDA_all_5_10aug\n",
      "Epoch 5/30\n",
      "2644/2644 [==============================] - 422s 159ms/step - loss: 0.8325 - accuracy: 0.6242 - balanced_recall: 0.6968 - balanced_precision: 0.4581 - balanced_f1_score: 0.5517 - auc_roc: 0.6890 - val_loss: 0.7240 - val_accuracy: 0.6911 - val_balanced_recall: 0.8344 - val_balanced_precision: 0.4788 - val_balanced_f1_score: 0.6074 - val_auc_roc: 0.7507\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.68539 to 0.69111, saving model to ./Saved_Models/EDA_b_10aug/EDA_all_5_10aug\\EDA_all_5_10aug\n",
      "Epoch 6/30\n",
      "2644/2644 [==============================] - 421s 159ms/step - loss: 0.8140 - accuracy: 0.6336 - balanced_recall: 0.7128 - balanced_precision: 0.4530 - balanced_f1_score: 0.5530 - auc_roc: 0.6851 - val_loss: 0.7310 - val_accuracy: 0.6895 - val_balanced_recall: 0.8485 - val_balanced_precision: 0.4652 - val_balanced_f1_score: 0.6001 - val_auc_roc: 0.7411\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.69111\n",
      "Epoch 7/30\n",
      "2644/2644 [==============================] - 420s 159ms/step - loss: 0.7951 - accuracy: 0.6448 - balanced_recall: 0.7319 - balanced_precision: 0.4524 - balanced_f1_score: 0.5582 - auc_roc: 0.6868 - val_loss: 0.7314 - val_accuracy: 0.6849 - val_balanced_recall: 0.8452 - val_balanced_precision: 0.4651 - val_balanced_f1_score: 0.5993 - val_auc_roc: 0.7296\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.69111\n",
      "Epoch 8/30\n",
      "2644/2644 [==============================] - 420s 159ms/step - loss: 0.7799 - accuracy: 0.6538 - balanced_recall: 0.7384 - balanced_precision: 0.4532 - balanced_f1_score: 0.5607 - auc_roc: 0.6880 - val_loss: 0.7414 - val_accuracy: 0.6817 - val_balanced_recall: 0.8605 - val_balanced_precision: 0.4548 - val_balanced_f1_score: 0.5944 - val_auc_roc: 0.7229\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.69111\n",
      "Epoch 9/30\n",
      "2644/2644 [==============================] - 419s 159ms/step - loss: 0.7614 - accuracy: 0.6644 - balanced_recall: 0.7514 - balanced_precision: 0.4548 - balanced_f1_score: 0.5657 - auc_roc: 0.6927 - val_loss: 0.7522 - val_accuracy: 0.6781 - val_balanced_recall: 0.8631 - val_balanced_precision: 0.4536 - val_balanced_f1_score: 0.5940 - val_auc_roc: 0.7189\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.69111\n",
      "Wall time: 1h 3min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# augmented with all = 0.5\n",
    "Accuracy_aug_all_5, Macro_F1_aug_all_5, ROC_AUC_aug_all_5, metrics_all_5 = fine_tune_BERT(X_train_all_5, X_dev_all_5, X_test_all_5, \n",
    "                                                            y_train_all_5, y_dev_orig, y_test_orig, 'EDA_all_5_10aug',\n",
    "                                                            learning_rate = 2e-05, epsilon=1e-08, \n",
    "                                                            train_layers = 1, epochs = 30, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.703068122724909,\n",
       " 0.6830480350859433,\n",
       " 0.7660838421593968,\n",
       " {'loss': [1.3042175769805908,\n",
       "   1.1071772575378418,\n",
       "   1.0218576192855835,\n",
       "   0.9753274917602539,\n",
       "   0.9385999441146851,\n",
       "   0.916458785533905,\n",
       "   0.9061144590377808,\n",
       "   0.8801248073577881,\n",
       "   0.8665584921836853,\n",
       "   0.8640263080596924,\n",
       "   0.8530023694038391,\n",
       "   0.8423669338226318,\n",
       "   0.8295612335205078,\n",
       "   0.8290104269981384,\n",
       "   0.8134371042251587,\n",
       "   0.8112388253211975,\n",
       "   0.8006798028945923,\n",
       "   0.7997586727142334,\n",
       "   0.7816831469535828,\n",
       "   0.7874255776405334,\n",
       "   0.7729077935218811,\n",
       "   0.7625702619552612,\n",
       "   0.7577171325683594,\n",
       "   0.7567818760871887,\n",
       "   0.7422648668289185],\n",
       "  'accuracy': [0.41318339109420776,\n",
       "   0.4789052903652191,\n",
       "   0.5170642733573914,\n",
       "   0.5486575961112976,\n",
       "   0.5652993321418762,\n",
       "   0.5792108178138733,\n",
       "   0.5849314332008362,\n",
       "   0.6017031669616699,\n",
       "   0.6070337295532227,\n",
       "   0.6091789603233337,\n",
       "   0.6165897250175476,\n",
       "   0.6254956722259521,\n",
       "   0.6275758743286133,\n",
       "   0.6322563886642456,\n",
       "   0.6345316171646118,\n",
       "   0.6367418766021729,\n",
       "   0.6414223313331604,\n",
       "   0.6445426940917969,\n",
       "   0.6516284346580505,\n",
       "   0.656438946723938,\n",
       "   0.6584541201591492,\n",
       "   0.6628096103668213,\n",
       "   0.6713904738426208,\n",
       "   0.6714555025100708,\n",
       "   0.6763960123062134],\n",
       "  'balanced_recall': [0.5771132111549377,\n",
       "   0.6479657292366028,\n",
       "   0.6777820587158203,\n",
       "   0.7057908773422241,\n",
       "   0.7249908447265625,\n",
       "   0.7328742146492004,\n",
       "   0.7407330870628357,\n",
       "   0.755496621131897,\n",
       "   0.758863091468811,\n",
       "   0.7601017951965332,\n",
       "   0.7704845666885376,\n",
       "   0.771750807762146,\n",
       "   0.7717141509056091,\n",
       "   0.7761049866676331,\n",
       "   0.7790464758872986,\n",
       "   0.7825970649719238,\n",
       "   0.7913976907730103,\n",
       "   0.7954404354095459,\n",
       "   0.8001755475997925,\n",
       "   0.8008033633232117,\n",
       "   0.805180549621582,\n",
       "   0.8076688051223755,\n",
       "   0.8151414394378662,\n",
       "   0.8158600330352783,\n",
       "   0.8225833773612976],\n",
       "  'balanced_precision': [0.3822810649871826,\n",
       "   0.4273926317691803,\n",
       "   0.4562928378582001,\n",
       "   0.47596219182014465,\n",
       "   0.4878315031528473,\n",
       "   0.49486202001571655,\n",
       "   0.4997231066226959,\n",
       "   0.5078943371772766,\n",
       "   0.5154119729995728,\n",
       "   0.5147815942764282,\n",
       "   0.5192795991897583,\n",
       "   0.5258592963218689,\n",
       "   0.5307452082633972,\n",
       "   0.533222496509552,\n",
       "   0.5358188152313232,\n",
       "   0.5383011698722839,\n",
       "   0.5419897437095642,\n",
       "   0.5444166660308838,\n",
       "   0.5456579923629761,\n",
       "   0.5443704128265381,\n",
       "   0.549816906452179,\n",
       "   0.5517266988754272,\n",
       "   0.5614181160926819,\n",
       "   0.5598947405815125,\n",
       "   0.5611206889152527],\n",
       "  'balanced_f1_score': [0.4596146047115326,\n",
       "   0.5147606134414673,\n",
       "   0.5451087355613708,\n",
       "   0.56824791431427,\n",
       "   0.5828589200973511,\n",
       "   0.5904591083526611,\n",
       "   0.5964430570602417,\n",
       "   0.6071000099182129,\n",
       "   0.6135233640670776,\n",
       "   0.6135303974151611,\n",
       "   0.620102047920227,\n",
       "   0.625131368637085,\n",
       "   0.6285189390182495,\n",
       "   0.6318332552909851,\n",
       "   0.6345340609550476,\n",
       "   0.6375276446342468,\n",
       "   0.6430719494819641,\n",
       "   0.6460381150245667,\n",
       "   0.6485262513160706,\n",
       "   0.6477392911911011,\n",
       "   0.6530736684799194,\n",
       "   0.6552408933639526,\n",
       "   0.664563775062561,\n",
       "   0.6636746525764465,\n",
       "   0.6667903065681458],\n",
       "  'auc_roc': [0.5876747965812683,\n",
       "   0.6620151996612549,\n",
       "   0.6992902159690857,\n",
       "   0.7271081805229187,\n",
       "   0.7419202923774719,\n",
       "   0.753631055355072,\n",
       "   0.7580767869949341,\n",
       "   0.7698896527290344,\n",
       "   0.7769994139671326,\n",
       "   0.7778997421264648,\n",
       "   0.783507764339447,\n",
       "   0.7896735668182373,\n",
       "   0.7946394085884094,\n",
       "   0.7961270809173584,\n",
       "   0.8007588982582092,\n",
       "   0.8017069101333618,\n",
       "   0.8071926236152649,\n",
       "   0.808745801448822,\n",
       "   0.8142538666725159,\n",
       "   0.8129966259002686,\n",
       "   0.8169496655464172,\n",
       "   0.8218579888343811,\n",
       "   0.8271116614341736,\n",
       "   0.8265542984008789,\n",
       "   0.8323754072189331],\n",
       "  'val_loss': [0.9417169094085693,\n",
       "   0.8825333714485168,\n",
       "   0.8407684564590454,\n",
       "   0.8209264874458313,\n",
       "   0.8108322024345398,\n",
       "   0.8013791441917419,\n",
       "   0.7861307859420776,\n",
       "   0.7819427847862244,\n",
       "   0.7700421810150146,\n",
       "   0.7670618891716003,\n",
       "   0.7581495642662048,\n",
       "   0.7706505060195923,\n",
       "   0.7569683790206909,\n",
       "   0.7546444535255432,\n",
       "   0.7490862011909485,\n",
       "   0.7546945214271545,\n",
       "   0.7371900677680969,\n",
       "   0.733688473701477,\n",
       "   0.749954879283905,\n",
       "   0.7331259250640869,\n",
       "   0.7305643558502197,\n",
       "   0.7315552234649658,\n",
       "   0.7326014637947083,\n",
       "   0.7323665022850037,\n",
       "   0.7388015985488892],\n",
       "  'val_accuracy': [0.5366614460945129,\n",
       "   0.5793031454086304,\n",
       "   0.6162246465682983,\n",
       "   0.6229849457740784,\n",
       "   0.6354654431343079,\n",
       "   0.64482581615448,\n",
       "   0.6411856412887573,\n",
       "   0.6547061800956726,\n",
       "   0.652626097202301,\n",
       "   0.6640665531158447,\n",
       "   0.6599063873291016,\n",
       "   0.6671866774559021,\n",
       "   0.66458660364151,\n",
       "   0.6760270595550537,\n",
       "   0.672386884689331,\n",
       "   0.6729069352149963,\n",
       "   0.6807072162628174,\n",
       "   0.6760270595550537,\n",
       "   0.6760270595550537,\n",
       "   0.6807072162628174,\n",
       "   0.6890275478363037,\n",
       "   0.6864274740219116,\n",
       "   0.6879875063896179,\n",
       "   0.6833073496818542,\n",
       "   0.6853874325752258],\n",
       "  'val_balanced_recall': [0.7150296568870544,\n",
       "   0.7308275699615479,\n",
       "   0.7621203660964966,\n",
       "   0.7694876790046692,\n",
       "   0.7706858515739441,\n",
       "   0.7640987634658813,\n",
       "   0.8055827021598816,\n",
       "   0.7940921187400818,\n",
       "   0.7966938018798828,\n",
       "   0.7815191149711609,\n",
       "   0.8122327327728271,\n",
       "   0.7892249226570129,\n",
       "   0.7862414717674255,\n",
       "   0.7997266054153442,\n",
       "   0.7951968908309937,\n",
       "   0.8017092347145081,\n",
       "   0.810870885848999,\n",
       "   0.8040432333946228,\n",
       "   0.7994245886802673,\n",
       "   0.8202713131904602,\n",
       "   0.8188891410827637,\n",
       "   0.8100736141204834,\n",
       "   0.8128117918968201,\n",
       "   0.8142409920692444,\n",
       "   0.8166034817695618],\n",
       "  'val_balanced_precision': [0.46855705976486206,\n",
       "   0.5105805993080139,\n",
       "   0.5263581871986389,\n",
       "   0.5406642556190491,\n",
       "   0.5402717590332031,\n",
       "   0.550575315952301,\n",
       "   0.5585888624191284,\n",
       "   0.5492990016937256,\n",
       "   0.5595665574073792,\n",
       "   0.5702555179595947,\n",
       "   0.5697022080421448,\n",
       "   0.5688157677650452,\n",
       "   0.5737301111221313,\n",
       "   0.5670596361160278,\n",
       "   0.5722362995147705,\n",
       "   0.5659425258636475,\n",
       "   0.5685086846351624,\n",
       "   0.5735786557197571,\n",
       "   0.5711215734481812,\n",
       "   0.5746936798095703,\n",
       "   0.5743053555488586,\n",
       "   0.5746250748634338,\n",
       "   0.5722030997276306,\n",
       "   0.5744199156761169,\n",
       "   0.5709693431854248],\n",
       "  'val_balanced_f1_score': [0.5649035573005676,\n",
       "   0.6001725792884827,\n",
       "   0.6216966509819031,\n",
       "   0.6341289281845093,\n",
       "   0.6341586709022522,\n",
       "   0.6390852332115173,\n",
       "   0.6587084531784058,\n",
       "   0.6482682824134827,\n",
       "   0.6565032601356506,\n",
       "   0.6585589051246643,\n",
       "   0.6687522530555725,\n",
       "   0.6601163744926453,\n",
       "   0.6624761819839478,\n",
       "   0.6626341342926025,\n",
       "   0.6645870804786682,\n",
       "   0.662534773349762,\n",
       "   0.6674906015396118,\n",
       "   0.6686950325965881,\n",
       "   0.6653440594673157,\n",
       "   0.6749442219734192,\n",
       "   0.6741613149642944,\n",
       "   0.6713376045227051,\n",
       "   0.6707272529602051,\n",
       "   0.6727254390716553,\n",
       "   0.6710900068283081],\n",
       "  'val_auc_roc': [0.7279118895530701,\n",
       "   0.7658151388168335,\n",
       "   0.7896966338157654,\n",
       "   0.800423264503479,\n",
       "   0.8080422282218933,\n",
       "   0.8138603568077087,\n",
       "   0.8127006888389587,\n",
       "   0.822168231010437,\n",
       "   0.8239614963531494,\n",
       "   0.8308663964271545,\n",
       "   0.8279192447662354,\n",
       "   0.8303250074386597,\n",
       "   0.8356307744979858,\n",
       "   0.8357832431793213,\n",
       "   0.8379611968994141,\n",
       "   0.8362555503845215,\n",
       "   0.8391574025154114,\n",
       "   0.8422852158546448,\n",
       "   0.8398692607879639,\n",
       "   0.8400759696960449,\n",
       "   0.8425033092498779,\n",
       "   0.8431538343429565,\n",
       "   0.8443856239318848,\n",
       "   0.8439677953720093,\n",
       "   0.8439218997955322]})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Accuracy_orig, Macro_F1_orig, ROC_AUC_orig, metrics_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy_aug_sr, Macro_F1_aug_sr, ROC_AUC_aug_sr, metrics_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy_aug_ri, Macro_F1_aug_ri, ROC_AUC_aug_ri, metrics_ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy_aug_rs, Macro_F1_aug_rs, ROC_AUC_aug_rs, metrics_rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy_aug_rd, Macro_F1_aug_rd, ROC_AUC_aug_rd, metrics_rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy_aug_all_1, Macro_F1_aug_all_1, ROC_AUC_aug_all_1, metrics_all_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy_aug_all_5, Macro_F1_aug_all_5, ROC_AUC_aug_all_5, metrics_all_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_name_list = ['Original Data', 'Augmented SR 0.1', 'Augmented RI 0.1', \n",
    "                   'Augmented RS 0.1', 'Augmented RD 0.1', 'Augmented All 0.1', 'Augmented All 0.5']\n",
    "\n",
    "acc_list = [Accuracy_orig, Accuracy_aug_sr, Accuracy_aug_ri, Accuracy_aug_rs, \n",
    "            Accuracy_aug_rd, Accuracy_aug_all_1, Accuracy_aug_all_5]\n",
    "\n",
    "macro_f1_list = [Macro_F1_orig, Macro_F1_aug_sr, Macro_F1_aug_ri, Macro_F1_aug_rs, \n",
    "                 Macro_F1_aug_rd, Macro_F1_aug_all_1, Macro_F1_aug_all_5]\n",
    "\n",
    "roc_auc_list = [ROC_AUC_orig, ROC_AUC_aug_sr, ROC_AUC_aug_ri, ROC_AUC_aug_rs, \n",
    "                ROC_AUC_aug_rd, ROC_AUC_aug_all_1, ROC_AUC_aug_all_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {'Trial Name' : trial_name_list, 'Test Accuracy Score' : acc_list, \n",
    "               'Test Macro F1 Score' : macro_f1_list, 'Test ROC AUC Score' : roc_auc_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trial Name</th>\n",
       "      <th>Test Accuracy Score</th>\n",
       "      <th>Test Macro F1 Score</th>\n",
       "      <th>Test ROC AUC Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Original Data</td>\n",
       "      <td>0.703068</td>\n",
       "      <td>0.683048</td>\n",
       "      <td>0.766084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Augmented SR 0.1</td>\n",
       "      <td>0.703588</td>\n",
       "      <td>0.678805</td>\n",
       "      <td>0.764023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Augmented RI 0.1</td>\n",
       "      <td>0.700988</td>\n",
       "      <td>0.684669</td>\n",
       "      <td>0.768324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Augmented RS 0.1</td>\n",
       "      <td>0.710868</td>\n",
       "      <td>0.691864</td>\n",
       "      <td>0.773969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Augmented RD 0.1</td>\n",
       "      <td>0.693708</td>\n",
       "      <td>0.672480</td>\n",
       "      <td>0.760032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Augmented All 0.1</td>\n",
       "      <td>0.699948</td>\n",
       "      <td>0.681153</td>\n",
       "      <td>0.766542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Augmented All 0.5</td>\n",
       "      <td>0.698388</td>\n",
       "      <td>0.678707</td>\n",
       "      <td>0.763331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Trial Name  Test Accuracy Score  Test Macro F1 Score  \\\n",
       "0      Original Data             0.703068             0.683048   \n",
       "1   Augmented SR 0.1             0.703588             0.678805   \n",
       "2   Augmented RI 0.1             0.700988             0.684669   \n",
       "3   Augmented RS 0.1             0.710868             0.691864   \n",
       "4   Augmented RD 0.1             0.693708             0.672480   \n",
       "5  Augmented All 0.1             0.699948             0.681153   \n",
       "6  Augmented All 0.5             0.698388             0.678707   \n",
       "\n",
       "   Test ROC AUC Score  \n",
       "0            0.766084  \n",
       "1            0.764023  \n",
       "2            0.768324  \n",
       "3            0.773969  \n",
       "4            0.760032  \n",
       "5            0.766542  \n",
       "6            0.763331  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(result_dict)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('./Saved_Models/EDA_b_10aug/All_DA_BERT_base_uncased_10aug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>balanced_recall</th>\n",
       "      <th>balanced_precision</th>\n",
       "      <th>balanced_f1_score</th>\n",
       "      <th>auc_roc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_balanced_recall</th>\n",
       "      <th>val_balanced_precision</th>\n",
       "      <th>val_balanced_f1_score</th>\n",
       "      <th>val_auc_roc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.304218</td>\n",
       "      <td>0.413183</td>\n",
       "      <td>0.577113</td>\n",
       "      <td>0.382281</td>\n",
       "      <td>0.459615</td>\n",
       "      <td>0.587675</td>\n",
       "      <td>0.941717</td>\n",
       "      <td>0.536661</td>\n",
       "      <td>0.715030</td>\n",
       "      <td>0.468557</td>\n",
       "      <td>0.564904</td>\n",
       "      <td>0.727912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.107177</td>\n",
       "      <td>0.478905</td>\n",
       "      <td>0.647966</td>\n",
       "      <td>0.427393</td>\n",
       "      <td>0.514761</td>\n",
       "      <td>0.662015</td>\n",
       "      <td>0.882533</td>\n",
       "      <td>0.579303</td>\n",
       "      <td>0.730828</td>\n",
       "      <td>0.510581</td>\n",
       "      <td>0.600173</td>\n",
       "      <td>0.765815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.021858</td>\n",
       "      <td>0.517064</td>\n",
       "      <td>0.677782</td>\n",
       "      <td>0.456293</td>\n",
       "      <td>0.545109</td>\n",
       "      <td>0.699290</td>\n",
       "      <td>0.840768</td>\n",
       "      <td>0.616225</td>\n",
       "      <td>0.762120</td>\n",
       "      <td>0.526358</td>\n",
       "      <td>0.621697</td>\n",
       "      <td>0.789697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.975327</td>\n",
       "      <td>0.548658</td>\n",
       "      <td>0.705791</td>\n",
       "      <td>0.475962</td>\n",
       "      <td>0.568248</td>\n",
       "      <td>0.727108</td>\n",
       "      <td>0.820926</td>\n",
       "      <td>0.622985</td>\n",
       "      <td>0.769488</td>\n",
       "      <td>0.540664</td>\n",
       "      <td>0.634129</td>\n",
       "      <td>0.800423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.938600</td>\n",
       "      <td>0.565299</td>\n",
       "      <td>0.724991</td>\n",
       "      <td>0.487832</td>\n",
       "      <td>0.582859</td>\n",
       "      <td>0.741920</td>\n",
       "      <td>0.810832</td>\n",
       "      <td>0.635465</td>\n",
       "      <td>0.770686</td>\n",
       "      <td>0.540272</td>\n",
       "      <td>0.634159</td>\n",
       "      <td>0.808042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.916459</td>\n",
       "      <td>0.579211</td>\n",
       "      <td>0.732874</td>\n",
       "      <td>0.494862</td>\n",
       "      <td>0.590459</td>\n",
       "      <td>0.753631</td>\n",
       "      <td>0.801379</td>\n",
       "      <td>0.644826</td>\n",
       "      <td>0.764099</td>\n",
       "      <td>0.550575</td>\n",
       "      <td>0.639085</td>\n",
       "      <td>0.813860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.906114</td>\n",
       "      <td>0.584931</td>\n",
       "      <td>0.740733</td>\n",
       "      <td>0.499723</td>\n",
       "      <td>0.596443</td>\n",
       "      <td>0.758077</td>\n",
       "      <td>0.786131</td>\n",
       "      <td>0.641186</td>\n",
       "      <td>0.805583</td>\n",
       "      <td>0.558589</td>\n",
       "      <td>0.658708</td>\n",
       "      <td>0.812701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.880125</td>\n",
       "      <td>0.601703</td>\n",
       "      <td>0.755497</td>\n",
       "      <td>0.507894</td>\n",
       "      <td>0.607100</td>\n",
       "      <td>0.769890</td>\n",
       "      <td>0.781943</td>\n",
       "      <td>0.654706</td>\n",
       "      <td>0.794092</td>\n",
       "      <td>0.549299</td>\n",
       "      <td>0.648268</td>\n",
       "      <td>0.822168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.866558</td>\n",
       "      <td>0.607034</td>\n",
       "      <td>0.758863</td>\n",
       "      <td>0.515412</td>\n",
       "      <td>0.613523</td>\n",
       "      <td>0.776999</td>\n",
       "      <td>0.770042</td>\n",
       "      <td>0.652626</td>\n",
       "      <td>0.796694</td>\n",
       "      <td>0.559567</td>\n",
       "      <td>0.656503</td>\n",
       "      <td>0.823961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.864026</td>\n",
       "      <td>0.609179</td>\n",
       "      <td>0.760102</td>\n",
       "      <td>0.514782</td>\n",
       "      <td>0.613530</td>\n",
       "      <td>0.777900</td>\n",
       "      <td>0.767062</td>\n",
       "      <td>0.664067</td>\n",
       "      <td>0.781519</td>\n",
       "      <td>0.570256</td>\n",
       "      <td>0.658559</td>\n",
       "      <td>0.830866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.853002</td>\n",
       "      <td>0.616590</td>\n",
       "      <td>0.770485</td>\n",
       "      <td>0.519280</td>\n",
       "      <td>0.620102</td>\n",
       "      <td>0.783508</td>\n",
       "      <td>0.758150</td>\n",
       "      <td>0.659906</td>\n",
       "      <td>0.812233</td>\n",
       "      <td>0.569702</td>\n",
       "      <td>0.668752</td>\n",
       "      <td>0.827919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.842367</td>\n",
       "      <td>0.625496</td>\n",
       "      <td>0.771751</td>\n",
       "      <td>0.525859</td>\n",
       "      <td>0.625131</td>\n",
       "      <td>0.789674</td>\n",
       "      <td>0.770651</td>\n",
       "      <td>0.667187</td>\n",
       "      <td>0.789225</td>\n",
       "      <td>0.568816</td>\n",
       "      <td>0.660116</td>\n",
       "      <td>0.830325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.829561</td>\n",
       "      <td>0.627576</td>\n",
       "      <td>0.771714</td>\n",
       "      <td>0.530745</td>\n",
       "      <td>0.628519</td>\n",
       "      <td>0.794639</td>\n",
       "      <td>0.756968</td>\n",
       "      <td>0.664587</td>\n",
       "      <td>0.786241</td>\n",
       "      <td>0.573730</td>\n",
       "      <td>0.662476</td>\n",
       "      <td>0.835631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.829010</td>\n",
       "      <td>0.632256</td>\n",
       "      <td>0.776105</td>\n",
       "      <td>0.533222</td>\n",
       "      <td>0.631833</td>\n",
       "      <td>0.796127</td>\n",
       "      <td>0.754644</td>\n",
       "      <td>0.676027</td>\n",
       "      <td>0.799727</td>\n",
       "      <td>0.567060</td>\n",
       "      <td>0.662634</td>\n",
       "      <td>0.835783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.813437</td>\n",
       "      <td>0.634532</td>\n",
       "      <td>0.779046</td>\n",
       "      <td>0.535819</td>\n",
       "      <td>0.634534</td>\n",
       "      <td>0.800759</td>\n",
       "      <td>0.749086</td>\n",
       "      <td>0.672387</td>\n",
       "      <td>0.795197</td>\n",
       "      <td>0.572236</td>\n",
       "      <td>0.664587</td>\n",
       "      <td>0.837961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.811239</td>\n",
       "      <td>0.636742</td>\n",
       "      <td>0.782597</td>\n",
       "      <td>0.538301</td>\n",
       "      <td>0.637528</td>\n",
       "      <td>0.801707</td>\n",
       "      <td>0.754695</td>\n",
       "      <td>0.672907</td>\n",
       "      <td>0.801709</td>\n",
       "      <td>0.565943</td>\n",
       "      <td>0.662535</td>\n",
       "      <td>0.836256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.800680</td>\n",
       "      <td>0.641422</td>\n",
       "      <td>0.791398</td>\n",
       "      <td>0.541990</td>\n",
       "      <td>0.643072</td>\n",
       "      <td>0.807193</td>\n",
       "      <td>0.737190</td>\n",
       "      <td>0.680707</td>\n",
       "      <td>0.810871</td>\n",
       "      <td>0.568509</td>\n",
       "      <td>0.667491</td>\n",
       "      <td>0.839157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.799759</td>\n",
       "      <td>0.644543</td>\n",
       "      <td>0.795440</td>\n",
       "      <td>0.544417</td>\n",
       "      <td>0.646038</td>\n",
       "      <td>0.808746</td>\n",
       "      <td>0.733688</td>\n",
       "      <td>0.676027</td>\n",
       "      <td>0.804043</td>\n",
       "      <td>0.573579</td>\n",
       "      <td>0.668695</td>\n",
       "      <td>0.842285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.781683</td>\n",
       "      <td>0.651628</td>\n",
       "      <td>0.800176</td>\n",
       "      <td>0.545658</td>\n",
       "      <td>0.648526</td>\n",
       "      <td>0.814254</td>\n",
       "      <td>0.749955</td>\n",
       "      <td>0.676027</td>\n",
       "      <td>0.799425</td>\n",
       "      <td>0.571122</td>\n",
       "      <td>0.665344</td>\n",
       "      <td>0.839869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.787426</td>\n",
       "      <td>0.656439</td>\n",
       "      <td>0.800803</td>\n",
       "      <td>0.544370</td>\n",
       "      <td>0.647739</td>\n",
       "      <td>0.812997</td>\n",
       "      <td>0.733126</td>\n",
       "      <td>0.680707</td>\n",
       "      <td>0.820271</td>\n",
       "      <td>0.574694</td>\n",
       "      <td>0.674944</td>\n",
       "      <td>0.840076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.772908</td>\n",
       "      <td>0.658454</td>\n",
       "      <td>0.805181</td>\n",
       "      <td>0.549817</td>\n",
       "      <td>0.653074</td>\n",
       "      <td>0.816950</td>\n",
       "      <td>0.730564</td>\n",
       "      <td>0.689028</td>\n",
       "      <td>0.818889</td>\n",
       "      <td>0.574305</td>\n",
       "      <td>0.674161</td>\n",
       "      <td>0.842503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.762570</td>\n",
       "      <td>0.662810</td>\n",
       "      <td>0.807669</td>\n",
       "      <td>0.551727</td>\n",
       "      <td>0.655241</td>\n",
       "      <td>0.821858</td>\n",
       "      <td>0.731555</td>\n",
       "      <td>0.686427</td>\n",
       "      <td>0.810074</td>\n",
       "      <td>0.574625</td>\n",
       "      <td>0.671338</td>\n",
       "      <td>0.843154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.757717</td>\n",
       "      <td>0.671390</td>\n",
       "      <td>0.815141</td>\n",
       "      <td>0.561418</td>\n",
       "      <td>0.664564</td>\n",
       "      <td>0.827112</td>\n",
       "      <td>0.732601</td>\n",
       "      <td>0.687988</td>\n",
       "      <td>0.812812</td>\n",
       "      <td>0.572203</td>\n",
       "      <td>0.670727</td>\n",
       "      <td>0.844386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.756782</td>\n",
       "      <td>0.671456</td>\n",
       "      <td>0.815860</td>\n",
       "      <td>0.559895</td>\n",
       "      <td>0.663675</td>\n",
       "      <td>0.826554</td>\n",
       "      <td>0.732367</td>\n",
       "      <td>0.683307</td>\n",
       "      <td>0.814241</td>\n",
       "      <td>0.574420</td>\n",
       "      <td>0.672725</td>\n",
       "      <td>0.843968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.742265</td>\n",
       "      <td>0.676396</td>\n",
       "      <td>0.822583</td>\n",
       "      <td>0.561121</td>\n",
       "      <td>0.666790</td>\n",
       "      <td>0.832375</td>\n",
       "      <td>0.738802</td>\n",
       "      <td>0.685387</td>\n",
       "      <td>0.816603</td>\n",
       "      <td>0.570969</td>\n",
       "      <td>0.671090</td>\n",
       "      <td>0.843922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  accuracy  balanced_recall  balanced_precision  \\\n",
       "0   1.304218  0.413183         0.577113            0.382281   \n",
       "1   1.107177  0.478905         0.647966            0.427393   \n",
       "2   1.021858  0.517064         0.677782            0.456293   \n",
       "3   0.975327  0.548658         0.705791            0.475962   \n",
       "4   0.938600  0.565299         0.724991            0.487832   \n",
       "5   0.916459  0.579211         0.732874            0.494862   \n",
       "6   0.906114  0.584931         0.740733            0.499723   \n",
       "7   0.880125  0.601703         0.755497            0.507894   \n",
       "8   0.866558  0.607034         0.758863            0.515412   \n",
       "9   0.864026  0.609179         0.760102            0.514782   \n",
       "10  0.853002  0.616590         0.770485            0.519280   \n",
       "11  0.842367  0.625496         0.771751            0.525859   \n",
       "12  0.829561  0.627576         0.771714            0.530745   \n",
       "13  0.829010  0.632256         0.776105            0.533222   \n",
       "14  0.813437  0.634532         0.779046            0.535819   \n",
       "15  0.811239  0.636742         0.782597            0.538301   \n",
       "16  0.800680  0.641422         0.791398            0.541990   \n",
       "17  0.799759  0.644543         0.795440            0.544417   \n",
       "18  0.781683  0.651628         0.800176            0.545658   \n",
       "19  0.787426  0.656439         0.800803            0.544370   \n",
       "20  0.772908  0.658454         0.805181            0.549817   \n",
       "21  0.762570  0.662810         0.807669            0.551727   \n",
       "22  0.757717  0.671390         0.815141            0.561418   \n",
       "23  0.756782  0.671456         0.815860            0.559895   \n",
       "24  0.742265  0.676396         0.822583            0.561121   \n",
       "\n",
       "    balanced_f1_score   auc_roc  val_loss  val_accuracy  val_balanced_recall  \\\n",
       "0            0.459615  0.587675  0.941717      0.536661             0.715030   \n",
       "1            0.514761  0.662015  0.882533      0.579303             0.730828   \n",
       "2            0.545109  0.699290  0.840768      0.616225             0.762120   \n",
       "3            0.568248  0.727108  0.820926      0.622985             0.769488   \n",
       "4            0.582859  0.741920  0.810832      0.635465             0.770686   \n",
       "5            0.590459  0.753631  0.801379      0.644826             0.764099   \n",
       "6            0.596443  0.758077  0.786131      0.641186             0.805583   \n",
       "7            0.607100  0.769890  0.781943      0.654706             0.794092   \n",
       "8            0.613523  0.776999  0.770042      0.652626             0.796694   \n",
       "9            0.613530  0.777900  0.767062      0.664067             0.781519   \n",
       "10           0.620102  0.783508  0.758150      0.659906             0.812233   \n",
       "11           0.625131  0.789674  0.770651      0.667187             0.789225   \n",
       "12           0.628519  0.794639  0.756968      0.664587             0.786241   \n",
       "13           0.631833  0.796127  0.754644      0.676027             0.799727   \n",
       "14           0.634534  0.800759  0.749086      0.672387             0.795197   \n",
       "15           0.637528  0.801707  0.754695      0.672907             0.801709   \n",
       "16           0.643072  0.807193  0.737190      0.680707             0.810871   \n",
       "17           0.646038  0.808746  0.733688      0.676027             0.804043   \n",
       "18           0.648526  0.814254  0.749955      0.676027             0.799425   \n",
       "19           0.647739  0.812997  0.733126      0.680707             0.820271   \n",
       "20           0.653074  0.816950  0.730564      0.689028             0.818889   \n",
       "21           0.655241  0.821858  0.731555      0.686427             0.810074   \n",
       "22           0.664564  0.827112  0.732601      0.687988             0.812812   \n",
       "23           0.663675  0.826554  0.732367      0.683307             0.814241   \n",
       "24           0.666790  0.832375  0.738802      0.685387             0.816603   \n",
       "\n",
       "    val_balanced_precision  val_balanced_f1_score  val_auc_roc  \n",
       "0                 0.468557               0.564904     0.727912  \n",
       "1                 0.510581               0.600173     0.765815  \n",
       "2                 0.526358               0.621697     0.789697  \n",
       "3                 0.540664               0.634129     0.800423  \n",
       "4                 0.540272               0.634159     0.808042  \n",
       "5                 0.550575               0.639085     0.813860  \n",
       "6                 0.558589               0.658708     0.812701  \n",
       "7                 0.549299               0.648268     0.822168  \n",
       "8                 0.559567               0.656503     0.823961  \n",
       "9                 0.570256               0.658559     0.830866  \n",
       "10                0.569702               0.668752     0.827919  \n",
       "11                0.568816               0.660116     0.830325  \n",
       "12                0.573730               0.662476     0.835631  \n",
       "13                0.567060               0.662634     0.835783  \n",
       "14                0.572236               0.664587     0.837961  \n",
       "15                0.565943               0.662535     0.836256  \n",
       "16                0.568509               0.667491     0.839157  \n",
       "17                0.573579               0.668695     0.842285  \n",
       "18                0.571122               0.665344     0.839869  \n",
       "19                0.574694               0.674944     0.840076  \n",
       "20                0.574305               0.674161     0.842503  \n",
       "21                0.574625               0.671338     0.843154  \n",
       "22                0.572203               0.670727     0.844386  \n",
       "23                0.574420               0.672725     0.843968  \n",
       "24                0.570969               0.671090     0.843922  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_org_df = pd.DataFrame(metrics_orig)\n",
    "\n",
    "metrics_org_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_list = [metrics_orig, metrics_sr, metrics_ri, metrics_rs, metrics_rd, metrics_all_1, metrics_all_5]\n",
    "name_list = ['fit_metrics_orig.csv', 'fit_metrics_sr.csv', 'fit_metrics_ri.csv', 'fit_metrics_rs.csv', 'fit_metrics_rd.csv', 'fit_metrics_all_1.csv', 'fit_metrics_all_5.csv']\n",
    "\n",
    "i = 0\n",
    "for m in metrics_list:\n",
    "    df = pd.DataFrame(m)\n",
    "    df.to_csv(name_list[i])\n",
    "    i += 1\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOeNGBZaTc3vi1iokDn5dgn",
   "collapsed_sections": [],
   "name": "Data_Processing",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02d7e8c53bf94bf48c06cea9abbd9aba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bf38f0560d3e493ca2b97fd974d00462",
       "IPY_MODEL_8a60b779e7d749d284b791b9e5126a62",
       "IPY_MODEL_2e51f1cad2b84893b537beb7e4ebf67c"
      ],
      "layout": "IPY_MODEL_d57bc505733b4c738cf84db24feba360"
     }
    },
    "047bcb706b304be09200793be7524708": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "071ca09b48bf4d9989852d66f8d57642": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8aa487d37b284b2ba88903923a0086d9",
       "IPY_MODEL_b3ba42a7365d418f9cbf7f68ede3b65a",
       "IPY_MODEL_c7e4cea875eb41c8950f97350c237730"
      ],
      "layout": "IPY_MODEL_15db3b4b336b41cdb973852bb3fef72f"
     }
    },
    "0c234fac6d10482089dacd2c03a5bbde": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12bf3b8f0bab4586847a07e8c438dafa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e67ba177d3744ea4bcc652774cd13abd",
       "IPY_MODEL_43eb8a62436a4c149e25f98f49a7c68d",
       "IPY_MODEL_3723a8c5f8eb47efa71e50a5ef924123"
      ],
      "layout": "IPY_MODEL_8039ef2f59ee482781f325f5a91b93e8"
     }
    },
    "15db3b4b336b41cdb973852bb3fef72f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b1a083fc8e643a78f055105960b523e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "263134a33eaa4345926dd786256c08eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2af599ea5c3d4874aff863303c5b5703": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b473e96d36604d71ae5d3d8e69cc01c3",
      "placeholder": "",
      "style": "IPY_MODEL_263134a33eaa4345926dd786256c08eb",
      "value": "Downloading: 100%"
     }
    },
    "2e51f1cad2b84893b537beb7e4ebf67c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3df12951362451685bea68b7091c69f",
      "placeholder": "",
      "style": "IPY_MODEL_b8b44bed109443fd9bba63a7039f3c93",
      "value": " 226k/226k [00:00&lt;00:00, 3.21MB/s]"
     }
    },
    "3723a8c5f8eb47efa71e50a5ef924123": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b1a083fc8e643a78f055105960b523e",
      "placeholder": "",
      "style": "IPY_MODEL_c26d9b9e910a43febfd588927c3e7756",
      "value": " 299/299 [00:00&lt;00:00, 5.18kB/s]"
     }
    },
    "3dfc3ee445f2403a8ed7b4e6f5a8b3d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "43eb8a62436a4c149e25f98f49a7c68d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7942944ae43141c0aa83ec76fc0deafa",
      "max": 299,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5d2efc2c1758475c9d1cd51cdc7edf0a",
      "value": 299
     }
    },
    "45b7612393254fe19709bdae7d7bbbe6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4656eb21ff2245b7be51f82c1e2c7e6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2af599ea5c3d4874aff863303c5b5703",
       "IPY_MODEL_c64b940afcc94ee3b2b88c582b54d97f",
       "IPY_MODEL_8a798f3e801d47df8771840991f7cfa3"
      ],
      "layout": "IPY_MODEL_ab8f5f020c76459aad12d4cab6a36a72"
     }
    },
    "5d2efc2c1758475c9d1cd51cdc7edf0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5e02a09e9fe24718a8f1f4167c42d099": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "70f68856bdb64c30aa57e6046468b3d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7942944ae43141c0aa83ec76fc0deafa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d3ff64481b64406aadb34439d2ad02b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7d77d8f8c129400d9f61b60da03891b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_98efd1135c4f438c8b8dd5c130cc667c",
       "IPY_MODEL_9aca8f0c4013472c8514d2d57f9ea3e8",
       "IPY_MODEL_807b39071d354a1ab7af51a05f1ef3b1"
      ],
      "layout": "IPY_MODEL_971c955528914b05afe5d12e9c626cf1"
     }
    },
    "8039ef2f59ee482781f325f5a91b93e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "807b39071d354a1ab7af51a05f1ef3b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc6b1e6a0e254b82b6c7277f10a5d542",
      "placeholder": "",
      "style": "IPY_MODEL_5e02a09e9fe24718a8f1f4167c42d099",
      "value": " 112/112 [00:00&lt;00:00, 2.56kB/s]"
     }
    },
    "8a60b779e7d749d284b791b9e5126a62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c234fac6d10482089dacd2c03a5bbde",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_70f68856bdb64c30aa57e6046468b3d6",
      "value": 231508
     }
    },
    "8a798f3e801d47df8771840991f7cfa3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_047bcb706b304be09200793be7524708",
      "placeholder": "",
      "style": "IPY_MODEL_c4792815ec0d4591bcaea2e7a0539e6b",
      "value": " 790/790 [00:00&lt;00:00, 14.6kB/s]"
     }
    },
    "8aa487d37b284b2ba88903923a0086d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee76f1517ed64f4fa49d998d2448f9c2",
      "placeholder": "",
      "style": "IPY_MODEL_e88a61936e0949c490366d0619869b6c",
      "value": "Downloading: 100%"
     }
    },
    "971c955528914b05afe5d12e9c626cf1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98efd1135c4f438c8b8dd5c130cc667c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b575b556bb9445ceb5c662b9d224e171",
      "placeholder": "",
      "style": "IPY_MODEL_acd4639a73904507b7884adc96512dc5",
      "value": "Downloading: 100%"
     }
    },
    "9aca8f0c4013472c8514d2d57f9ea3e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d2803d4a8a794a83a2ebcf61ce3097ff",
      "max": 112,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f9a30d0d10744d14a47b500251c1973c",
      "value": 112
     }
    },
    "9e9b4804607046678d3d132528486b0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ab8f5f020c76459aad12d4cab6a36a72": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "acd4639a73904507b7884adc96512dc5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b3ba42a7365d418f9cbf7f68ede3b65a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d12cbe52662546d1be82cfd4536dfdfc",
      "max": 437996231,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_45b7612393254fe19709bdae7d7bbbe6",
      "value": 437996231
     }
    },
    "b473e96d36604d71ae5d3d8e69cc01c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b575b556bb9445ceb5c662b9d224e171": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b8b44bed109443fd9bba63a7039f3c93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf38f0560d3e493ca2b97fd974d00462": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_feb0fac998144444bfd9373ce537bbc3",
      "placeholder": "",
      "style": "IPY_MODEL_7d3ff64481b64406aadb34439d2ad02b",
      "value": "Downloading: 100%"
     }
    },
    "c26d9b9e910a43febfd588927c3e7756": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c3df12951362451685bea68b7091c69f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4792815ec0d4591bcaea2e7a0539e6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c64b940afcc94ee3b2b88c582b54d97f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e35113cc44f34a1fbb22f109f49f7bdd",
      "max": 790,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3dfc3ee445f2403a8ed7b4e6f5a8b3d3",
      "value": 790
     }
    },
    "c7e4cea875eb41c8950f97350c237730": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d7abb004da9d42b787717cfdb3657f4e",
      "placeholder": "",
      "style": "IPY_MODEL_ee89c9881c75441fbc9ec827d623134e",
      "value": " 418M/418M [00:11&lt;00:00, 37.0MB/s]"
     }
    },
    "d12cbe52662546d1be82cfd4536dfdfc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2803d4a8a794a83a2ebcf61ce3097ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d57bc505733b4c738cf84db24feba360": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7abb004da9d42b787717cfdb3657f4e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e35113cc44f34a1fbb22f109f49f7bdd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e567a77fb29d47c98fe19c440b2d31e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e67ba177d3744ea4bcc652774cd13abd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e567a77fb29d47c98fe19c440b2d31e8",
      "placeholder": "",
      "style": "IPY_MODEL_9e9b4804607046678d3d132528486b0b",
      "value": "Downloading: 100%"
     }
    },
    "e88a61936e0949c490366d0619869b6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ee76f1517ed64f4fa49d998d2448f9c2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee89c9881c75441fbc9ec827d623134e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f9a30d0d10744d14a47b500251c1973c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fc6b1e6a0e254b82b6c7277f10a5d542": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "feb0fac998144444bfd9373ce537bbc3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
